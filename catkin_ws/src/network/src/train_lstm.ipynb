{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCH = 50\n",
    "GAMMA = 0.9\n",
    "STEP_SIZE = 200\n",
    "LR = 0.001\n",
    "USE_GPU = True\n",
    "decoder = ['buoy', 'dock', 'light_buoy', 'totem']\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.Resize(227),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_path = '/media/arg_ws3/TOSHIBA EXT/data/trajectory/root.txt'\n",
    "img_path = '/media/arg_ws3/TOSHIBA EXT/data/trajectory/images/'\n",
    "ann_path = '/media/arg_ws3/TOSHIBA EXT/data/trajectory/annotations/'\n",
    "model_path = '../model/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "data_list_file = open(list_path,'r')\n",
    "raw_data_list = data_list_file.read().splitlines()\n",
    "data_list = []\n",
    "for data in raw_data_list:\n",
    "    data_split = data.split(',')\n",
    "    first_frame = data_split[0]\n",
    "    data_len = int(data_split[1])\n",
    "    if data_len >= 10:\n",
    "        data_list.append([first_frame, data_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Layer Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRN(nn.Module):\n",
    "    def __init__(self, local_size=1, alpha=1.0, beta=0.75, ACROSS_CHANNELS=True):\n",
    "        super(LRN, self).__init__()\n",
    "        self.ACROSS_CHANNELS = ACROSS_CHANNELS\n",
    "        if ACROSS_CHANNELS:\n",
    "            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1),\n",
    "                    stride=1,\n",
    "                    padding=(int((local_size-1.0)/2), 0, 0))\n",
    "        else:\n",
    "            self.average=nn.AvgPool2d(kernel_size=local_size,\n",
    "                    stride=1,\n",
    "                    padding=int((local_size-1.0)/2))\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            div = x.pow(2).unsqueeze(1)\n",
    "            div = self.average(div).squeeze(1)\n",
    "            div = div.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        else:\n",
    "            div = x.pow(2)\n",
    "            div = self.average(div)\n",
    "            div = div.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        x = x.div(div)\n",
    "        return x\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class alexnet_conv_layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alexnet_conv_layers, self).__init__()\n",
    "        self.base_features = torchvision.models.alexnet(pretrained = True).features\n",
    "        self.skip1 = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels=16, kernel_size=1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        self.skip2 = nn.Sequential(\n",
    "            nn.Conv2d(192, out_channels=32, kernel_size=1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        self.skip5 = nn.Sequential(\n",
    "            nn.Conv2d(256, out_channels=64, kernel_size=1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Linear(37104 * 2, 2048),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Freeze those weights\n",
    "        for p in self.base_features.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        layer_extractor_x = []\n",
    "        layer_extractor_y = []\n",
    "        for idx, model in enumerate(self.base_features):\n",
    "            x = model(x)\n",
    "            y = model(y)\n",
    "            if idx in {2, 5, 11}: # layer output of conv1, conv2 , conv5(before pooling layer)\n",
    "                layer_extractor_x.append(x)\n",
    "                layer_extractor_y.append(y)\n",
    "                \n",
    "        x_out_flat = x.view(1, -1) #(1, 256, 6, 6) --> (1, 9216)\n",
    "        x_out_skip1 = self.skip1(layer_extractor_x[0]) #(1, 64, 27, 27) -> (11664)\n",
    "        x_out_skip2 = self.skip2(layer_extractor_x[1]) #(1, 192, 13, 13) -> (5408)\n",
    "        x_out_skip5 = self.skip5(layer_extractor_x[2]) #(1, 256, 13, 13) -> (10816)\n",
    "        x_out = torch.cat((x_out_skip1, x_out_skip2, x_out_skip5, x_out_flat), dim=1)\n",
    "        \n",
    "        y_out_flat = y.view(1, -1) #(1, 256, 6, 6) --> (1, 9216)\n",
    "        y_out_skip1 = self.skip1(layer_extractor_y[0]) #(1, 64, 27, 27) -> (11664)\n",
    "        y_out_skip2 = self.skip2(layer_extractor_y[1]) #(1, 192, 13, 13) -> (5408)\n",
    "        y_out_skip5 = self.skip5(layer_extractor_y[2]) #(1, 256, 13, 13) -> (10816)\n",
    "        y_out = torch.cat((y_out_skip1, y_out_skip2, y_out_skip5, y_out_flat), dim=1)\n",
    "        \n",
    "        final_out = torch.cat((x_out, y_out), dim=1)\n",
    "        conv_out = self.conv6(final_out) # (1, 2048)\n",
    "        return conv_out\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * 0.1\n",
    "        print(\"Change learning rate to: \", param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51])\n",
      "torch.Size([1, 51])\n"
     ]
    }
   ],
   "source": [
    "a = Variable(torch.rand(1, 51)).cuda()\n",
    "print(a.shape)\n",
    "b = torch.zeros(1, 51).cuda()\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredictNet,self).__init__()\n",
    "        self.LSTM_SIZE = 256\n",
    "        alexnet = torchvision.models.alexnet(pretrained = True)\n",
    "        self.feature = alexnet.features\n",
    "        classifier = list(alexnet.classifier.children())\n",
    "        self.classifier = nn.Sequential(*classifier[:-1])\n",
    "        self.lstm1 =nn.LSTMCell(4096 + 2, self.LSTM_SIZE)\n",
    "        self.lstm2 =nn.LSTMCell(self.LSTM_SIZE, self.LSTM_SIZE)\n",
    "        \n",
    "        self.lstm1_x =nn.LSTMCell(4096 + 1, self.LSTM_SIZE)\n",
    "        self.lstm2_x = nn.LSTMCell(self.LSTM_SIZE, self.LSTM_SIZE)\n",
    "        self.lstm1_y =nn.LSTMCell(4096 + 1, self.LSTM_SIZE)\n",
    "        self.lstm2_y = nn.LSTMCell(self.LSTM_SIZE, self.LSTM_SIZE)\n",
    "        self.linear_x = nn.Linear(self.LSTM_SIZE, 1)\n",
    "        self.linear_y = nn.Linear(self.LSTM_SIZE, 1)\n",
    "        self.linear = nn.Linear(self.LSTM_SIZE, 2)\n",
    "        \n",
    "        self.h_1 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.h_2 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.c_1 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.c_2 = self.get_hidden(self.LSTM_SIZE)\n",
    "        \n",
    "        self.h_x1 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.h_y1 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.h_x2 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.h_y2 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.c_x1 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.c_y1 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.c_x2 = self.get_hidden(self.LSTM_SIZE)\n",
    "        self.c_y2 = self.get_hidden(self.LSTM_SIZE)\n",
    "\n",
    "    def init_hidden(self, num):\n",
    "        self.h_1 = self.get_hidden(num)\n",
    "        self.h_2 = self.get_hidden(num)\n",
    "        self.c_1 = self.get_hidden(num)\n",
    "        self.c_2 = self.get_hidden(num)\n",
    "        \n",
    "        self.h_x1 = self.get_hidden(num)\n",
    "        self.h_y1 = self.get_hidden(num)\n",
    "        self.h_x2 = self.get_hidden(num)\n",
    "        self.h_y2 = self.get_hidden(num)\n",
    "        self.c_x1 = self.get_hidden(num)\n",
    "        self.c_y1 = self.get_hidden(num)\n",
    "        self.c_x2 = self.get_hidden(num)\n",
    "        self.c_y2 = self.get_hidden(num)\n",
    "\n",
    "    def get_hidden(self, num):\n",
    "        if USE_GPU:\n",
    "            #return (Variable(torch.rand(1, num)).cuda(), Variable(torch.rand(1, num)).cuda())\n",
    "            return Variable(torch.rand(1, num)).cuda()\n",
    "            #return torch.zeros(1, num).cuda()\n",
    "        else:\n",
    "            #return (Variable(torch.rand(1, num)), Variable(torch.rand(1, num)))\n",
    "            return Variable(torch.rand(1, num))\n",
    "            #return torch.zeros(t1, num)\n",
    "        \n",
    "    def forward(self, img, input_x, input_y):\n",
    "        '''h_x1 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        h_y1 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        h_x2 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        h_y2 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        c_x1 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        c_y1 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        c_x2 = torch.zeros(1, self.LSTM_SIZE).cuda()\n",
    "        c_y2 = torch.zeros(1, self.LSTM_SIZE).cuda()'''\n",
    "        \n",
    "        img_features = self.feature(img.unsqueeze(0))\n",
    "        img_features = img_features.view(img_features.size(0), -1)\n",
    "        img_features = self.classifier(img_features).view(-1)\n",
    "        x = input_x\n",
    "        y = input_y\n",
    "        \n",
    "        '''cat_x = torch.cat((img_features, x), dim=0).view(1, -1)\n",
    "        h_x1, c_x1 = self.lstm1_x(cat_x, (h_x1, c_x1))\n",
    "        h_x2, c_x2 = self.lstm2_y(h_x1, (h_x2, c_x2))\n",
    "        output_x = self.linear_x(h_x2)\n",
    "        \n",
    "        cat_y = torch.cat((img_features, y), dim=0).view(1, -1)\n",
    "        h_y1, c_y1 = self.lstm1_y(cat_y, (h_y1, c_y1))\n",
    "        h_y2, c_y2 = self.lstm2_y(h_y1, (h_y2, c_y2))\n",
    "        output_y = self.linear_y(h_y2)'''\n",
    "        \n",
    "        '''cat_x = torch.cat((img_features, x), dim=0).view(1, -1)\n",
    "        self.h_x1, self.c_x1 = self.lstm1_x(cat_x, (self.h_x1, self.c_x1))\n",
    "        self.h_x2, self.c_x2 = self.lstm2_y(self.h_x1, (self.h_x2, self.c_x2))\n",
    "        output_x = self.linear_x(self.h_x2)\n",
    "        \n",
    "        cat_y = torch.cat((img_features, y), dim=0).view(1, -1)\n",
    "        self.h_y1, self.c_y1 = self.lstm1_y(cat_y, (self.h_y1, self.c_y1))\n",
    "        self.h_y2, self.c_y2 = self.lstm2_y(self.h_y1, (self.h_y2, self.c_y2))\n",
    "        output_y = self.linear_y(self.h_y2)\n",
    "        output = torch.cat((output_x, output_y))'''\n",
    "        \n",
    "        cat = torch.cat((img_features, x, y), dim=0).view(1, -1)\n",
    "        self.h_1, self.c_1 = self.lstm1(cat, (self.h_1, self.c_1))\n",
    "        self.h_2, self.c_2 = self.lstm2(self.h_1, (self.h_2, self.c_2))\n",
    "        output = self.linear(self.h_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'np.random.seed(0)\\ntorch.manual_seed(0)\\nnet = PredictNet()\\n#net.double()\\ncriterion = nn.MSELoss()\\noptimizer = torch.optim.LBFGS(net.parameters(), lr=0.1)\\nif USE_GPU:\\n    net = net.cuda()\\n    criterion = criterion.cuda()\\nnet = train_model(optimizer, criterion, net, 1)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "img_transform = transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                    ])'''\n",
    "def train_model(optimizer, criterion, net, num_epochs):\n",
    "    x = None\n",
    "    y = None\n",
    "    img = None\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch != 0 and epoch % 5 == 0:\n",
    "            adjust_learning_rate(optim)\n",
    "        curr_loss = 0.0\n",
    "        dataset_size = len(data_list)\n",
    "        sample_list = random.sample(range(0, dataset_size), dataset_size)\n",
    "        for idx in sample_list:\n",
    "            frame_name = data_list[idx][0]\n",
    "            data_len = data_list[idx][1]\n",
    "            target_x = []\n",
    "            target_y = []\n",
    "            target = []\n",
    "            out_x = []\n",
    "            out_y = []\n",
    "            out = []\n",
    "            net.init_hidden(net.LSTM_SIZE)\n",
    "            for frame_num in range(data_len-1):\n",
    "                file = open(ann_path + frame_name + '.txt','r')\n",
    "                file_split = file.read().splitlines()\n",
    "                next_frame = file_split[1]\n",
    "                img = Image.open(img_path + next_frame + '.jpg')\n",
    "                img = img.convert('RGB')\n",
    "                img = img_transform(img)\n",
    "                x = torch.tensor([float(file_split[4]) - float(file_split[2])], requires_grad=False)\n",
    "                y = torch.tensor([float(file_split[5]) - float(file_split[3])], requires_grad=False)\n",
    "                if USE_GPU:\n",
    "                    img = img.cuda()\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                if frame_num != 0:\n",
    "                    target_x = target_x + [x]\n",
    "                    target_y = target_y + [y]\n",
    "                output_x, output_y = net(img, x, y)\n",
    "                out_x = out_x + [output_x]\n",
    "                out_y = out_y + [output_y]\n",
    "                frame_name = next_frame\n",
    "            file = open(ann_path + frame_name + '.txt','r')\n",
    "            file_split = file.read().splitlines()\n",
    "            x = torch.tensor([float(file_split[4]) - float(file_split[2])])\n",
    "            y = torch.tensor([float(file_split[5]) - float(file_split[3])])\n",
    "            if USE_GPU:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            target_x = target_x + [x]\n",
    "            target_y = target_y + [y]\n",
    "                \n",
    "            target_x = torch.stack(target_x, 1).squeeze(1).view(-1)\n",
    "            target_y = torch.stack(target_y, 1).squeeze(1).view(-1)\n",
    "            target = torch.cat((target_x, target_y))\n",
    "            out_x = torch.stack(out_x, 1).squeeze(2).view(-1)\n",
    "            out_y = torch.stack(out_y, 1).squeeze(2).view(-1)\n",
    "            out = torch.cat((out_x, out_y)) #torch.Size([n])\n",
    "            \n",
    "            def clousure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(out, target)\n",
    "                print(epoch, '>>', idx, '>> loss:', loss.item())\n",
    "                loss.backward(retain_graph = True)\n",
    "                return loss\n",
    "            optimizer.step(clousure)\n",
    "            #torch.cuda.empty_cache()\n",
    "            \n",
    "    return net\n",
    "\n",
    "'''np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "net = PredictNet()\n",
    "#net.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(net.parameters(), lr=0.1)\n",
    "if USE_GPU:\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "net = train_model(optimizer, criterion, net, 1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 || Seq: 0 / 172 || loss: 90.99676804921843\n",
      "Epoch:  0 || Seq: 10 / 172 || loss: 100.79533333910837\n",
      "Epoch:  0 || Seq: 20 / 172 || loss: 196.01861368870595\n",
      "Epoch:  0 || Seq: 30 / 172 || loss: 298.86436388227673\n",
      "Epoch:  0 || Seq: 40 / 172 || loss: 25.858854489934554\n",
      "Epoch:  0 || Seq: 50 / 172 || loss: 49.74922990558609\n",
      "Epoch:  0 || Seq: 60 / 172 || loss: 4.43985784612596\n",
      "Epoch:  0 || Seq: 70 / 172 || loss: 83.84747210106191\n",
      "Epoch:  0 || Seq: 80 / 172 || loss: 23.133172557046194\n",
      "Epoch:  0 || Seq: 90 / 172 || loss: 172.89617608470672\n",
      "Epoch:  0 || Seq: 100 / 172 || loss: 10.111371204257011\n",
      "Epoch:  0 || Seq: 110 / 172 || loss: 181.9653766155243\n",
      "Epoch:  0 || Seq: 120 / 172 || loss: 113.53533525157557\n",
      "Epoch:  0 || Seq: 130 / 172 || loss: 61.13784436980883\n",
      "Epoch:  0 || Seq: 140 / 172 || loss: 305.86746164917946\n",
      "Epoch:  0 || Seq: 150 / 172 || loss: 102.12728127360384\n",
      "Epoch:  0 || Seq: 160 / 172 || loss: 2.8743587280623615\n",
      "Epoch:  0 || Seq: 170 / 172 || loss: 218.61271546103737\n",
      "Epoch:  0 || Loss:  102.52556433192522\n",
      "Epoch:  1 || Seq: 0 / 172 || loss: 39.52538217249371\n",
      "Epoch:  1 || Seq: 10 / 172 || loss: 377.53708732758577\n",
      "Epoch:  1 || Seq: 20 / 172 || loss: 49.91182899993146\n",
      "Epoch:  1 || Seq: 30 / 172 || loss: 91.88030470183983\n",
      "Epoch:  1 || Seq: 40 / 172 || loss: 85.1321703342781\n",
      "Epoch:  1 || Seq: 50 / 172 || loss: 36.522576910796\n",
      "Epoch:  1 || Seq: 60 / 172 || loss: 100.30822814470157\n",
      "Epoch:  1 || Seq: 70 / 172 || loss: 54.01063191920519\n",
      "Epoch:  1 || Seq: 80 / 172 || loss: 24.74097011383209\n",
      "Epoch:  1 || Seq: 90 / 172 || loss: 62.21133384747164\n",
      "Epoch:  1 || Seq: 100 / 172 || loss: 290.3621445536613\n",
      "Epoch:  1 || Seq: 110 / 172 || loss: 21.64039903216892\n",
      "Epoch:  1 || Seq: 120 / 172 || loss: 201.9319209883872\n",
      "Epoch:  1 || Seq: 130 / 172 || loss: 49.34221702763656\n",
      "Epoch:  1 || Seq: 140 / 172 || loss: 340.9319604439171\n",
      "Epoch:  1 || Seq: 150 / 172 || loss: 6.128610953100417\n",
      "Epoch:  1 || Seq: 160 / 172 || loss: 54.46873258203268\n",
      "Epoch:  1 || Seq: 170 / 172 || loss: 18.70497033529682\n",
      "Epoch:  1 || Loss:  81.66752738603596\n",
      "Epoch:  2 || Seq: 0 / 172 || loss: 11.314940316247744\n",
      "Epoch:  2 || Seq: 10 / 172 || loss: 12.764597415835375\n",
      "Epoch:  2 || Seq: 20 / 172 || loss: 55.0757815944297\n",
      "Epoch:  2 || Seq: 30 / 172 || loss: 41.32323196206597\n",
      "Epoch:  2 || Seq: 40 / 172 || loss: 20.850708979002846\n",
      "Epoch:  2 || Seq: 50 / 172 || loss: 4.450083621405065\n",
      "Epoch:  2 || Seq: 60 / 172 || loss: 2.953963269557183\n",
      "Epoch:  2 || Seq: 70 / 172 || loss: 8.836408420838415\n",
      "Epoch:  2 || Seq: 80 / 172 || loss: 65.31190740389209\n",
      "Epoch:  2 || Seq: 90 / 172 || loss: 121.1324617467148\n",
      "Epoch:  2 || Seq: 100 / 172 || loss: 270.76875502874867\n",
      "Epoch:  2 || Seq: 110 / 172 || loss: 66.95209932579073\n",
      "Epoch:  2 || Seq: 120 / 172 || loss: 7.631124597901362\n",
      "Epoch:  2 || Seq: 130 / 172 || loss: 206.86680581669012\n",
      "Epoch:  2 || Seq: 140 / 172 || loss: 186.39833886171692\n",
      "Epoch:  2 || Seq: 150 / 172 || loss: 77.22793322566542\n",
      "Epoch:  2 || Seq: 160 / 172 || loss: 5.617728040553629\n",
      "Epoch:  2 || Seq: 170 / 172 || loss: 34.63074158250608\n",
      "Epoch:  2 || Loss:  77.9393871914753\n",
      "Epoch:  3 || Seq: 0 / 172 || loss: 247.69240365216606\n",
      "Epoch:  3 || Seq: 10 / 172 || loss: 0.5380117588647408\n",
      "Epoch:  3 || Seq: 20 / 172 || loss: 10.019258737564087\n",
      "Epoch:  3 || Seq: 30 / 172 || loss: 19.404746066080406\n",
      "Epoch:  3 || Seq: 40 / 172 || loss: 262.0431305411458\n",
      "Epoch:  3 || Seq: 50 / 172 || loss: 16.133116521251697\n",
      "Epoch:  3 || Seq: 60 / 172 || loss: 2.4126311199506745\n",
      "Epoch:  3 || Seq: 70 / 172 || loss: 11.523461854722546\n",
      "Epoch:  3 || Seq: 80 / 172 || loss: 29.625452781905945\n",
      "Epoch:  3 || Seq: 90 / 172 || loss: 56.21490839747712\n",
      "Epoch:  3 || Seq: 100 / 172 || loss: 21.121425634936283\n",
      "Epoch:  3 || Seq: 110 / 172 || loss: 125.69001270888839\n",
      "Epoch:  3 || Seq: 120 / 172 || loss: 121.07806076357762\n",
      "Epoch:  3 || Seq: 130 / 172 || loss: 93.98932610464323\n",
      "Epoch:  3 || Seq: 140 / 172 || loss: 9.115112900578728\n",
      "Epoch:  3 || Seq: 150 / 172 || loss: 120.18779594934097\n",
      "Epoch:  3 || Seq: 160 / 172 || loss: 43.937316889946274\n",
      "Epoch:  3 || Seq: 170 / 172 || loss: 72.80741573621829\n",
      "Epoch:  3 || Loss:  81.64380788703748\n",
      "Epoch:  4 || Seq: 0 / 172 || loss: 36.934906779141684\n",
      "Epoch:  4 || Seq: 10 / 172 || loss: 42.33935267366469\n",
      "Epoch:  4 || Seq: 20 / 172 || loss: 5.527993445750326\n",
      "Epoch:  4 || Seq: 30 / 172 || loss: 2.011094765854068\n",
      "Epoch:  4 || Seq: 40 / 172 || loss: 77.1042688669903\n",
      "Epoch:  4 || Seq: 50 / 172 || loss: 45.79133115075883\n",
      "Epoch:  4 || Seq: 60 / 172 || loss: 146.90979451272221\n",
      "Epoch:  4 || Seq: 70 / 172 || loss: 157.71073070439425\n",
      "Epoch:  4 || Seq: 80 / 172 || loss: 57.49589553387726\n",
      "Epoch:  4 || Seq: 90 / 172 || loss: 49.64147999608203\n",
      "Epoch:  4 || Seq: 100 / 172 || loss: 136.38821367407218\n",
      "Epoch:  4 || Seq: 110 / 172 || loss: 1.8149647261598147\n",
      "Epoch:  4 || Seq: 120 / 172 || loss: 15.418845167590511\n",
      "Epoch:  4 || Seq: 130 / 172 || loss: 168.94417460175853\n",
      "Epoch:  4 || Seq: 140 / 172 || loss: 23.050228735612286\n",
      "Epoch:  4 || Seq: 150 / 172 || loss: 19.541740587353708\n",
      "Epoch:  4 || Seq: 160 / 172 || loss: 17.266961850376138\n",
      "Epoch:  4 || Seq: 170 / 172 || loss: 80.0454006514379\n",
      "Epoch:  4 || Loss:  72.9008283542252\n",
      "Epoch:  5 || Seq: 0 / 172 || loss: 67.23103652792898\n",
      "Epoch:  5 || Seq: 10 / 172 || loss: 113.80642023517026\n",
      "Epoch:  5 || Seq: 20 / 172 || loss: 16.116524773466057\n",
      "Epoch:  5 || Seq: 30 / 172 || loss: 92.40292504238896\n",
      "Epoch:  5 || Seq: 40 / 172 || loss: 46.96523575857282\n",
      "Epoch:  5 || Seq: 50 / 172 || loss: 32.9390489293591\n",
      "Epoch:  5 || Seq: 60 / 172 || loss: 125.83360530476256\n",
      "Epoch:  5 || Seq: 70 / 172 || loss: 59.535868852088846\n",
      "Epoch:  5 || Seq: 80 / 172 || loss: 52.632888778884805\n",
      "Epoch:  5 || Seq: 90 / 172 || loss: 59.48865105128951\n",
      "Epoch:  5 || Seq: 100 / 172 || loss: 55.73046774417162\n",
      "Epoch:  5 || Seq: 110 / 172 || loss: 210.526565099688\n",
      "Epoch:  5 || Seq: 120 / 172 || loss: 128.74455642879548\n",
      "Epoch:  5 || Seq: 130 / 172 || loss: 114.92855603894905\n",
      "Epoch:  5 || Seq: 140 / 172 || loss: 175.20260787097845\n",
      "Epoch:  5 || Seq: 150 / 172 || loss: 31.99183174351541\n",
      "Epoch:  5 || Seq: 160 / 172 || loss: 206.0835384345303\n",
      "Epoch:  5 || Seq: 170 / 172 || loss: 6.110981526343446\n",
      "Epoch:  5 || Loss:  75.01722909017136\n",
      "Epoch:  6 || Seq: 0 / 172 || loss: 100.06845910057172\n",
      "Epoch:  6 || Seq: 10 / 172 || loss: 34.111040523572065\n",
      "Epoch:  6 || Seq: 20 / 172 || loss: 63.941643898084294\n",
      "Epoch:  6 || Seq: 30 / 172 || loss: 12.051210304850649\n",
      "Epoch:  6 || Seq: 40 / 172 || loss: 52.30866547025597\n",
      "Epoch:  6 || Seq: 50 / 172 || loss: 13.090064491257072\n",
      "Epoch:  6 || Seq: 60 / 172 || loss: 26.396031582378782\n",
      "Epoch:  6 || Seq: 70 / 172 || loss: 66.42727956790465\n",
      "Epoch:  6 || Seq: 80 / 172 || loss: 82.99311494983732\n",
      "Epoch:  6 || Seq: 90 / 172 || loss: 106.49014576214056\n",
      "Epoch:  6 || Seq: 100 / 172 || loss: 151.51272686327096\n",
      "Epoch:  6 || Seq: 110 / 172 || loss: 307.91014576771045\n",
      "Epoch:  6 || Seq: 120 / 172 || loss: 65.46406047939188\n",
      "Epoch:  6 || Seq: 130 / 172 || loss: 40.84505689062644\n",
      "Epoch:  6 || Seq: 140 / 172 || loss: 167.71547418243716\n",
      "Epoch:  6 || Seq: 150 / 172 || loss: 37.230332103214764\n",
      "Epoch:  6 || Seq: 160 / 172 || loss: 68.44963573070261\n",
      "Epoch:  6 || Seq: 170 / 172 || loss: 147.7738373811272\n",
      "Epoch:  6 || Loss:  66.38401597876104\n",
      "Epoch:  7 || Seq: 0 / 172 || loss: 42.91051902398467\n",
      "Epoch:  7 || Seq: 10 / 172 || loss: 17.29899419405881\n",
      "Epoch:  7 || Seq: 20 / 172 || loss: 21.501715561922857\n",
      "Epoch:  7 || Seq: 30 / 172 || loss: 18.964650109432114\n",
      "Epoch:  7 || Seq: 40 / 172 || loss: 32.79561050387807\n",
      "Epoch:  7 || Seq: 50 / 172 || loss: 202.4227368319812\n",
      "Epoch:  7 || Seq: 60 / 172 || loss: 12.143240529824705\n",
      "Epoch:  7 || Seq: 70 / 172 || loss: 117.07772399501309\n",
      "Epoch:  7 || Seq: 80 / 172 || loss: 240.8471132704212\n",
      "Epoch:  7 || Seq: 90 / 172 || loss: 194.76588712154978\n",
      "Epoch:  7 || Seq: 100 / 172 || loss: 56.88998171692947\n",
      "Epoch:  7 || Seq: 110 / 172 || loss: 4.0204336870780315\n",
      "Epoch:  7 || Seq: 120 / 172 || loss: 191.64362049170515\n",
      "Epoch:  7 || Seq: 130 / 172 || loss: 164.3265054876154\n",
      "Epoch:  7 || Seq: 140 / 172 || loss: 229.6711450740695\n",
      "Epoch:  7 || Seq: 150 / 172 || loss: 7.933110875846899\n",
      "Epoch:  7 || Seq: 160 / 172 || loss: 0.23651129426434636\n",
      "Epoch:  7 || Seq: 170 / 172 || loss: 3.374240570313608\n",
      "Epoch:  7 || Loss:  77.45606206329091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8 || Seq: 0 / 172 || loss: 3.988186791539192\n",
      "Epoch:  8 || Seq: 10 / 172 || loss: 156.5586829481735\n",
      "Epoch:  8 || Seq: 20 / 172 || loss: 7.289586374070495\n",
      "Epoch:  8 || Seq: 30 / 172 || loss: 181.73599937646404\n",
      "Epoch:  8 || Seq: 40 / 172 || loss: 3.523729741461575\n",
      "Epoch:  8 || Seq: 50 / 172 || loss: 44.92291262894869\n",
      "Epoch:  8 || Seq: 60 / 172 || loss: 29.4814059470143\n",
      "Epoch:  8 || Seq: 70 / 172 || loss: 328.5134985588491\n",
      "Epoch:  8 || Seq: 80 / 172 || loss: 7.805354603699276\n",
      "Epoch:  8 || Seq: 90 / 172 || loss: 35.28450381522998\n",
      "Epoch:  8 || Seq: 100 / 172 || loss: 2.280439412944313\n",
      "Epoch:  8 || Seq: 110 / 172 || loss: 4.6612158318325365\n",
      "Epoch:  8 || Seq: 120 / 172 || loss: 21.09280849248171\n",
      "Epoch:  8 || Seq: 130 / 172 || loss: 26.520316538930636\n",
      "Epoch:  8 || Seq: 140 / 172 || loss: 10.449601402428621\n",
      "Epoch:  8 || Seq: 150 / 172 || loss: 129.79698225872025\n",
      "Epoch:  8 || Seq: 160 / 172 || loss: 9.29745793600367\n",
      "Epoch:  8 || Seq: 170 / 172 || loss: 18.25991296126846\n",
      "Epoch:  8 || Loss:  57.674425242821165\n",
      "Epoch:  9 || Seq: 0 / 172 || loss: 90.77215399273804\n",
      "Epoch:  9 || Seq: 10 / 172 || loss: 53.82433204445988\n",
      "Epoch:  9 || Seq: 20 / 172 || loss: 122.06611932231033\n",
      "Epoch:  9 || Seq: 30 / 172 || loss: 0.9909021198553475\n",
      "Epoch:  9 || Seq: 40 / 172 || loss: 91.11285416729127\n",
      "Epoch:  9 || Seq: 50 / 172 || loss: 80.42656661177936\n",
      "Epoch:  9 || Seq: 60 / 172 || loss: 29.750808843411505\n",
      "Epoch:  9 || Seq: 70 / 172 || loss: 56.46878676176552\n",
      "Epoch:  9 || Seq: 80 / 172 || loss: 63.29906332890193\n",
      "Epoch:  9 || Seq: 90 / 172 || loss: 28.111021486552136\n",
      "Epoch:  9 || Seq: 100 / 172 || loss: 13.91715179932745\n",
      "Epoch:  9 || Seq: 110 / 172 || loss: 5.212702733522747\n",
      "Epoch:  9 || Seq: 120 / 172 || loss: 123.51464138031005\n",
      "Epoch:  9 || Seq: 130 / 172 || loss: 13.785348072964032\n",
      "Epoch:  9 || Seq: 140 / 172 || loss: 176.98122772456784\n",
      "Epoch:  9 || Seq: 150 / 172 || loss: 1.513853886644082\n",
      "Epoch:  9 || Seq: 160 / 172 || loss: 26.101995121989603\n",
      "Epoch:  9 || Seq: 170 / 172 || loss: 59.62611959584885\n",
      "Epoch:  9 || Loss:  67.31336664379934\n",
      "Epoch:  10 || Seq: 0 / 172 || loss: 38.711235607635565\n",
      "Epoch:  10 || Seq: 10 / 172 || loss: 45.836051331600174\n",
      "Epoch:  10 || Seq: 20 / 172 || loss: 5.367598277173545\n",
      "Epoch:  10 || Seq: 30 / 172 || loss: 88.95187429594807\n",
      "Epoch:  10 || Seq: 40 / 172 || loss: 1.8762579069162408\n",
      "Epoch:  10 || Seq: 50 / 172 || loss: 42.73609743799482\n",
      "Epoch:  10 || Seq: 60 / 172 || loss: 24.1626448726389\n",
      "Epoch:  10 || Seq: 70 / 172 || loss: 1.1628910899300882\n",
      "Epoch:  10 || Seq: 80 / 172 || loss: 112.3681869422612\n",
      "Epoch:  10 || Seq: 90 / 172 || loss: 4.564546002870581\n",
      "Epoch:  10 || Seq: 100 / 172 || loss: 18.911094574215397\n",
      "Epoch:  10 || Seq: 110 / 172 || loss: 12.555320100579412\n",
      "Epoch:  10 || Seq: 120 / 172 || loss: 95.12077714011127\n",
      "Epoch:  10 || Seq: 130 / 172 || loss: 18.251831304370647\n",
      "Epoch:  10 || Seq: 140 / 172 || loss: 207.61392087130636\n",
      "Epoch:  10 || Seq: 150 / 172 || loss: 22.683364768912238\n",
      "Epoch:  10 || Seq: 160 / 172 || loss: 3.611129638522385\n",
      "Epoch:  10 || Seq: 170 / 172 || loss: 112.52443225930135\n",
      "Epoch:  10 || Loss:  61.310346391514074\n",
      "Epoch:  11 || Seq: 0 / 172 || loss: 91.146284667982\n",
      "Epoch:  11 || Seq: 10 / 172 || loss: 5.044049724226906\n",
      "Epoch:  11 || Seq: 20 / 172 || loss: 38.26686476311879\n",
      "Epoch:  11 || Seq: 30 / 172 || loss: 22.694636446956014\n",
      "Epoch:  11 || Seq: 40 / 172 || loss: 4.157506974270711\n",
      "Epoch:  11 || Seq: 50 / 172 || loss: 12.328344704395231\n",
      "Epoch:  11 || Seq: 60 / 172 || loss: 55.532091962271615\n",
      "Epoch:  11 || Seq: 70 / 172 || loss: 3.3414232712239027\n",
      "Epoch:  11 || Seq: 80 / 172 || loss: 121.79333628802782\n",
      "Epoch:  11 || Seq: 90 / 172 || loss: 383.23235621179145\n",
      "Epoch:  11 || Seq: 100 / 172 || loss: 13.32436972623691\n",
      "Epoch:  11 || Seq: 110 / 172 || loss: 10.389276274619624\n",
      "Epoch:  11 || Seq: 120 / 172 || loss: 55.1962931224477\n",
      "Epoch:  11 || Seq: 130 / 172 || loss: 12.618121696868911\n",
      "Epoch:  11 || Seq: 140 / 172 || loss: 8.255428921431303\n",
      "Epoch:  11 || Seq: 150 / 172 || loss: 49.706967011523936\n",
      "Epoch:  11 || Seq: 160 / 172 || loss: 21.732175410259515\n",
      "Epoch:  11 || Seq: 170 / 172 || loss: 31.067054172444745\n",
      "Epoch:  11 || Loss:  63.25062426190486\n",
      "Epoch:  12 || Seq: 0 / 172 || loss: 7.721096799561852\n",
      "Epoch:  12 || Seq: 10 / 172 || loss: 67.0653409473598\n",
      "Epoch:  12 || Seq: 20 / 172 || loss: 8.790309069811233\n",
      "Epoch:  12 || Seq: 30 / 172 || loss: 57.92569985630474\n",
      "Epoch:  12 || Seq: 40 / 172 || loss: 8.325778701553645\n",
      "Epoch:  12 || Seq: 50 / 172 || loss: 5.549779324460057\n",
      "Epoch:  12 || Seq: 60 / 172 || loss: 102.52742608230457\n",
      "Epoch:  12 || Seq: 70 / 172 || loss: 27.399172746435866\n",
      "Epoch:  12 || Seq: 80 / 172 || loss: 218.6873748023808\n",
      "Epoch:  12 || Seq: 90 / 172 || loss: 143.65783165084818\n",
      "Epoch:  12 || Seq: 100 / 172 || loss: 13.678270284334818\n",
      "Epoch:  12 || Seq: 110 / 172 || loss: 6.009134226111009\n",
      "Epoch:  12 || Seq: 120 / 172 || loss: 14.220139524439888\n",
      "Epoch:  12 || Seq: 130 / 172 || loss: 64.19414849232204\n",
      "Epoch:  12 || Seq: 140 / 172 || loss: 13.095018768778537\n",
      "Epoch:  12 || Seq: 150 / 172 || loss: 39.907779212575406\n",
      "Epoch:  12 || Seq: 160 / 172 || loss: 14.725390811334364\n",
      "Epoch:  12 || Seq: 170 / 172 || loss: 20.319177997464845\n",
      "Epoch:  12 || Loss:  64.9294878311466\n",
      "Epoch:  13 || Seq: 0 / 172 || loss: 4.737681139162497\n",
      "Epoch:  13 || Seq: 10 / 172 || loss: 181.64719143793494\n",
      "Epoch:  13 || Seq: 20 / 172 || loss: 18.473210860181737\n",
      "Epoch:  13 || Seq: 30 / 172 || loss: 30.473911804311417\n",
      "Epoch:  13 || Seq: 40 / 172 || loss: 60.80370942466834\n",
      "Epoch:  13 || Seq: 50 / 172 || loss: 139.90421271420294\n",
      "Epoch:  13 || Seq: 60 / 172 || loss: 2.178510875266511\n",
      "Epoch:  13 || Seq: 70 / 172 || loss: 17.455937149759848\n",
      "Epoch:  13 || Seq: 80 / 172 || loss: 19.976005224657623\n",
      "Epoch:  13 || Seq: 90 / 172 || loss: 78.37617570024572\n",
      "Epoch:  13 || Seq: 100 / 172 || loss: 1.9211704248047776\n",
      "Epoch:  13 || Seq: 110 / 172 || loss: 104.59872408553957\n",
      "Epoch:  13 || Seq: 120 / 172 || loss: 67.29712247870424\n",
      "Epoch:  13 || Seq: 130 / 172 || loss: 46.76662257409577\n",
      "Epoch:  13 || Seq: 140 / 172 || loss: 47.61931729972219\n",
      "Epoch:  13 || Seq: 150 / 172 || loss: 5.539647986998518\n",
      "Epoch:  13 || Seq: 160 / 172 || loss: 18.617078342152617\n",
      "Epoch:  13 || Seq: 170 / 172 || loss: 96.90760825293339\n",
      "Epoch:  13 || Loss:  68.54038397550742\n",
      "Epoch:  14 || Seq: 0 / 172 || loss: 16.700549870108564\n",
      "Epoch:  14 || Seq: 10 / 172 || loss: 98.61968572391197\n",
      "Epoch:  14 || Seq: 20 / 172 || loss: 2.303413212454567\n",
      "Epoch:  14 || Seq: 30 / 172 || loss: 132.40738516052565\n",
      "Epoch:  14 || Seq: 40 / 172 || loss: 351.60054684414166\n",
      "Epoch:  14 || Seq: 50 / 172 || loss: 15.397787636012904\n",
      "Epoch:  14 || Seq: 60 / 172 || loss: 19.028326419880614\n",
      "Epoch:  14 || Seq: 70 / 172 || loss: 15.74137931518172\n",
      "Epoch:  14 || Seq: 80 / 172 || loss: 6.4828367550911095\n",
      "Epoch:  14 || Seq: 90 / 172 || loss: 2.9868413308525787\n",
      "Epoch:  14 || Seq: 100 / 172 || loss: 2.421120004970222\n",
      "Epoch:  14 || Seq: 110 / 172 || loss: 36.98662244528532\n",
      "Epoch:  14 || Seq: 120 / 172 || loss: 123.70295345144612\n",
      "Epoch:  14 || Seq: 130 / 172 || loss: 36.45744232609868\n",
      "Epoch:  14 || Seq: 140 / 172 || loss: 29.907359869053057\n",
      "Epoch:  14 || Seq: 150 / 172 || loss: 167.7333780874498\n",
      "Epoch:  14 || Seq: 160 / 172 || loss: 4.5328649684786795\n",
      "Epoch:  14 || Seq: 170 / 172 || loss: 37.324075593643\n",
      "Epoch:  14 || Loss:  70.78215936566941\n",
      "Epoch:  15 || Seq: 0 / 172 || loss: 16.008289709687233\n",
      "Epoch:  15 || Seq: 10 / 172 || loss: 124.0072681894121\n",
      "Epoch:  15 || Seq: 20 / 172 || loss: 24.35657565793618\n",
      "Epoch:  15 || Seq: 30 / 172 || loss: 153.65340225654654\n",
      "Epoch:  15 || Seq: 40 / 172 || loss: 12.05267850628921\n",
      "Epoch:  15 || Seq: 50 / 172 || loss: 4.195429379362613\n",
      "Epoch:  15 || Seq: 60 / 172 || loss: 1.9194863172347791\n",
      "Epoch:  15 || Seq: 70 / 172 || loss: 21.646951239125336\n",
      "Epoch:  15 || Seq: 80 / 172 || loss: 19.867789568512105\n",
      "Epoch:  15 || Seq: 90 / 172 || loss: 69.83584662358386\n",
      "Epoch:  15 || Seq: 100 / 172 || loss: 35.47662571888594\n",
      "Epoch:  15 || Seq: 110 / 172 || loss: 88.33735957470418\n",
      "Epoch:  15 || Seq: 120 / 172 || loss: 117.27953470438719\n",
      "Epoch:  15 || Seq: 130 / 172 || loss: 174.85301107887855\n",
      "Epoch:  15 || Seq: 140 / 172 || loss: 16.851180485013174\n",
      "Epoch:  15 || Seq: 150 / 172 || loss: 16.479511198845056\n",
      "Epoch:  15 || Seq: 160 / 172 || loss: 3.15673114065654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15 || Seq: 170 / 172 || loss: 7.205359734689409\n",
      "Epoch:  15 || Loss:  61.215717663120884\n",
      "Epoch:  16 || Seq: 0 / 172 || loss: 9.8794913017191\n",
      "Epoch:  16 || Seq: 10 / 172 || loss: 43.98685060370092\n",
      "Epoch:  16 || Seq: 20 / 172 || loss: 13.753465568232867\n",
      "Epoch:  16 || Seq: 30 / 172 || loss: 20.90573579427742\n",
      "Epoch:  16 || Seq: 40 / 172 || loss: 160.45187806618364\n",
      "Epoch:  16 || Seq: 50 / 172 || loss: 175.06846104167403\n",
      "Epoch:  16 || Seq: 60 / 172 || loss: 15.05189286065778\n",
      "Epoch:  16 || Seq: 70 / 172 || loss: 61.67074175179005\n",
      "Epoch:  16 || Seq: 80 / 172 || loss: 9.520323563612214\n",
      "Epoch:  16 || Seq: 90 / 172 || loss: 12.95519976483451\n",
      "Epoch:  16 || Seq: 100 / 172 || loss: 23.188692173399474\n",
      "Epoch:  16 || Seq: 110 / 172 || loss: 0.8558970311373149\n",
      "Epoch:  16 || Seq: 120 / 172 || loss: 64.25232198244582\n",
      "Epoch:  16 || Seq: 130 / 172 || loss: 15.916577694447417\n",
      "Epoch:  16 || Seq: 140 / 172 || loss: 36.67743196165189\n",
      "Epoch:  16 || Seq: 150 / 172 || loss: 186.4296432348589\n",
      "Epoch:  16 || Seq: 160 / 172 || loss: 185.28246119171382\n",
      "Epoch:  16 || Seq: 170 / 172 || loss: 135.6894563009517\n",
      "Epoch:  16 || Loss:  62.17982817209402\n",
      "Epoch:  17 || Seq: 0 / 172 || loss: 7.801924595511274\n",
      "Epoch:  17 || Seq: 10 / 172 || loss: 22.260338162236355\n",
      "Epoch:  17 || Seq: 20 / 172 || loss: 17.245750404865248\n",
      "Epoch:  17 || Seq: 30 / 172 || loss: 18.75141608276788\n",
      "Epoch:  17 || Seq: 40 / 172 || loss: 0.5907073307025712\n",
      "Epoch:  17 || Seq: 50 / 172 || loss: 35.848895819683094\n",
      "Epoch:  17 || Seq: 60 / 172 || loss: 9.352852945878672\n",
      "Epoch:  17 || Seq: 70 / 172 || loss: 42.141038953072645\n",
      "Epoch:  17 || Seq: 80 / 172 || loss: 4.595704512527356\n",
      "Epoch:  17 || Seq: 90 / 172 || loss: 55.09574692123781\n",
      "Epoch:  17 || Seq: 100 / 172 || loss: 24.45447133340252\n",
      "Epoch:  17 || Seq: 110 / 172 || loss: 60.879388223970615\n",
      "Epoch:  17 || Seq: 120 / 172 || loss: 47.402384811769785\n",
      "Epoch:  17 || Seq: 130 / 172 || loss: 87.17538371254341\n",
      "Epoch:  17 || Seq: 140 / 172 || loss: 137.16727083226044\n",
      "Epoch:  17 || Seq: 150 / 172 || loss: 119.68031379288954\n",
      "Epoch:  17 || Seq: 160 / 172 || loss: 88.8232498257421\n",
      "Epoch:  17 || Seq: 170 / 172 || loss: 128.27838518044777\n",
      "Epoch:  17 || Loss:  57.61155537832237\n",
      "Epoch:  18 || Seq: 0 / 172 || loss: 31.520631112903356\n",
      "Epoch:  18 || Seq: 10 / 172 || loss: 40.899014971950756\n",
      "Epoch:  18 || Seq: 20 / 172 || loss: 8.004063572268933\n",
      "Epoch:  18 || Seq: 30 / 172 || loss: 133.17733720501909\n",
      "Epoch:  18 || Seq: 40 / 172 || loss: 16.88220413309591\n",
      "Epoch:  18 || Seq: 50 / 172 || loss: 317.7323020953685\n",
      "Epoch:  18 || Seq: 60 / 172 || loss: 17.795864861289207\n",
      "Epoch:  18 || Seq: 70 / 172 || loss: 135.64933845557664\n",
      "Epoch:  18 || Seq: 80 / 172 || loss: 4.589542064153577\n",
      "Epoch:  18 || Seq: 90 / 172 || loss: 4.232894184644378\n",
      "Epoch:  18 || Seq: 100 / 172 || loss: 152.44461400456288\n",
      "Epoch:  18 || Seq: 110 / 172 || loss: 66.13040735004788\n",
      "Epoch:  18 || Seq: 120 / 172 || loss: 125.97271887857642\n",
      "Epoch:  18 || Seq: 130 / 172 || loss: 26.649779435570558\n",
      "Epoch:  18 || Seq: 140 / 172 || loss: 63.85876998236697\n",
      "Epoch:  18 || Seq: 150 / 172 || loss: 4.358541818908774\n",
      "Epoch:  18 || Seq: 160 / 172 || loss: 25.241839384615776\n",
      "Epoch:  18 || Seq: 170 / 172 || loss: 9.635619833108244\n",
      "Epoch:  18 || Loss:  63.46418305321283\n",
      "Epoch:  19 || Seq: 0 / 172 || loss: 8.270798116901211\n",
      "Epoch:  19 || Seq: 10 / 172 || loss: 107.94305032748906\n",
      "Epoch:  19 || Seq: 20 / 172 || loss: 127.52692753645591\n",
      "Epoch:  19 || Seq: 30 / 172 || loss: 13.058385020742813\n",
      "Epoch:  19 || Seq: 40 / 172 || loss: 133.20275385515845\n",
      "Epoch:  19 || Seq: 50 / 172 || loss: 47.14613386045676\n",
      "Epoch:  19 || Seq: 60 / 172 || loss: 34.863394617949695\n",
      "Epoch:  19 || Seq: 70 / 172 || loss: 21.737083026518423\n",
      "Epoch:  19 || Seq: 80 / 172 || loss: 65.44384141266346\n",
      "Epoch:  19 || Seq: 90 / 172 || loss: 93.45383581727516\n",
      "Epoch:  19 || Seq: 100 / 172 || loss: 8.33324471116066\n",
      "Epoch:  19 || Seq: 110 / 172 || loss: 20.87032159069634\n",
      "Epoch:  19 || Seq: 120 / 172 || loss: 47.93113323091882\n",
      "Epoch:  19 || Seq: 130 / 172 || loss: 131.35493877009654\n",
      "Epoch:  19 || Seq: 140 / 172 || loss: 4.0520354093362885\n",
      "Epoch:  19 || Seq: 150 / 172 || loss: 132.2532157181413\n",
      "Epoch:  19 || Seq: 160 / 172 || loss: 24.72523799992078\n",
      "Epoch:  19 || Seq: 170 / 172 || loss: 103.25384879329484\n",
      "Epoch:  19 || Loss:  58.95766506027533\n",
      "Epoch:  20 || Seq: 0 / 172 || loss: 180.96989581701072\n",
      "Epoch:  20 || Seq: 10 / 172 || loss: 21.89664546412241\n",
      "Epoch:  20 || Seq: 20 / 172 || loss: 48.26321031215937\n",
      "Epoch:  20 || Seq: 30 / 172 || loss: 68.13742545843124\n",
      "Epoch:  20 || Seq: 40 / 172 || loss: 7.406535712792538\n",
      "Epoch:  20 || Seq: 50 / 172 || loss: 3.831054323341786\n",
      "Epoch:  20 || Seq: 60 / 172 || loss: 27.219004607837025\n",
      "Epoch:  20 || Seq: 70 / 172 || loss: 32.386033544298265\n",
      "Epoch:  20 || Seq: 80 / 172 || loss: 29.252151310694618\n",
      "Epoch:  20 || Seq: 90 / 172 || loss: 1.6029716634657234\n",
      "Epoch:  20 || Seq: 100 / 172 || loss: 89.56588297743228\n",
      "Epoch:  20 || Seq: 110 / 172 || loss: 79.96045176413926\n",
      "Epoch:  20 || Seq: 120 / 172 || loss: 6.826899986326073\n",
      "Epoch:  20 || Seq: 130 / 172 || loss: 23.12644101402045\n",
      "Epoch:  20 || Seq: 140 / 172 || loss: 46.419745152473055\n",
      "Epoch:  20 || Seq: 150 / 172 || loss: 82.8003567071522\n",
      "Epoch:  20 || Seq: 160 / 172 || loss: 93.13921530350395\n",
      "Epoch:  20 || Seq: 170 / 172 || loss: 101.70962400247271\n",
      "Epoch:  20 || Loss:  55.63387879383769\n",
      "Epoch:  21 || Seq: 0 / 172 || loss: 113.5601970798336\n",
      "Epoch:  21 || Seq: 10 / 172 || loss: 21.547650136494713\n",
      "Epoch:  21 || Seq: 20 / 172 || loss: 168.25895541610763\n",
      "Epoch:  21 || Seq: 30 / 172 || loss: 1.8478175667405594\n",
      "Epoch:  21 || Seq: 40 / 172 || loss: 25.566574547025894\n",
      "Epoch:  21 || Seq: 50 / 172 || loss: 98.3179163640986\n",
      "Epoch:  21 || Seq: 60 / 172 || loss: 259.09506664727814\n",
      "Epoch:  21 || Seq: 70 / 172 || loss: 35.879130695995535\n",
      "Epoch:  21 || Seq: 80 / 172 || loss: 5.299668338650788\n",
      "Epoch:  21 || Seq: 90 / 172 || loss: 4.455491480068304\n",
      "Epoch:  21 || Seq: 100 / 172 || loss: 154.34155891676033\n",
      "Epoch:  21 || Seq: 110 / 172 || loss: 19.609663321442593\n",
      "Epoch:  21 || Seq: 120 / 172 || loss: 37.1241953942041\n",
      "Epoch:  21 || Seq: 130 / 172 || loss: 86.88502892648631\n",
      "Epoch:  21 || Seq: 140 / 172 || loss: 32.65673418305814\n",
      "Epoch:  21 || Seq: 150 / 172 || loss: 122.12068581935905\n",
      "Epoch:  21 || Seq: 160 / 172 || loss: 53.18183289516518\n",
      "Epoch:  21 || Seq: 170 / 172 || loss: 21.53186554344077\n",
      "Epoch:  21 || Loss:  56.38330749573136\n",
      "Epoch:  22 || Seq: 0 / 172 || loss: 4.521963387672954\n",
      "Epoch:  22 || Seq: 10 / 172 || loss: 99.68800503282378\n",
      "Epoch:  22 || Seq: 20 / 172 || loss: 83.17299428784575\n",
      "Epoch:  22 || Seq: 30 / 172 || loss: 1.6957176860826286\n",
      "Epoch:  22 || Seq: 40 / 172 || loss: 2.951005481319347\n",
      "Epoch:  22 || Seq: 50 / 172 || loss: 42.65898020241153\n",
      "Epoch:  22 || Seq: 60 / 172 || loss: 41.776983055248856\n",
      "Epoch:  22 || Seq: 70 / 172 || loss: 1.506744367270065\n",
      "Epoch:  22 || Seq: 80 / 172 || loss: 4.085841354996185\n",
      "Epoch:  22 || Seq: 90 / 172 || loss: 147.69738476572093\n",
      "Epoch:  22 || Seq: 100 / 172 || loss: 201.95775627954438\n",
      "Epoch:  22 || Seq: 110 / 172 || loss: 7.174202921614051\n",
      "Epoch:  22 || Seq: 120 / 172 || loss: 52.96387979792516\n",
      "Epoch:  22 || Seq: 130 / 172 || loss: 40.767651955871024\n",
      "Epoch:  22 || Seq: 140 / 172 || loss: 1.8285803880232077\n",
      "Epoch:  22 || Seq: 150 / 172 || loss: 59.0963142181144\n",
      "Epoch:  22 || Seq: 160 / 172 || loss: 39.77568711491767\n",
      "Epoch:  22 || Seq: 170 / 172 || loss: 8.556115336716175\n",
      "Epoch:  22 || Loss:  58.53580505669091\n",
      "Epoch:  23 || Seq: 0 / 172 || loss: 330.300447505254\n",
      "Epoch:  23 || Seq: 10 / 172 || loss: 141.9511118453741\n",
      "Epoch:  23 || Seq: 20 / 172 || loss: 42.956568214439734\n",
      "Epoch:  23 || Seq: 30 / 172 || loss: 61.5976915188918\n",
      "Epoch:  23 || Seq: 40 / 172 || loss: 120.09776458102523\n",
      "Epoch:  23 || Seq: 50 / 172 || loss: 4.74839142194161\n",
      "Epoch:  23 || Seq: 60 / 172 || loss: 38.24986692448334\n",
      "Epoch:  23 || Seq: 70 / 172 || loss: 8.73066368133628\n",
      "Epoch:  23 || Seq: 80 / 172 || loss: 26.100878277098673\n",
      "Epoch:  23 || Seq: 90 / 172 || loss: 62.637642471782435\n",
      "Epoch:  23 || Seq: 100 / 172 || loss: 43.314877297806866\n",
      "Epoch:  23 || Seq: 110 / 172 || loss: 31.80134595947143\n",
      "Epoch:  23 || Seq: 120 / 172 || loss: 53.93411609297618\n",
      "Epoch:  23 || Seq: 130 / 172 || loss: 53.69945989129434\n",
      "Epoch:  23 || Seq: 140 / 172 || loss: 39.109101174491705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23 || Seq: 150 / 172 || loss: 1.7783598009246397\n",
      "Epoch:  23 || Seq: 160 / 172 || loss: 23.94622819514378\n",
      "Epoch:  23 || Seq: 170 / 172 || loss: 2.376844108514524\n",
      "Epoch:  23 || Loss:  65.61357518323479\n",
      "Epoch:  24 || Seq: 0 / 172 || loss: 1.2994023669469992\n",
      "Epoch:  24 || Seq: 10 / 172 || loss: 4.761895168601768\n",
      "Epoch:  24 || Seq: 20 / 172 || loss: 31.33940552002213\n",
      "Epoch:  24 || Seq: 30 / 172 || loss: 2.403219158880765\n",
      "Epoch:  24 || Seq: 40 / 172 || loss: 142.86068184840042\n",
      "Epoch:  24 || Seq: 50 / 172 || loss: 2.8550958052219357\n",
      "Epoch:  24 || Seq: 60 / 172 || loss: 119.54953050647269\n",
      "Epoch:  24 || Seq: 70 / 172 || loss: 186.07500284910202\n",
      "Epoch:  24 || Seq: 80 / 172 || loss: 47.632067817908066\n",
      "Epoch:  24 || Seq: 90 / 172 || loss: 11.144619779894128\n",
      "Epoch:  24 || Seq: 100 / 172 || loss: 73.73626357865093\n",
      "Epoch:  24 || Seq: 110 / 172 || loss: 19.46833936889705\n",
      "Epoch:  24 || Seq: 120 / 172 || loss: 46.398866674154995\n",
      "Epoch:  24 || Seq: 130 / 172 || loss: 21.811099308232468\n",
      "Epoch:  24 || Seq: 140 / 172 || loss: 14.637972067912212\n",
      "Epoch:  24 || Seq: 150 / 172 || loss: 127.4549671428071\n",
      "Epoch:  24 || Seq: 160 / 172 || loss: 6.505305302683264\n",
      "Epoch:  24 || Seq: 170 / 172 || loss: 56.60800534465279\n",
      "Epoch:  24 || Loss:  55.16007336299368\n",
      "Epoch:  25 || Seq: 0 / 172 || loss: 32.005426758900285\n",
      "Epoch:  25 || Seq: 10 / 172 || loss: 94.18437296517871\n",
      "Epoch:  25 || Seq: 20 / 172 || loss: 13.672339055554144\n",
      "Epoch:  25 || Seq: 30 / 172 || loss: 35.01757357249347\n",
      "Epoch:  25 || Seq: 40 / 172 || loss: 204.40557217898055\n",
      "Epoch:  25 || Seq: 50 / 172 || loss: 66.05166216753423\n",
      "Epoch:  25 || Seq: 60 / 172 || loss: 24.25774572176092\n",
      "Epoch:  25 || Seq: 70 / 172 || loss: 5.513688050750976\n",
      "Epoch:  25 || Seq: 80 / 172 || loss: 26.135975546015786\n",
      "Epoch:  25 || Seq: 90 / 172 || loss: 38.994255701603834\n",
      "Epoch:  25 || Seq: 100 / 172 || loss: 167.58262027226962\n",
      "Epoch:  25 || Seq: 110 / 172 || loss: 3.820157443656196\n",
      "Epoch:  25 || Seq: 120 / 172 || loss: 22.1449488770207\n",
      "Epoch:  25 || Seq: 130 / 172 || loss: 179.99000725285345\n",
      "Epoch:  25 || Seq: 140 / 172 || loss: 12.232719887029962\n",
      "Epoch:  25 || Seq: 150 / 172 || loss: 40.8043722756207\n",
      "Epoch:  25 || Seq: 160 / 172 || loss: 145.721385627985\n",
      "Epoch:  25 || Seq: 170 / 172 || loss: 125.04916130171881\n",
      "Epoch:  25 || Loss:  60.60756812223967\n",
      "Epoch:  26 || Seq: 0 / 172 || loss: 103.3559087444058\n",
      "Epoch:  26 || Seq: 10 / 172 || loss: 20.53052707617947\n",
      "Epoch:  26 || Seq: 20 / 172 || loss: 118.88884854922071\n",
      "Epoch:  26 || Seq: 30 / 172 || loss: 2.33857568388619\n",
      "Epoch:  26 || Seq: 40 / 172 || loss: 6.034803340584039\n",
      "Epoch:  26 || Seq: 50 / 172 || loss: 23.446479467784656\n",
      "Epoch:  26 || Seq: 60 / 172 || loss: 12.918060660362244\n",
      "Epoch:  26 || Seq: 70 / 172 || loss: 120.70380395572437\n",
      "Epoch:  26 || Seq: 80 / 172 || loss: 24.60488672973588\n",
      "Epoch:  26 || Seq: 90 / 172 || loss: 49.220764408508934\n",
      "Epoch:  26 || Seq: 100 / 172 || loss: 7.850733659385393\n",
      "Epoch:  26 || Seq: 110 / 172 || loss: 117.29197240943232\n",
      "Epoch:  26 || Seq: 120 / 172 || loss: 20.708679246910226\n",
      "Epoch:  26 || Seq: 130 / 172 || loss: 17.61364287432904\n",
      "Epoch:  26 || Seq: 140 / 172 || loss: 209.78426874309778\n",
      "Epoch:  26 || Seq: 150 / 172 || loss: 228.13651734929192\n",
      "Epoch:  26 || Seq: 160 / 172 || loss: 19.43643402989902\n",
      "Epoch:  26 || Seq: 170 / 172 || loss: 38.95470500762375\n",
      "Epoch:  26 || Loss:  52.38015216564266\n",
      "Epoch:  27 || Seq: 0 / 172 || loss: 113.12874414433132\n",
      "Epoch:  27 || Seq: 10 / 172 || loss: 42.195390962623954\n",
      "Epoch:  27 || Seq: 20 / 172 || loss: 96.65897482862839\n",
      "Epoch:  27 || Seq: 30 / 172 || loss: 20.282535930474598\n",
      "Epoch:  27 || Seq: 40 / 172 || loss: 2.311690286600164\n",
      "Epoch:  27 || Seq: 50 / 172 || loss: 31.951169549616882\n",
      "Epoch:  27 || Seq: 60 / 172 || loss: 31.44418910930031\n",
      "Epoch:  27 || Seq: 70 / 172 || loss: 34.52612517969239\n",
      "Epoch:  27 || Seq: 80 / 172 || loss: 179.64990189172593\n",
      "Epoch:  27 || Seq: 90 / 172 || loss: 16.191235155091686\n",
      "Epoch:  27 || Seq: 100 / 172 || loss: 22.829804551644393\n",
      "Epoch:  27 || Seq: 110 / 172 || loss: 3.614932652727208\n",
      "Epoch:  27 || Seq: 120 / 172 || loss: 8.346579636790251\n",
      "Epoch:  27 || Seq: 130 / 172 || loss: 50.0952193220146\n",
      "Epoch:  27 || Seq: 140 / 172 || loss: 86.8600890568768\n",
      "Epoch:  27 || Seq: 150 / 172 || loss: 86.09484588727355\n",
      "Epoch:  27 || Seq: 160 / 172 || loss: 48.84963816404343\n",
      "Epoch:  27 || Seq: 170 / 172 || loss: 171.75737379993447\n",
      "Epoch:  27 || Loss:  60.34833758207751\n",
      "Epoch:  28 || Seq: 0 / 172 || loss: 8.271237263231717\n",
      "Epoch:  28 || Seq: 10 / 172 || loss: 74.31149185883164\n",
      "Epoch:  28 || Seq: 20 / 172 || loss: 1.9467913395824976\n",
      "Epoch:  28 || Seq: 30 / 172 || loss: 14.395517706350589\n",
      "Epoch:  28 || Seq: 40 / 172 || loss: 194.59601593915033\n",
      "Epoch:  28 || Seq: 50 / 172 || loss: 77.28153561567888\n",
      "Epoch:  28 || Seq: 60 / 172 || loss: 76.61977097971248\n",
      "Epoch:  28 || Seq: 70 / 172 || loss: 64.55509479811008\n",
      "Epoch:  28 || Seq: 80 / 172 || loss: 42.20789865156015\n",
      "Epoch:  28 || Seq: 90 / 172 || loss: 183.51991918157128\n",
      "Epoch:  28 || Seq: 100 / 172 || loss: 28.234971598114658\n",
      "Epoch:  28 || Seq: 110 / 172 || loss: 33.5703980811162\n",
      "Epoch:  28 || Seq: 120 / 172 || loss: 208.78078422427177\n",
      "Epoch:  28 || Seq: 130 / 172 || loss: 84.14799367729574\n",
      "Epoch:  28 || Seq: 140 / 172 || loss: 29.06879818109446\n",
      "Epoch:  28 || Seq: 150 / 172 || loss: 53.010430758818984\n",
      "Epoch:  28 || Seq: 160 / 172 || loss: 17.963689282536507\n",
      "Epoch:  28 || Seq: 170 / 172 || loss: 6.52028264204526\n",
      "Epoch:  28 || Loss:  45.67444407144339\n",
      "Epoch:  29 || Seq: 0 / 172 || loss: 75.63481346406043\n",
      "Epoch:  29 || Seq: 10 / 172 || loss: 28.84356301504633\n",
      "Epoch:  29 || Seq: 20 / 172 || loss: 1.8461652458287203\n",
      "Epoch:  29 || Seq: 30 / 172 || loss: 28.239281383903673\n",
      "Epoch:  29 || Seq: 40 / 172 || loss: 23.662147996380277\n",
      "Epoch:  29 || Seq: 50 / 172 || loss: 50.17413588328054\n",
      "Epoch:  29 || Seq: 60 / 172 || loss: 17.772066296023482\n",
      "Epoch:  29 || Seq: 70 / 172 || loss: 9.751124368980527\n",
      "Epoch:  29 || Seq: 80 / 172 || loss: 56.85928091136828\n",
      "Epoch:  29 || Seq: 90 / 172 || loss: 30.3175924369134\n",
      "Epoch:  29 || Seq: 100 / 172 || loss: 7.8197369055124\n",
      "Epoch:  29 || Seq: 110 / 172 || loss: 11.960527153359726\n",
      "Epoch:  29 || Seq: 120 / 172 || loss: 53.314374669955214\n",
      "Epoch:  29 || Seq: 130 / 172 || loss: 99.18995802104473\n",
      "Epoch:  29 || Seq: 140 / 172 || loss: 10.814110671480497\n",
      "Epoch:  29 || Seq: 150 / 172 || loss: 24.737504212547922\n",
      "Epoch:  29 || Seq: 160 / 172 || loss: 77.34620607693991\n",
      "Epoch:  29 || Seq: 170 / 172 || loss: 136.02021576728552\n",
      "Epoch:  29 || Loss:  46.900894265325725\n",
      "Change learning rate to:  0.0001\n",
      "Epoch:  30 || Seq: 0 / 172 || loss: 47.147846824244446\n",
      "Epoch:  30 || Seq: 10 / 172 || loss: 215.22283228793566\n",
      "Epoch:  30 || Seq: 20 / 172 || loss: 15.127520406947417\n",
      "Epoch:  30 || Seq: 30 / 172 || loss: 190.41456385010056\n",
      "Epoch:  30 || Seq: 40 / 172 || loss: 68.81861106554668\n",
      "Epoch:  30 || Seq: 50 / 172 || loss: 18.74774560928345\n",
      "Epoch:  30 || Seq: 60 / 172 || loss: 105.45934381363568\n",
      "Epoch:  30 || Seq: 70 / 172 || loss: 92.78809131212209\n",
      "Epoch:  30 || Seq: 80 / 172 || loss: 84.29166863030858\n",
      "Epoch:  30 || Seq: 90 / 172 || loss: 92.64165403621811\n",
      "Epoch:  30 || Seq: 100 / 172 || loss: 93.33017869429155\n",
      "Epoch:  30 || Seq: 110 / 172 || loss: 95.86259815621783\n",
      "Epoch:  30 || Seq: 120 / 172 || loss: 204.9085915905237\n",
      "Epoch:  30 || Seq: 130 / 172 || loss: 63.36619335730319\n",
      "Epoch:  30 || Seq: 140 / 172 || loss: 133.4534671153128\n",
      "Epoch:  30 || Seq: 150 / 172 || loss: 153.94366930325825\n",
      "Epoch:  30 || Seq: 160 / 172 || loss: 299.6127751668294\n",
      "Epoch:  30 || Seq: 170 / 172 || loss: 295.86473605507297\n",
      "Epoch:  30 || Loss:  144.51796620509324\n",
      "Epoch:  31 || Seq: 0 / 172 || loss: 96.38360806107521\n",
      "Epoch:  31 || Seq: 10 / 172 || loss: 49.62241005133838\n",
      "Epoch:  31 || Seq: 20 / 172 || loss: 119.46221691006104\n",
      "Epoch:  31 || Seq: 30 / 172 || loss: 61.50061030486332\n",
      "Epoch:  31 || Seq: 40 / 172 || loss: 248.01584191549392\n",
      "Epoch:  31 || Seq: 50 / 172 || loss: 135.24575654097967\n",
      "Epoch:  31 || Seq: 60 / 172 || loss: 16.287569493055344\n",
      "Epoch:  31 || Seq: 70 / 172 || loss: 43.570415090918544\n",
      "Epoch:  31 || Seq: 80 / 172 || loss: 46.60814280106741\n",
      "Epoch:  31 || Seq: 90 / 172 || loss: 254.7273596790102\n",
      "Epoch:  31 || Seq: 100 / 172 || loss: 247.56382162027336\n",
      "Epoch:  31 || Seq: 110 / 172 || loss: 107.86555897949204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  31 || Seq: 120 / 172 || loss: 95.59677578852727\n",
      "Epoch:  31 || Seq: 130 / 172 || loss: 269.45021842916805\n",
      "Epoch:  31 || Seq: 140 / 172 || loss: 108.71424359083176\n",
      "Epoch:  31 || Seq: 150 / 172 || loss: 17.877661092281343\n",
      "Epoch:  31 || Seq: 160 / 172 || loss: 252.28568444532507\n",
      "Epoch:  31 || Seq: 170 / 172 || loss: 310.4108480724196\n",
      "Epoch:  31 || Loss:  128.70066979076483\n",
      "Epoch:  32 || Seq: 0 / 172 || loss: 463.13542646376624\n",
      "Epoch:  32 || Seq: 10 / 172 || loss: 197.18602628077497\n",
      "Epoch:  32 || Seq: 20 / 172 || loss: 5.080745633544498\n",
      "Epoch:  32 || Seq: 30 / 172 || loss: 106.14952096730471\n",
      "Epoch:  32 || Seq: 40 / 172 || loss: 138.83884070813656\n",
      "Epoch:  32 || Seq: 50 / 172 || loss: 187.35451565867987\n",
      "Epoch:  32 || Seq: 60 / 172 || loss: 86.3153649220864\n",
      "Epoch:  32 || Seq: 70 / 172 || loss: 6.017449736595154\n",
      "Epoch:  32 || Seq: 80 / 172 || loss: 228.3295732246091\n",
      "Epoch:  32 || Seq: 90 / 172 || loss: 129.06849628145045\n",
      "Epoch:  32 || Seq: 100 / 172 || loss: 201.7995296198389\n",
      "Epoch:  32 || Seq: 110 / 172 || loss: 71.4935138092979\n",
      "Epoch:  32 || Seq: 120 / 172 || loss: 89.81259150002887\n",
      "Epoch:  32 || Seq: 130 / 172 || loss: 59.24601960513327\n",
      "Epoch:  32 || Seq: 140 / 172 || loss: 15.97127858797709\n",
      "Epoch:  32 || Seq: 150 / 172 || loss: 135.614019073141\n",
      "Epoch:  32 || Seq: 160 / 172 || loss: 284.578019229571\n",
      "Epoch:  32 || Seq: 170 / 172 || loss: 22.870009215054807\n",
      "Epoch:  32 || Loss:  121.73989938016834\n",
      "Epoch:  33 || Seq: 0 / 172 || loss: 120.90584213000078\n",
      "Epoch:  33 || Seq: 10 / 172 || loss: 79.64294141217282\n",
      "Epoch:  33 || Seq: 20 / 172 || loss: 374.611763381958\n",
      "Epoch:  33 || Seq: 30 / 172 || loss: 165.22683092422733\n",
      "Epoch:  33 || Seq: 40 / 172 || loss: 37.50862366536801\n",
      "Epoch:  33 || Seq: 50 / 172 || loss: 16.888666981144954\n",
      "Epoch:  33 || Seq: 60 / 172 || loss: 49.03693005152874\n",
      "Epoch:  33 || Seq: 70 / 172 || loss: 116.81848384857177\n",
      "Epoch:  33 || Seq: 80 / 172 || loss: 49.26706905066967\n",
      "Epoch:  33 || Seq: 90 / 172 || loss: 618.3746417252266\n",
      "Epoch:  33 || Seq: 100 / 172 || loss: 243.53938519403337\n",
      "Epoch:  33 || Seq: 110 / 172 || loss: 9.24134400971234\n",
      "Epoch:  33 || Seq: 120 / 172 || loss: 374.2195673855868\n",
      "Epoch:  33 || Seq: 130 / 172 || loss: 73.7635897227696\n",
      "Epoch:  33 || Seq: 140 / 172 || loss: 159.94340115010738\n",
      "Epoch:  33 || Seq: 150 / 172 || loss: 304.7524074264444\n",
      "Epoch:  33 || Seq: 160 / 172 || loss: 33.12796014282973\n",
      "Epoch:  33 || Seq: 170 / 172 || loss: 28.039926083464373\n",
      "Epoch:  33 || Loss:  120.63749147578181\n",
      "Epoch:  34 || Seq: 0 / 172 || loss: 76.18263902832513\n",
      "Epoch:  34 || Seq: 10 / 172 || loss: 159.38805074124213\n",
      "Epoch:  34 || Seq: 20 / 172 || loss: 296.96746707447573\n",
      "Epoch:  34 || Seq: 30 / 172 || loss: 103.42649117821739\n",
      "Epoch:  34 || Seq: 40 / 172 || loss: 12.808837965219503\n",
      "Epoch:  34 || Seq: 50 / 172 || loss: 16.240682559688725\n",
      "Epoch:  34 || Seq: 60 / 172 || loss: 13.00275125708431\n",
      "Epoch:  34 || Seq: 70 / 172 || loss: 56.079343469519365\n",
      "Epoch:  34 || Seq: 80 / 172 || loss: 219.46389110386372\n",
      "Epoch:  34 || Seq: 90 / 172 || loss: 126.03313408978283\n",
      "Epoch:  34 || Seq: 100 / 172 || loss: 127.88585489598059\n",
      "Epoch:  34 || Seq: 110 / 172 || loss: 311.05509473383427\n",
      "Epoch:  34 || Seq: 120 / 172 || loss: 249.02383628258337\n",
      "Epoch:  34 || Seq: 130 / 172 || loss: 173.62051081458728\n",
      "Epoch:  34 || Seq: 140 / 172 || loss: 225.45575917910747\n",
      "Epoch:  34 || Seq: 150 / 172 || loss: 40.92652950304396\n",
      "Epoch:  34 || Seq: 160 / 172 || loss: 180.07945465110242\n",
      "Epoch:  34 || Seq: 170 / 172 || loss: 26.37673486603631\n",
      "Epoch:  34 || Loss:  117.2350498895858\n",
      "Epoch:  35 || Seq: 0 / 172 || loss: 111.52719066017552\n",
      "Epoch:  35 || Seq: 10 / 172 || loss: 37.91771224141121\n",
      "Epoch:  35 || Seq: 20 / 172 || loss: 44.70987056270242\n",
      "Epoch:  35 || Seq: 30 / 172 || loss: 316.9023154631355\n",
      "Epoch:  35 || Seq: 40 / 172 || loss: 18.318994430819572\n",
      "Epoch:  35 || Seq: 50 / 172 || loss: 34.30706621067865\n",
      "Epoch:  35 || Seq: 60 / 172 || loss: 22.304589122533798\n",
      "Epoch:  35 || Seq: 70 / 172 || loss: 40.921380838880744\n",
      "Epoch:  35 || Seq: 80 / 172 || loss: 40.157362113097825\n",
      "Epoch:  35 || Seq: 90 / 172 || loss: 32.52212244898081\n",
      "Epoch:  35 || Seq: 100 / 172 || loss: 213.40389748623497\n",
      "Epoch:  35 || Seq: 110 / 172 || loss: 10.529634929838634\n",
      "Epoch:  35 || Seq: 120 / 172 || loss: 230.61157239278157\n",
      "Epoch:  35 || Seq: 130 / 172 || loss: 178.66368306384368\n",
      "Epoch:  35 || Seq: 140 / 172 || loss: 207.71290968764913\n",
      "Epoch:  35 || Seq: 150 / 172 || loss: 256.4174479675293\n",
      "Epoch:  35 || Seq: 160 / 172 || loss: 154.17883553036623\n",
      "Epoch:  35 || Seq: 170 / 172 || loss: 514.3662643432617\n",
      "Epoch:  35 || Loss:  118.73109384682131\n",
      "Epoch:  36 || Seq: 0 / 172 || loss: 24.27268460392952\n",
      "Epoch:  36 || Seq: 10 / 172 || loss: 158.3527915704818\n",
      "Epoch:  36 || Seq: 20 / 172 || loss: 227.78613149158826\n",
      "Epoch:  36 || Seq: 30 / 172 || loss: 140.1745663748847\n",
      "Epoch:  36 || Seq: 40 / 172 || loss: 200.2630754449591\n",
      "Epoch:  36 || Seq: 50 / 172 || loss: 29.62637689709382\n",
      "Epoch:  36 || Seq: 60 / 172 || loss: 10.415429021505748\n",
      "Epoch:  36 || Seq: 70 / 172 || loss: 194.23834496677705\n",
      "Epoch:  36 || Seq: 80 / 172 || loss: 80.72230257262444\n",
      "Epoch:  36 || Seq: 90 / 172 || loss: 86.74553946779403\n",
      "Epoch:  36 || Seq: 100 / 172 || loss: 58.592012849403545\n",
      "Epoch:  36 || Seq: 110 / 172 || loss: 14.848451184599023\n",
      "Epoch:  36 || Seq: 120 / 172 || loss: 40.885249855437955\n",
      "Epoch:  36 || Seq: 130 / 172 || loss: 265.7959340174993\n",
      "Epoch:  36 || Seq: 140 / 172 || loss: 51.325558503468834\n",
      "Epoch:  36 || Seq: 150 / 172 || loss: 113.81227521101634\n",
      "Epoch:  36 || Seq: 160 / 172 || loss: 381.26760277648765\n",
      "Epoch:  36 || Seq: 170 / 172 || loss: 115.20309036813285\n",
      "Epoch:  36 || Loss:  123.62669316736597\n",
      "Epoch:  37 || Seq: 0 / 172 || loss: 146.05419985949993\n",
      "Epoch:  37 || Seq: 10 / 172 || loss: 122.77711855489761\n",
      "Epoch:  37 || Seq: 20 / 172 || loss: 222.5960176116542\n",
      "Epoch:  37 || Seq: 30 / 172 || loss: 80.89081866045792\n",
      "Epoch:  37 || Seq: 40 / 172 || loss: 94.33790141804144\n",
      "Epoch:  37 || Seq: 50 / 172 || loss: 81.4659259769891\n",
      "Epoch:  37 || Seq: 60 / 172 || loss: 28.108452104777097\n",
      "Epoch:  37 || Seq: 70 / 172 || loss: 81.00443759153131\n",
      "Epoch:  37 || Seq: 80 / 172 || loss: 317.5377886707061\n",
      "Epoch:  37 || Seq: 90 / 172 || loss: 88.97544575765642\n",
      "Epoch:  37 || Seq: 100 / 172 || loss: 26.58532026410103\n",
      "Epoch:  37 || Seq: 110 / 172 || loss: 459.8083370108354\n",
      "Epoch:  37 || Seq: 120 / 172 || loss: 461.18401690533284\n",
      "Epoch:  37 || Seq: 130 / 172 || loss: 57.356983608319695\n",
      "Epoch:  37 || Seq: 140 / 172 || loss: 103.90609747264534\n",
      "Epoch:  37 || Seq: 150 / 172 || loss: 136.00044387403656\n",
      "Epoch:  37 || Seq: 160 / 172 || loss: 165.50190347929797\n",
      "Epoch:  37 || Seq: 170 / 172 || loss: 159.4296906877102\n",
      "Epoch:  37 || Loss:  122.67921104660576\n",
      "Epoch:  38 || Seq: 0 / 172 || loss: 252.784208997488\n",
      "Epoch:  38 || Seq: 10 / 172 || loss: 142.53243212950858\n",
      "Epoch:  38 || Seq: 20 / 172 || loss: 18.980418626083218\n",
      "Epoch:  38 || Seq: 30 / 172 || loss: 18.857530622255233\n",
      "Epoch:  38 || Seq: 40 / 172 || loss: 18.780713842673734\n",
      "Epoch:  38 || Seq: 50 / 172 || loss: 14.854339686739776\n",
      "Epoch:  38 || Seq: 60 / 172 || loss: 117.8680895805359\n",
      "Epoch:  38 || Seq: 70 / 172 || loss: 14.935375839471817\n",
      "Epoch:  38 || Seq: 80 / 172 || loss: 150.46060746245914\n",
      "Epoch:  38 || Seq: 90 / 172 || loss: 166.15401225931504\n",
      "Epoch:  38 || Seq: 100 / 172 || loss: 113.02902323775925\n",
      "Epoch:  38 || Seq: 110 / 172 || loss: 83.98374509438872\n",
      "Epoch:  38 || Seq: 120 / 172 || loss: 174.17222131726643\n",
      "Epoch:  38 || Seq: 130 / 172 || loss: 44.768759114046894\n",
      "Epoch:  38 || Seq: 140 / 172 || loss: 15.166135207033502\n",
      "Epoch:  38 || Seq: 150 / 172 || loss: 12.512905226508156\n",
      "Epoch:  38 || Seq: 160 / 172 || loss: 7.272376914819081\n",
      "Epoch:  38 || Seq: 170 / 172 || loss: 81.50234662244718\n",
      "Epoch:  38 || Loss:  122.8176112295515\n",
      "Epoch:  39 || Seq: 0 / 172 || loss: 83.02326736599207\n",
      "Epoch:  39 || Seq: 10 / 172 || loss: 12.409960414354618\n",
      "Epoch:  39 || Seq: 20 / 172 || loss: 9.126630152933872\n",
      "Epoch:  39 || Seq: 30 / 172 || loss: 67.34973490638927\n",
      "Epoch:  39 || Seq: 40 / 172 || loss: 47.82318584124247\n",
      "Epoch:  39 || Seq: 50 / 172 || loss: 57.857542754538976\n",
      "Epoch:  39 || Seq: 60 / 172 || loss: 146.39659348302163\n",
      "Epoch:  39 || Seq: 70 / 172 || loss: 42.7597024012357\n",
      "Epoch:  39 || Seq: 80 / 172 || loss: 215.11274210024965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  39 || Seq: 90 / 172 || loss: 66.87870294907513\n",
      "Epoch:  39 || Seq: 100 / 172 || loss: 492.3956649303436\n",
      "Epoch:  39 || Seq: 110 / 172 || loss: 114.88773877038197\n",
      "Epoch:  39 || Seq: 120 / 172 || loss: 385.8568552652995\n",
      "Epoch:  39 || Seq: 130 / 172 || loss: 77.54468310624361\n",
      "Epoch:  39 || Seq: 140 / 172 || loss: 285.37482215881346\n",
      "Epoch:  39 || Seq: 150 / 172 || loss: 170.53955546669337\n",
      "Epoch:  39 || Seq: 160 / 172 || loss: 93.78429665932289\n",
      "Epoch:  39 || Seq: 170 / 172 || loss: 47.13914577121084\n",
      "Epoch:  39 || Loss:  124.74534070944902\n",
      "Epoch:  40 || Seq: 0 / 172 || loss: 242.2550914446513\n",
      "Epoch:  40 || Seq: 10 / 172 || loss: 0.5584374569857923\n",
      "Epoch:  40 || Seq: 20 / 172 || loss: 160.58921556819405\n",
      "Epoch:  40 || Seq: 30 / 172 || loss: 235.13365434741135\n",
      "Epoch:  40 || Seq: 40 / 172 || loss: 88.9961458183825\n",
      "Epoch:  40 || Seq: 50 / 172 || loss: 80.99098526224493\n",
      "Epoch:  40 || Seq: 60 / 172 || loss: 74.11944997467492\n",
      "Epoch:  40 || Seq: 70 / 172 || loss: 66.42134241517633\n",
      "Epoch:  40 || Seq: 80 / 172 || loss: 107.79359674065009\n",
      "Epoch:  40 || Seq: 90 / 172 || loss: 111.69995530535068\n",
      "Epoch:  40 || Seq: 100 / 172 || loss: 28.084723600511143\n",
      "Epoch:  40 || Seq: 110 / 172 || loss: 49.270424326615675\n",
      "Epoch:  40 || Seq: 120 / 172 || loss: 222.09981714064875\n",
      "Epoch:  40 || Seq: 130 / 172 || loss: 59.31348443031311\n",
      "Epoch:  40 || Seq: 140 / 172 || loss: 25.108863262217103\n",
      "Epoch:  40 || Seq: 150 / 172 || loss: 176.47301641339436\n",
      "Epoch:  40 || Seq: 160 / 172 || loss: 155.1792921016091\n",
      "Epoch:  40 || Seq: 170 / 172 || loss: 28.69605639576912\n",
      "Epoch:  40 || Loss:  120.98052476944564\n",
      "Epoch:  41 || Seq: 0 / 172 || loss: 21.805343937451163\n",
      "Epoch:  41 || Seq: 10 / 172 || loss: 71.87657437117204\n",
      "Epoch:  41 || Seq: 20 / 172 || loss: 20.38158418238163\n",
      "Epoch:  41 || Seq: 30 / 172 || loss: 389.8980451311384\n",
      "Epoch:  41 || Seq: 40 / 172 || loss: 214.27466445314613\n",
      "Epoch:  41 || Seq: 50 / 172 || loss: 33.32015681266785\n",
      "Epoch:  41 || Seq: 60 / 172 || loss: 282.7586365558884\n",
      "Epoch:  41 || Seq: 70 / 172 || loss: 390.98380185365676\n",
      "Epoch:  41 || Seq: 80 / 172 || loss: 175.52100662958054\n",
      "Epoch:  41 || Seq: 90 / 172 || loss: 76.77879071777517\n",
      "Epoch:  41 || Seq: 100 / 172 || loss: 359.61125909619864\n",
      "Epoch:  41 || Seq: 110 / 172 || loss: 33.459516738590445\n",
      "Epoch:  41 || Seq: 120 / 172 || loss: 700.0225297361612\n",
      "Epoch:  41 || Seq: 130 / 172 || loss: 138.3100101294426\n",
      "Epoch:  41 || Seq: 140 / 172 || loss: 14.353275143106778\n",
      "Epoch:  41 || Seq: 150 / 172 || loss: 80.29438526585699\n",
      "Epoch:  41 || Seq: 160 / 172 || loss: 93.35360120236874\n",
      "Epoch:  41 || Seq: 170 / 172 || loss: 20.635648618851388\n",
      "Epoch:  41 || Loss:  126.31336167949965\n",
      "Epoch:  42 || Seq: 0 / 172 || loss: 120.24062651615928\n",
      "Epoch:  42 || Seq: 10 / 172 || loss: 324.14009806513786\n",
      "Epoch:  42 || Seq: 20 / 172 || loss: 145.42722107143334\n",
      "Epoch:  42 || Seq: 30 / 172 || loss: 486.0234425876459\n",
      "Epoch:  42 || Seq: 40 / 172 || loss: 83.82716788294223\n",
      "Epoch:  42 || Seq: 50 / 172 || loss: 10.945134584720318\n",
      "Epoch:  42 || Seq: 60 / 172 || loss: 28.014368363966543\n",
      "Epoch:  42 || Seq: 70 / 172 || loss: 75.36520543988598\n",
      "Epoch:  42 || Seq: 80 / 172 || loss: 535.9435831677655\n",
      "Epoch:  42 || Seq: 90 / 172 || loss: 44.199318648530884\n",
      "Epoch:  42 || Seq: 100 / 172 || loss: 67.79116756275847\n",
      "Epoch:  42 || Seq: 110 / 172 || loss: 201.094129263858\n",
      "Epoch:  42 || Seq: 120 / 172 || loss: 19.561032480918445\n",
      "Epoch:  42 || Seq: 130 / 172 || loss: 70.15607951691045\n",
      "Epoch:  42 || Seq: 140 / 172 || loss: 223.1881421130489\n",
      "Epoch:  42 || Seq: 150 / 172 || loss: 109.5678449604999\n",
      "Epoch:  42 || Seq: 160 / 172 || loss: 392.4421631882588\n",
      "Epoch:  42 || Seq: 170 / 172 || loss: 43.80550454440527\n",
      "Epoch:  42 || Loss:  120.78720732140317\n",
      "Epoch:  43 || Seq: 0 / 172 || loss: 66.3973348516005\n",
      "Epoch:  43 || Seq: 10 / 172 || loss: 33.93544348329306\n",
      "Epoch:  43 || Seq: 20 / 172 || loss: 83.52038118839263\n",
      "Epoch:  43 || Seq: 30 / 172 || loss: 187.3195142467739\n",
      "Epoch:  43 || Seq: 40 / 172 || loss: 90.1351590116339\n",
      "Epoch:  43 || Seq: 50 / 172 || loss: 177.9554232453045\n",
      "Epoch:  43 || Seq: 60 / 172 || loss: 58.52890825876966\n",
      "Epoch:  43 || Seq: 70 / 172 || loss: 98.43244316168129\n",
      "Epoch:  43 || Seq: 80 / 172 || loss: 50.552597111179715\n",
      "Epoch:  43 || Seq: 90 / 172 || loss: 214.31151310469096\n",
      "Epoch:  43 || Seq: 100 / 172 || loss: 217.9958575986899\n",
      "Epoch:  43 || Seq: 110 / 172 || loss: 170.54714846160883\n",
      "Epoch:  43 || Seq: 120 / 172 || loss: 32.71276318131444\n",
      "Epoch:  43 || Seq: 130 / 172 || loss: 208.31749077983525\n",
      "Epoch:  43 || Seq: 140 / 172 || loss: 307.4453365965323\n",
      "Epoch:  43 || Seq: 150 / 172 || loss: 179.00087659444512\n",
      "Epoch:  43 || Seq: 160 / 172 || loss: 160.60471961611793\n",
      "Epoch:  43 || Seq: 170 / 172 || loss: 146.6602847445961\n",
      "Epoch:  43 || Loss:  126.82928929778708\n",
      "Epoch:  44 || Seq: 0 / 172 || loss: 108.66866289941888\n",
      "Epoch:  44 || Seq: 10 / 172 || loss: 108.08245173641932\n",
      "Epoch:  44 || Seq: 20 / 172 || loss: 31.770660188637283\n",
      "Epoch:  44 || Seq: 30 / 172 || loss: 440.47475440979\n",
      "Epoch:  44 || Seq: 40 / 172 || loss: 290.29736996650695\n",
      "Epoch:  44 || Seq: 50 / 172 || loss: 170.59323151906332\n",
      "Epoch:  44 || Seq: 60 / 172 || loss: 52.5820833155756\n",
      "Epoch:  44 || Seq: 70 / 172 || loss: 53.38944634395953\n",
      "Epoch:  44 || Seq: 80 / 172 || loss: 83.90273408464273\n",
      "Epoch:  44 || Seq: 90 / 172 || loss: 47.609696162864566\n",
      "Epoch:  44 || Seq: 100 / 172 || loss: 346.34791204333305\n",
      "Epoch:  44 || Seq: 110 / 172 || loss: 44.06577531029196\n",
      "Epoch:  44 || Seq: 120 / 172 || loss: 199.08979933857918\n",
      "Epoch:  44 || Seq: 130 / 172 || loss: 203.1482004808343\n",
      "Epoch:  44 || Seq: 140 / 172 || loss: 103.18967553816344\n",
      "Epoch:  44 || Seq: 150 / 172 || loss: 17.02748839922536\n",
      "Epoch:  44 || Seq: 160 / 172 || loss: 66.17725840567859\n",
      "Epoch:  44 || Seq: 170 / 172 || loss: 125.52957973410102\n",
      "Epoch:  44 || Loss:  137.46801254977754\n",
      "Epoch:  45 || Seq: 0 / 172 || loss: 163.70940563366526\n",
      "Epoch:  45 || Seq: 10 / 172 || loss: 141.03508883714676\n",
      "Epoch:  45 || Seq: 20 / 172 || loss: 162.11086935922503\n",
      "Epoch:  45 || Seq: 30 / 172 || loss: 428.21902084350586\n",
      "Epoch:  45 || Seq: 40 / 172 || loss: 19.014045407774393\n",
      "Epoch:  45 || Seq: 50 / 172 || loss: 174.91243951022625\n",
      "Epoch:  45 || Seq: 60 / 172 || loss: 28.938553217070876\n",
      "Epoch:  45 || Seq: 70 / 172 || loss: 16.113934215158224\n",
      "Epoch:  45 || Seq: 80 / 172 || loss: 318.4897828809917\n",
      "Epoch:  45 || Seq: 90 / 172 || loss: 44.90730123656491\n",
      "Epoch:  45 || Seq: 100 / 172 || loss: 268.4337164627181\n",
      "Epoch:  45 || Seq: 110 / 172 || loss: 157.28682046857747\n",
      "Epoch:  45 || Seq: 120 / 172 || loss: 62.03466926493483\n",
      "Epoch:  45 || Seq: 130 / 172 || loss: 66.15804263513536\n",
      "Epoch:  45 || Seq: 140 / 172 || loss: 107.61654054734015\n",
      "Epoch:  45 || Seq: 150 / 172 || loss: 16.74840914408366\n",
      "Epoch:  45 || Seq: 160 / 172 || loss: 35.233172564279464\n",
      "Epoch:  45 || Seq: 170 / 172 || loss: 11.119599312405626\n",
      "Epoch:  45 || Loss:  123.09946035255047\n",
      "Epoch:  46 || Seq: 0 / 172 || loss: 4.582029573153704\n",
      "Epoch:  46 || Seq: 10 / 172 || loss: 150.39835333824158\n",
      "Epoch:  46 || Seq: 20 / 172 || loss: 0.49044842058570154\n",
      "Epoch:  46 || Seq: 30 / 172 || loss: 67.04578465223312\n",
      "Epoch:  46 || Seq: 40 / 172 || loss: 165.84426241322438\n",
      "Epoch:  46 || Seq: 50 / 172 || loss: 259.761341713942\n",
      "Epoch:  46 || Seq: 60 / 172 || loss: 181.9739009275929\n",
      "Epoch:  46 || Seq: 70 / 172 || loss: 66.9869220495224\n",
      "Epoch:  46 || Seq: 80 / 172 || loss: 57.57065210863948\n",
      "Epoch:  46 || Seq: 90 / 172 || loss: 17.178396847512985\n",
      "Epoch:  46 || Seq: 100 / 172 || loss: 0.588521575050739\n",
      "Epoch:  46 || Seq: 110 / 172 || loss: 118.90134379699042\n",
      "Epoch:  46 || Seq: 120 / 172 || loss: 267.53906378149986\n",
      "Epoch:  46 || Seq: 130 / 172 || loss: 22.285589052446404\n",
      "Epoch:  46 || Seq: 140 / 172 || loss: 61.9194868746258\n",
      "Epoch:  46 || Seq: 150 / 172 || loss: 114.53076061449553\n",
      "Epoch:  46 || Seq: 160 / 172 || loss: 41.31674540042877\n",
      "Epoch:  46 || Seq: 170 / 172 || loss: 28.185504352345188\n",
      "Epoch:  46 || Loss:  114.74154840991866\n",
      "Epoch:  47 || Seq: 0 / 172 || loss: 51.66494827179445\n",
      "Epoch:  47 || Seq: 10 / 172 || loss: 14.746031261980534\n",
      "Epoch:  47 || Seq: 20 / 172 || loss: 20.349786838309633\n",
      "Epoch:  47 || Seq: 30 / 172 || loss: 105.69949462092839\n",
      "Epoch:  47 || Seq: 40 / 172 || loss: 53.92021903513281\n",
      "Epoch:  47 || Seq: 50 / 172 || loss: 85.50574261403602\n",
      "Epoch:  47 || Seq: 60 / 172 || loss: 46.668032423545476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  47 || Seq: 70 / 172 || loss: 320.2104503729691\n",
      "Epoch:  47 || Seq: 80 / 172 || loss: 367.0260797675167\n",
      "Epoch:  47 || Seq: 90 / 172 || loss: 8.319695055950433\n",
      "Epoch:  47 || Seq: 100 / 172 || loss: 4.685702295353015\n",
      "Epoch:  47 || Seq: 110 / 172 || loss: 448.4829124890578\n",
      "Epoch:  47 || Seq: 120 / 172 || loss: 0.9117554935969805\n",
      "Epoch:  47 || Seq: 130 / 172 || loss: 250.90596724847953\n",
      "Epoch:  47 || Seq: 140 / 172 || loss: 148.97902047232276\n",
      "Epoch:  47 || Seq: 150 / 172 || loss: 11.967814959230877\n",
      "Epoch:  47 || Seq: 160 / 172 || loss: 133.97223810548587\n",
      "Epoch:  47 || Seq: 170 / 172 || loss: 143.76444505616868\n",
      "Epoch:  47 || Loss:  115.98713340798926\n",
      "Epoch:  48 || Seq: 0 / 172 || loss: 50.42396341916174\n",
      "Epoch:  48 || Seq: 10 / 172 || loss: 55.74877380980895\n",
      "Epoch:  48 || Seq: 20 / 172 || loss: 368.6011578740459\n",
      "Epoch:  48 || Seq: 30 / 172 || loss: 249.97470801217216\n",
      "Epoch:  48 || Seq: 40 / 172 || loss: 90.76213106665077\n",
      "Epoch:  48 || Seq: 50 / 172 || loss: 2.0778946322388947\n",
      "Epoch:  48 || Seq: 60 / 172 || loss: 20.986119822844078\n",
      "Epoch:  48 || Seq: 70 / 172 || loss: 156.94946972059552\n",
      "Epoch:  48 || Seq: 80 / 172 || loss: 144.5132246091962\n",
      "Epoch:  48 || Seq: 90 / 172 || loss: 165.65407222102988\n",
      "Epoch:  48 || Seq: 100 / 172 || loss: 65.71334933104615\n",
      "Epoch:  48 || Seq: 110 / 172 || loss: 128.92634634776851\n",
      "Epoch:  48 || Seq: 120 / 172 || loss: 13.34845460266045\n",
      "Epoch:  48 || Seq: 130 / 172 || loss: 122.06673850882798\n",
      "Epoch:  48 || Seq: 140 / 172 || loss: 102.71335287197776\n",
      "Epoch:  48 || Seq: 150 / 172 || loss: 79.44650058226571\n",
      "Epoch:  48 || Seq: 160 / 172 || loss: 88.19934358596802\n",
      "Epoch:  48 || Seq: 170 / 172 || loss: 94.84865484454416\n",
      "Epoch:  48 || Loss:  103.07074149848819\n",
      "Epoch:  49 || Seq: 0 / 172 || loss: 78.90545128285885\n",
      "Epoch:  49 || Seq: 10 / 172 || loss: 240.04299350641668\n",
      "Epoch:  49 || Seq: 20 / 172 || loss: 25.76383512963851\n",
      "Epoch:  49 || Seq: 30 / 172 || loss: 20.53599503305223\n",
      "Epoch:  49 || Seq: 40 / 172 || loss: 55.25005599043586\n",
      "Epoch:  49 || Seq: 50 / 172 || loss: 19.232081594447102\n",
      "Epoch:  49 || Seq: 60 / 172 || loss: 46.36363379444395\n",
      "Epoch:  49 || Seq: 70 / 172 || loss: 279.3128971300627\n",
      "Epoch:  49 || Seq: 80 / 172 || loss: 15.238643741323834\n",
      "Epoch:  49 || Seq: 90 / 172 || loss: 28.723642124849206\n",
      "Epoch:  49 || Seq: 100 / 172 || loss: 281.9669730017583\n",
      "Epoch:  49 || Seq: 110 / 172 || loss: 82.95839340860645\n",
      "Epoch:  49 || Seq: 120 / 172 || loss: 137.18742108013896\n",
      "Epoch:  49 || Seq: 130 / 172 || loss: 140.7256052153451\n",
      "Epoch:  49 || Seq: 140 / 172 || loss: 61.258978603588\n",
      "Epoch:  49 || Seq: 150 / 172 || loss: 16.12876491813824\n",
      "Epoch:  49 || Seq: 160 / 172 || loss: 221.0792109159132\n",
      "Epoch:  49 || Seq: 170 / 172 || loss: 39.567328702154995\n",
      "Epoch:  49 || Loss:  113.2553435661328\n",
      "Epoch:  50 || Seq: 0 / 172 || loss: 182.59775727904506\n",
      "Epoch:  50 || Seq: 10 / 172 || loss: 238.6969784507528\n",
      "Epoch:  50 || Seq: 20 / 172 || loss: 516.8757651785146\n",
      "Epoch:  50 || Seq: 30 / 172 || loss: 358.8887628418071\n",
      "Epoch:  50 || Seq: 40 / 172 || loss: 19.52050123218743\n",
      "Epoch:  50 || Seq: 50 / 172 || loss: 105.79137297027877\n",
      "Epoch:  50 || Seq: 60 / 172 || loss: 307.91573631727977\n",
      "Epoch:  50 || Seq: 70 / 172 || loss: 66.69000698328018\n",
      "Epoch:  50 || Seq: 80 / 172 || loss: 19.172191678080708\n",
      "Epoch:  50 || Seq: 90 / 172 || loss: 30.666205386654475\n",
      "Epoch:  50 || Seq: 100 / 172 || loss: 58.10616951613199\n",
      "Epoch:  50 || Seq: 110 / 172 || loss: 160.87589961302376\n",
      "Epoch:  50 || Seq: 120 / 172 || loss: 10.394640539590593\n",
      "Epoch:  50 || Seq: 130 / 172 || loss: 32.835133847780526\n",
      "Epoch:  50 || Seq: 140 / 172 || loss: 130.3209168550869\n",
      "Epoch:  50 || Seq: 150 / 172 || loss: 82.38625933144384\n",
      "Epoch:  50 || Seq: 160 / 172 || loss: 43.61529495780213\n",
      "Epoch:  50 || Seq: 170 / 172 || loss: 421.27762335809797\n",
      "Epoch:  50 || Loss:  117.6897394896417\n",
      "Epoch:  51 || Seq: 0 / 172 || loss: 105.64413906008967\n",
      "Epoch:  51 || Seq: 10 / 172 || loss: 46.76331513840705\n",
      "Epoch:  51 || Seq: 20 / 172 || loss: 63.71716049171629\n",
      "Epoch:  51 || Seq: 30 / 172 || loss: 230.19169378839433\n",
      "Epoch:  51 || Seq: 40 / 172 || loss: 147.35742937841198\n",
      "Epoch:  51 || Seq: 50 / 172 || loss: 133.5305864546034\n",
      "Epoch:  51 || Seq: 60 / 172 || loss: 135.3505053747268\n",
      "Epoch:  51 || Seq: 70 / 172 || loss: 86.70461909961084\n",
      "Epoch:  51 || Seq: 80 / 172 || loss: 90.29967301445348\n",
      "Epoch:  51 || Seq: 90 / 172 || loss: 45.03046729084518\n",
      "Epoch:  51 || Seq: 100 / 172 || loss: 267.6882721128918\n",
      "Epoch:  51 || Seq: 110 / 172 || loss: 100.3793300882956\n",
      "Epoch:  51 || Seq: 120 / 172 || loss: 114.15689713720765\n",
      "Epoch:  51 || Seq: 130 / 172 || loss: 414.8269931568819\n",
      "Epoch:  51 || Seq: 140 / 172 || loss: 75.61771498173475\n",
      "Epoch:  51 || Seq: 150 / 172 || loss: 141.42077747144197\n",
      "Epoch:  51 || Seq: 160 / 172 || loss: 203.44354638180906\n",
      "Epoch:  51 || Seq: 170 / 172 || loss: 57.56512491570579\n",
      "Epoch:  51 || Loss:  112.03573710699364\n",
      "Epoch:  52 || Seq: 0 / 172 || loss: 40.96016726799371\n",
      "Epoch:  52 || Seq: 10 / 172 || loss: 192.52112909949696\n",
      "Epoch:  52 || Seq: 20 / 172 || loss: 20.7497630577821\n",
      "Epoch:  52 || Seq: 30 / 172 || loss: 7.182321005087272\n",
      "Epoch:  52 || Seq: 40 / 172 || loss: 18.017010762503272\n",
      "Epoch:  52 || Seq: 50 / 172 || loss: 30.984548568725586\n",
      "Epoch:  52 || Seq: 60 / 172 || loss: 97.36260871092479\n",
      "Epoch:  52 || Seq: 70 / 172 || loss: 221.97693990441886\n",
      "Epoch:  52 || Seq: 80 / 172 || loss: 240.86238879778168\n",
      "Epoch:  52 || Seq: 90 / 172 || loss: 38.972335408529034\n",
      "Epoch:  52 || Seq: 100 / 172 || loss: 297.0727917417651\n",
      "Epoch:  52 || Seq: 110 / 172 || loss: 47.84169329889119\n",
      "Epoch:  52 || Seq: 120 / 172 || loss: 7.737744649202796\n",
      "Epoch:  52 || Seq: 130 / 172 || loss: 1.1149137274442182\n",
      "Epoch:  52 || Seq: 140 / 172 || loss: 73.13000349132787\n",
      "Epoch:  52 || Seq: 150 / 172 || loss: 229.89708998143948\n",
      "Epoch:  52 || Seq: 160 / 172 || loss: 76.9526660499212\n",
      "Epoch:  52 || Seq: 170 / 172 || loss: 125.72837935738704\n",
      "Epoch:  52 || Loss:  117.73289325290257\n",
      "Epoch:  53 || Seq: 0 / 172 || loss: 43.721842509508136\n",
      "Epoch:  53 || Seq: 10 / 172 || loss: 30.830066363016766\n",
      "Epoch:  53 || Seq: 20 / 172 || loss: 89.28471299012502\n",
      "Epoch:  53 || Seq: 30 / 172 || loss: 17.509994410806232\n",
      "Epoch:  53 || Seq: 40 / 172 || loss: 102.85840088129044\n",
      "Epoch:  53 || Seq: 50 / 172 || loss: 159.82784144982696\n",
      "Epoch:  53 || Seq: 60 / 172 || loss: 306.59172461926937\n",
      "Epoch:  53 || Seq: 70 / 172 || loss: 61.836277103916345\n",
      "Epoch:  53 || Seq: 80 / 172 || loss: 254.9328384399414\n",
      "Epoch:  53 || Seq: 90 / 172 || loss: 18.443147904832255\n",
      "Epoch:  53 || Seq: 100 / 172 || loss: 78.02647996759697\n",
      "Epoch:  53 || Seq: 110 / 172 || loss: 66.66610983057933\n",
      "Epoch:  53 || Seq: 120 / 172 || loss: 93.55339206029338\n",
      "Epoch:  53 || Seq: 130 / 172 || loss: 326.79353352718886\n",
      "Epoch:  53 || Seq: 140 / 172 || loss: 90.9238998661749\n",
      "Epoch:  53 || Seq: 150 / 172 || loss: 29.77179784563027\n",
      "Epoch:  53 || Seq: 160 / 172 || loss: 151.29099189683242\n",
      "Epoch:  53 || Seq: 170 / 172 || loss: 64.84127818836885\n",
      "Epoch:  53 || Loss:  103.74426102950959\n",
      "Epoch:  54 || Seq: 0 / 172 || loss: 41.361361328998335\n",
      "Epoch:  54 || Seq: 10 / 172 || loss: 12.49070472226859\n",
      "Epoch:  54 || Seq: 20 / 172 || loss: 62.20044298982248\n",
      "Epoch:  54 || Seq: 30 / 172 || loss: 75.71633613531789\n",
      "Epoch:  54 || Seq: 40 / 172 || loss: 44.32311173596165\n",
      "Epoch:  54 || Seq: 50 / 172 || loss: 43.30825426464989\n",
      "Epoch:  54 || Seq: 60 / 172 || loss: 65.4000663140323\n",
      "Epoch:  54 || Seq: 70 / 172 || loss: 103.95362597245436\n",
      "Epoch:  54 || Seq: 80 / 172 || loss: 73.10340270945537\n",
      "Epoch:  54 || Seq: 90 / 172 || loss: 63.34715445222039\n",
      "Epoch:  54 || Seq: 100 / 172 || loss: 147.00747078549466\n",
      "Epoch:  54 || Seq: 110 / 172 || loss: 177.5465143206106\n",
      "Epoch:  54 || Seq: 120 / 172 || loss: 89.23886364853631\n",
      "Epoch:  54 || Seq: 130 / 172 || loss: 55.718450333063416\n",
      "Epoch:  54 || Seq: 140 / 172 || loss: 36.60456409375183\n",
      "Epoch:  54 || Seq: 150 / 172 || loss: 19.838979335058305\n",
      "Epoch:  54 || Seq: 160 / 172 || loss: 126.26413540083628\n",
      "Epoch:  54 || Seq: 170 / 172 || loss: 2.975870729982853\n",
      "Epoch:  54 || Loss:  101.84466475408708\n",
      "Epoch:  55 || Seq: 0 / 172 || loss: 12.446209095606935\n",
      "Epoch:  55 || Seq: 10 / 172 || loss: 22.68693554236476\n",
      "Epoch:  55 || Seq: 20 / 172 || loss: 34.57915494615032\n",
      "Epoch:  55 || Seq: 30 / 172 || loss: 281.4192923259735\n",
      "Epoch:  55 || Seq: 40 / 172 || loss: 212.86988685904618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  55 || Seq: 50 / 172 || loss: 106.52662366930524\n",
      "Epoch:  55 || Seq: 60 / 172 || loss: 95.0293667782098\n",
      "Epoch:  55 || Seq: 70 / 172 || loss: 132.8359892948065\n",
      "Epoch:  55 || Seq: 80 / 172 || loss: 155.90798240441543\n",
      "Epoch:  55 || Seq: 90 / 172 || loss: 54.167988968392216\n",
      "Epoch:  55 || Seq: 100 / 172 || loss: 77.06954795207518\n",
      "Epoch:  55 || Seq: 110 / 172 || loss: 145.23495396545954\n",
      "Epoch:  55 || Seq: 120 / 172 || loss: 73.21577903202602\n",
      "Epoch:  55 || Seq: 130 / 172 || loss: 76.85157479483046\n",
      "Epoch:  55 || Seq: 140 / 172 || loss: 238.5012109777467\n",
      "Epoch:  55 || Seq: 150 / 172 || loss: 161.0904116278345\n",
      "Epoch:  55 || Seq: 160 / 172 || loss: 187.13257353061013\n",
      "Epoch:  55 || Seq: 170 / 172 || loss: 12.327687488360839\n",
      "Epoch:  55 || Loss:  109.35702025643101\n",
      "Epoch:  56 || Seq: 0 / 172 || loss: 61.93806671924804\n",
      "Epoch:  56 || Seq: 10 / 172 || loss: 123.2596858768509\n",
      "Epoch:  56 || Seq: 20 / 172 || loss: 57.59313343465328\n",
      "Epoch:  56 || Seq: 30 / 172 || loss: 229.34831767074334\n",
      "Epoch:  56 || Seq: 40 / 172 || loss: 16.329826301998562\n",
      "Epoch:  56 || Seq: 50 / 172 || loss: 195.73033089800313\n",
      "Epoch:  56 || Seq: 60 / 172 || loss: 25.52649468296896\n",
      "Epoch:  56 || Seq: 70 / 172 || loss: 44.76169953482418\n",
      "Epoch:  56 || Seq: 80 / 172 || loss: 20.009736485341016\n",
      "Epoch:  56 || Seq: 90 / 172 || loss: 241.82709878839944\n",
      "Epoch:  56 || Seq: 100 / 172 || loss: 360.17816321055096\n",
      "Epoch:  56 || Seq: 110 / 172 || loss: 9.445517461746931\n",
      "Epoch:  56 || Seq: 120 / 172 || loss: 92.10948016169608\n",
      "Epoch:  56 || Seq: 130 / 172 || loss: 74.78315556844076\n",
      "Epoch:  56 || Seq: 140 / 172 || loss: 124.67443962395191\n",
      "Epoch:  56 || Seq: 150 / 172 || loss: 124.64243832976457\n",
      "Epoch:  56 || Seq: 160 / 172 || loss: 137.60144827216862\n",
      "Epoch:  56 || Seq: 170 / 172 || loss: 236.87588293588271\n",
      "Epoch:  56 || Loss:  123.72939883132122\n",
      "Epoch:  57 || Seq: 0 / 172 || loss: 10.83357425365183\n",
      "Epoch:  57 || Seq: 10 / 172 || loss: 12.627955503540026\n",
      "Epoch:  57 || Seq: 20 / 172 || loss: 112.76917636394501\n",
      "Epoch:  57 || Seq: 30 / 172 || loss: 346.256251736411\n",
      "Epoch:  57 || Seq: 40 / 172 || loss: 349.62403771001846\n",
      "Epoch:  57 || Seq: 50 / 172 || loss: 9.61894897421201\n",
      "Epoch:  57 || Seq: 60 / 172 || loss: 5.452903943903306\n",
      "Epoch:  57 || Seq: 70 / 172 || loss: 82.44666669297936\n",
      "Epoch:  57 || Seq: 80 / 172 || loss: 62.32406167499721\n",
      "Epoch:  57 || Seq: 90 / 172 || loss: 33.20218339152634\n",
      "Epoch:  57 || Seq: 100 / 172 || loss: 290.2980135174261\n",
      "Epoch:  57 || Seq: 110 / 172 || loss: 74.75225471074765\n",
      "Epoch:  57 || Seq: 120 / 172 || loss: 32.77111983924572\n",
      "Epoch:  57 || Seq: 130 / 172 || loss: 627.993522644043\n",
      "Epoch:  57 || Seq: 140 / 172 || loss: 264.75504589940493\n",
      "Epoch:  57 || Seq: 150 / 172 || loss: 25.39429235997561\n",
      "Epoch:  57 || Seq: 160 / 172 || loss: 48.1151726367297\n",
      "Epoch:  57 || Seq: 170 / 172 || loss: 62.822585467079826\n",
      "Epoch:  57 || Loss:  114.91179912541361\n",
      "Epoch:  58 || Seq: 0 / 172 || loss: 2.033812225278881\n",
      "Epoch:  58 || Seq: 10 / 172 || loss: 27.33089412562549\n",
      "Epoch:  58 || Seq: 20 / 172 || loss: 14.73853743993319\n",
      "Epoch:  58 || Seq: 30 / 172 || loss: 46.36615599070986\n",
      "Epoch:  58 || Seq: 40 / 172 || loss: 564.0632864634196\n",
      "Epoch:  58 || Seq: 50 / 172 || loss: 190.72351137662676\n",
      "Epoch:  58 || Seq: 60 / 172 || loss: 53.779266351286104\n",
      "Epoch:  58 || Seq: 70 / 172 || loss: 191.44765423473558\n",
      "Epoch:  58 || Seq: 80 / 172 || loss: 292.8305126362377\n",
      "Epoch:  58 || Seq: 90 / 172 || loss: 29.561193732887897\n",
      "Epoch:  58 || Seq: 100 / 172 || loss: 30.61726591461583\n",
      "Epoch:  58 || Seq: 110 / 172 || loss: 10.978080412816434\n",
      "Epoch:  58 || Seq: 120 / 172 || loss: 160.45231685508043\n",
      "Epoch:  58 || Seq: 130 / 172 || loss: 100.28226007545987\n",
      "Epoch:  58 || Seq: 140 / 172 || loss: 170.8379104325646\n",
      "Epoch:  58 || Seq: 150 / 172 || loss: 14.492014684847423\n",
      "Epoch:  58 || Seq: 160 / 172 || loss: 16.48520573547908\n",
      "Epoch:  58 || Seq: 170 / 172 || loss: 33.574930250644684\n",
      "Epoch:  58 || Loss:  102.01456952986813\n",
      "Epoch:  59 || Seq: 0 / 172 || loss: 62.141642110422254\n",
      "Epoch:  59 || Seq: 10 / 172 || loss: 120.05997174466029\n",
      "Epoch:  59 || Seq: 20 / 172 || loss: 338.4411796739568\n",
      "Epoch:  59 || Seq: 30 / 172 || loss: 25.755259718745947\n",
      "Epoch:  59 || Seq: 40 / 172 || loss: 259.56444157473743\n",
      "Epoch:  59 || Seq: 50 / 172 || loss: 13.941009735868823\n",
      "Epoch:  59 || Seq: 60 / 172 || loss: 6.455761515035587\n",
      "Epoch:  59 || Seq: 70 / 172 || loss: 259.0050608139971\n",
      "Epoch:  59 || Seq: 80 / 172 || loss: 252.77638391852378\n",
      "Epoch:  59 || Seq: 90 / 172 || loss: 108.9241063692607\n",
      "Epoch:  59 || Seq: 100 / 172 || loss: 82.19935514107347\n",
      "Epoch:  59 || Seq: 110 / 172 || loss: 174.46682714874095\n",
      "Epoch:  59 || Seq: 120 / 172 || loss: 27.20736186352414\n",
      "Epoch:  59 || Seq: 130 / 172 || loss: 5.901355801455793\n",
      "Epoch:  59 || Seq: 140 / 172 || loss: 64.94198514319756\n",
      "Epoch:  59 || Seq: 150 / 172 || loss: 17.71327153005098\n",
      "Epoch:  59 || Seq: 160 / 172 || loss: 113.40259654483488\n",
      "Epoch:  59 || Seq: 170 / 172 || loss: 17.0142997730989\n",
      "Epoch:  59 || Loss:  114.90127313001311\n",
      "Change learning rate to:  1e-05\n",
      "Epoch:  60 || Seq: 0 / 172 || loss: 112.8269877187137\n",
      "Epoch:  60 || Seq: 10 / 172 || loss: 78.82452105456277\n",
      "Epoch:  60 || Seq: 20 / 172 || loss: 298.1539622220126\n",
      "Epoch:  60 || Seq: 30 / 172 || loss: 291.09594174948603\n",
      "Epoch:  60 || Seq: 40 / 172 || loss: 3.4736563493375128\n",
      "Epoch:  60 || Seq: 50 / 172 || loss: 203.18872440951125\n",
      "Epoch:  60 || Seq: 60 / 172 || loss: 468.39762217203776\n",
      "Epoch:  60 || Seq: 70 / 172 || loss: 372.3698097864787\n",
      "Epoch:  60 || Seq: 80 / 172 || loss: 243.87685690985785\n",
      "Epoch:  60 || Seq: 90 / 172 || loss: 158.70529668471391\n",
      "Epoch:  60 || Seq: 100 / 172 || loss: 113.00307017293844\n",
      "Epoch:  60 || Seq: 110 / 172 || loss: 317.0278885936737\n",
      "Epoch:  60 || Seq: 120 / 172 || loss: 247.83123522400857\n",
      "Epoch:  60 || Seq: 130 / 172 || loss: 49.42249518747513\n",
      "Epoch:  60 || Seq: 140 / 172 || loss: 78.17793140833133\n",
      "Epoch:  60 || Seq: 150 / 172 || loss: 46.21967592678572\n",
      "Epoch:  60 || Seq: 160 / 172 || loss: 126.07312998890876\n",
      "Epoch:  60 || Seq: 170 / 172 || loss: 1.8195970822125673\n",
      "Epoch:  60 || Loss:  172.1102119043485\n",
      "Epoch:  61 || Seq: 0 / 172 || loss: 235.24713706729935\n",
      "Epoch:  61 || Seq: 10 / 172 || loss: 3.208576425036881\n",
      "Epoch:  61 || Seq: 20 / 172 || loss: 169.6140466928482\n",
      "Epoch:  61 || Seq: 30 / 172 || loss: 298.73963267008463\n",
      "Epoch:  61 || Seq: 40 / 172 || loss: 75.50174402307582\n",
      "Epoch:  61 || Seq: 50 / 172 || loss: 259.3164232969284\n",
      "Epoch:  61 || Seq: 60 / 172 || loss: 296.5348031785753\n",
      "Epoch:  61 || Seq: 70 / 172 || loss: 179.47170471009753\n",
      "Epoch:  61 || Seq: 80 / 172 || loss: 296.6968692449423\n",
      "Epoch:  61 || Seq: 90 / 172 || loss: 43.42379094784459\n",
      "Epoch:  61 || Seq: 100 / 172 || loss: 56.03080516702988\n",
      "Epoch:  61 || Seq: 110 / 172 || loss: 428.3721902029855\n",
      "Epoch:  61 || Seq: 120 / 172 || loss: 170.47962250264192\n",
      "Epoch:  61 || Seq: 130 / 172 || loss: 3.0849194054516325\n",
      "Epoch:  61 || Seq: 140 / 172 || loss: 5.297199497966358\n",
      "Epoch:  61 || Seq: 150 / 172 || loss: 485.03974056243896\n",
      "Epoch:  61 || Seq: 160 / 172 || loss: 210.68662081266703\n",
      "Epoch:  61 || Seq: 170 / 172 || loss: 73.59401927339404\n",
      "Epoch:  61 || Loss:  169.10342892627924\n",
      "Epoch:  62 || Seq: 0 / 172 || loss: 171.62762482626283\n",
      "Epoch:  62 || Seq: 10 / 172 || loss: 45.48678696497033\n",
      "Epoch:  62 || Seq: 20 / 172 || loss: 14.317990215495229\n",
      "Epoch:  62 || Seq: 30 / 172 || loss: 20.359745226408307\n",
      "Epoch:  62 || Seq: 40 / 172 || loss: 115.09622006106656\n",
      "Epoch:  62 || Seq: 50 / 172 || loss: 11.550836767469134\n",
      "Epoch:  62 || Seq: 60 / 172 || loss: 56.10763168334961\n",
      "Epoch:  62 || Seq: 70 / 172 || loss: 160.06443657338\n",
      "Epoch:  62 || Seq: 80 / 172 || loss: 384.3039834282615\n",
      "Epoch:  62 || Seq: 90 / 172 || loss: 24.361820676175522\n",
      "Epoch:  62 || Seq: 100 / 172 || loss: 299.28236915826795\n",
      "Epoch:  62 || Seq: 110 / 172 || loss: 296.0431959950365\n",
      "Epoch:  62 || Seq: 120 / 172 || loss: 27.089251074939966\n",
      "Epoch:  62 || Seq: 130 / 172 || loss: 245.70952288309732\n",
      "Epoch:  62 || Seq: 140 / 172 || loss: 13.607787707677254\n",
      "Epoch:  62 || Seq: 150 / 172 || loss: 385.44117210388185\n",
      "Epoch:  62 || Seq: 160 / 172 || loss: 36.46695407231649\n",
      "Epoch:  62 || Seq: 170 / 172 || loss: 144.45374147593975\n",
      "Epoch:  62 || Loss:  167.6458108983034\n",
      "Epoch:  63 || Seq: 0 / 172 || loss: 277.2304439368573\n",
      "Epoch:  63 || Seq: 10 / 172 || loss: 147.10660730911573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  63 || Seq: 20 / 172 || loss: 314.3847990036011\n",
      "Epoch:  63 || Seq: 30 / 172 || loss: 40.587725192308426\n",
      "Epoch:  63 || Seq: 40 / 172 || loss: 194.7002442303826\n",
      "Epoch:  63 || Seq: 50 / 172 || loss: 329.24802843007177\n",
      "Epoch:  63 || Seq: 60 / 172 || loss: 15.679181883111596\n",
      "Epoch:  63 || Seq: 70 / 172 || loss: 140.30702407836915\n",
      "Epoch:  63 || Seq: 80 / 172 || loss: 108.22284895098872\n",
      "Epoch:  63 || Seq: 90 / 172 || loss: 46.090267402785166\n",
      "Epoch:  63 || Seq: 100 / 172 || loss: 95.29324495224725\n",
      "Epoch:  63 || Seq: 110 / 172 || loss: 294.77342107892036\n",
      "Epoch:  63 || Seq: 120 / 172 || loss: 198.48889198303223\n",
      "Epoch:  63 || Seq: 130 / 172 || loss: 226.82959782160245\n",
      "Epoch:  63 || Seq: 140 / 172 || loss: 330.23174283862113\n",
      "Epoch:  63 || Seq: 150 / 172 || loss: 520.6444232647235\n",
      "Epoch:  63 || Seq: 160 / 172 || loss: 190.6920120767925\n",
      "Epoch:  63 || Seq: 170 / 172 || loss: 1.8492846892255803\n",
      "Epoch:  63 || Loss:  166.3683594331492\n",
      "Epoch:  64 || Seq: 0 / 172 || loss: 133.14916380441613\n",
      "Epoch:  64 || Seq: 10 / 172 || loss: 99.01337572355988\n",
      "Epoch:  64 || Seq: 20 / 172 || loss: 15.201474366088709\n",
      "Epoch:  64 || Seq: 30 / 172 || loss: 2.2665935665063444\n",
      "Epoch:  64 || Seq: 40 / 172 || loss: 66.28070787121268\n",
      "Epoch:  64 || Seq: 50 / 172 || loss: 9.233246293326374\n",
      "Epoch:  64 || Seq: 60 / 172 || loss: 123.74136272229646\n",
      "Epoch:  64 || Seq: 70 / 172 || loss: 47.359912205310096\n",
      "Epoch:  64 || Seq: 80 / 172 || loss: 213.45008506774903\n",
      "Epoch:  64 || Seq: 90 / 172 || loss: 10.898387301937104\n",
      "Epoch:  64 || Seq: 100 / 172 || loss: 46.323805424902176\n",
      "Epoch:  64 || Seq: 110 / 172 || loss: 201.33237799056448\n",
      "Epoch:  64 || Seq: 120 / 172 || loss: 3.5848678651520807\n",
      "Epoch:  64 || Seq: 130 / 172 || loss: 382.255226817131\n",
      "Epoch:  64 || Seq: 140 / 172 || loss: 241.65407311531806\n",
      "Epoch:  64 || Seq: 150 / 172 || loss: 9.219919806462713\n",
      "Epoch:  64 || Seq: 160 / 172 || loss: 306.8837606832385\n",
      "Epoch:  64 || Seq: 170 / 172 || loss: 640.4681381225586\n",
      "Epoch:  64 || Loss:  168.69838341550707\n",
      "Epoch:  65 || Seq: 0 / 172 || loss: 190.28865881889098\n",
      "Epoch:  65 || Seq: 10 / 172 || loss: 2.5142528166373572\n",
      "Epoch:  65 || Seq: 20 / 172 || loss: 309.57082221621556\n",
      "Epoch:  65 || Seq: 30 / 172 || loss: 2.9809794973407406\n",
      "Epoch:  65 || Seq: 40 / 172 || loss: 112.19731173664331\n",
      "Epoch:  65 || Seq: 50 / 172 || loss: 80.33809001629169\n",
      "Epoch:  65 || Seq: 60 / 172 || loss: 168.48150650813028\n",
      "Epoch:  65 || Seq: 70 / 172 || loss: 65.69826025121353\n",
      "Epoch:  65 || Seq: 80 / 172 || loss: 289.5244550551138\n",
      "Epoch:  65 || Seq: 90 / 172 || loss: 138.01832658366152\n",
      "Epoch:  65 || Seq: 100 / 172 || loss: 118.96082285472325\n",
      "Epoch:  65 || Seq: 110 / 172 || loss: 80.19852308126596\n",
      "Epoch:  65 || Seq: 120 / 172 || loss: 288.9050095876058\n",
      "Epoch:  65 || Seq: 130 / 172 || loss: 366.59921112060545\n",
      "Epoch:  65 || Seq: 140 / 172 || loss: 23.569150035188574\n",
      "Epoch:  65 || Seq: 150 / 172 || loss: 163.89942402309842\n",
      "Epoch:  65 || Seq: 160 / 172 || loss: 286.1620624859667\n",
      "Epoch:  65 || Seq: 170 / 172 || loss: 482.7504587677809\n",
      "Epoch:  65 || Loss:  167.35214525580827\n",
      "Epoch:  66 || Seq: 0 / 172 || loss: 9.44041847542394\n",
      "Epoch:  66 || Seq: 10 / 172 || loss: 345.15444575959725\n",
      "Epoch:  66 || Seq: 20 / 172 || loss: 137.45416577657065\n",
      "Epoch:  66 || Seq: 30 / 172 || loss: 182.9403172492981\n",
      "Epoch:  66 || Seq: 40 / 172 || loss: 3.7381804079601637\n",
      "Epoch:  66 || Seq: 50 / 172 || loss: 31.148704663229484\n",
      "Epoch:  66 || Seq: 60 / 172 || loss: 127.04810743982142\n",
      "Epoch:  66 || Seq: 70 / 172 || loss: 69.74469566345215\n",
      "Epoch:  66 || Seq: 80 / 172 || loss: 602.8308905454783\n",
      "Epoch:  66 || Seq: 90 / 172 || loss: 75.81141396494289\n",
      "Epoch:  66 || Seq: 100 / 172 || loss: 142.81428978085518\n",
      "Epoch:  66 || Seq: 110 / 172 || loss: 60.17194574757626\n",
      "Epoch:  66 || Seq: 120 / 172 || loss: 526.1520681381226\n",
      "Epoch:  66 || Seq: 130 / 172 || loss: 331.37497087873635\n",
      "Epoch:  66 || Seq: 140 / 172 || loss: 295.9601573944092\n",
      "Epoch:  66 || Seq: 150 / 172 || loss: 22.164329528808594\n",
      "Epoch:  66 || Seq: 160 / 172 || loss: 319.7423273722331\n",
      "Epoch:  66 || Seq: 170 / 172 || loss: 71.98734893798829\n",
      "Epoch:  66 || Loss:  158.7910829023804\n",
      "Epoch:  67 || Seq: 0 / 172 || loss: 77.33754889079584\n",
      "Epoch:  67 || Seq: 10 / 172 || loss: 205.1033306800975\n",
      "Epoch:  67 || Seq: 20 / 172 || loss: 170.25621851494438\n",
      "Epoch:  67 || Seq: 30 / 172 || loss: 202.21637812978588\n",
      "Epoch:  67 || Seq: 40 / 172 || loss: 40.70925736427307\n",
      "Epoch:  67 || Seq: 50 / 172 || loss: 20.90670169312507\n",
      "Epoch:  67 || Seq: 60 / 172 || loss: 15.837381809165603\n",
      "Epoch:  67 || Seq: 70 / 172 || loss: 103.51427660490337\n",
      "Epoch:  67 || Seq: 80 / 172 || loss: 305.8931202491124\n",
      "Epoch:  67 || Seq: 90 / 172 || loss: 17.32012431388316\n",
      "Epoch:  67 || Seq: 100 / 172 || loss: 322.6054662638546\n",
      "Epoch:  67 || Seq: 110 / 172 || loss: 5.862293219400777\n",
      "Epoch:  67 || Seq: 120 / 172 || loss: 12.476141057393857\n",
      "Epoch:  67 || Seq: 130 / 172 || loss: 360.53662357330325\n",
      "Epoch:  67 || Seq: 140 / 172 || loss: 53.2499272210213\n",
      "Epoch:  67 || Seq: 150 / 172 || loss: 22.37235702673594\n",
      "Epoch:  67 || Seq: 160 / 172 || loss: 6.597326717235976\n",
      "Epoch:  67 || Seq: 170 / 172 || loss: 6.450738525508266\n",
      "Epoch:  67 || Loss:  165.25374814845802\n",
      "Epoch:  68 || Seq: 0 / 172 || loss: 259.55221836383527\n",
      "Epoch:  68 || Seq: 10 / 172 || loss: 30.990806981313995\n",
      "Epoch:  68 || Seq: 20 / 172 || loss: 31.20112114865333\n",
      "Epoch:  68 || Seq: 30 / 172 || loss: 14.297207619832909\n",
      "Epoch:  68 || Seq: 40 / 172 || loss: 31.214220725310344\n",
      "Epoch:  68 || Seq: 50 / 172 || loss: 28.536097190882032\n",
      "Epoch:  68 || Seq: 60 / 172 || loss: 58.556461079915366\n",
      "Epoch:  68 || Seq: 70 / 172 || loss: 234.88110675811768\n",
      "Epoch:  68 || Seq: 80 / 172 || loss: 187.52777001417564\n",
      "Epoch:  68 || Seq: 90 / 172 || loss: 99.26724901442465\n",
      "Epoch:  68 || Seq: 100 / 172 || loss: 121.1190326416658\n",
      "Epoch:  68 || Seq: 110 / 172 || loss: 34.79320975806978\n",
      "Epoch:  68 || Seq: 120 / 172 || loss: 14.192176919234427\n",
      "Epoch:  68 || Seq: 130 / 172 || loss: 133.6510181427002\n",
      "Epoch:  68 || Seq: 140 / 172 || loss: 255.64331554371864\n",
      "Epoch:  68 || Seq: 150 / 172 || loss: 299.3873782609792\n",
      "Epoch:  68 || Seq: 160 / 172 || loss: 94.78682666354709\n",
      "Epoch:  68 || Seq: 170 / 172 || loss: 127.81957650953724\n",
      "Epoch:  68 || Loss:  156.2164445074384\n",
      "Epoch:  69 || Seq: 0 / 172 || loss: 152.59171123466183\n",
      "Epoch:  69 || Seq: 10 / 172 || loss: 95.48208923102356\n",
      "Epoch:  69 || Seq: 20 / 172 || loss: 170.9380441393171\n",
      "Epoch:  69 || Seq: 30 / 172 || loss: 41.364404251700954\n",
      "Epoch:  69 || Seq: 40 / 172 || loss: 157.0565264892578\n",
      "Epoch:  69 || Seq: 50 / 172 || loss: 99.6401246112326\n",
      "Epoch:  69 || Seq: 60 / 172 || loss: 363.4495091629028\n",
      "Epoch:  69 || Seq: 70 / 172 || loss: 125.07896321614584\n",
      "Epoch:  69 || Seq: 80 / 172 || loss: 103.31277892986934\n",
      "Epoch:  69 || Seq: 90 / 172 || loss: 204.6346448527442\n",
      "Epoch:  69 || Seq: 100 / 172 || loss: 241.80519896287186\n",
      "Epoch:  69 || Seq: 110 / 172 || loss: 5.971296690759205\n",
      "Epoch:  69 || Seq: 120 / 172 || loss: 113.65668267011642\n",
      "Epoch:  69 || Seq: 130 / 172 || loss: 89.0410115814209\n",
      "Epoch:  69 || Seq: 140 / 172 || loss: 192.32818765523066\n",
      "Epoch:  69 || Seq: 150 / 172 || loss: 217.3500004642909\n",
      "Epoch:  69 || Seq: 160 / 172 || loss: 111.29430430099882\n",
      "Epoch:  69 || Seq: 170 / 172 || loss: 152.0707494682736\n",
      "Epoch:  69 || Loss:  163.09030248519713\n",
      "Epoch:  70 || Seq: 0 / 172 || loss: 119.22717265619173\n",
      "Epoch:  70 || Seq: 10 / 172 || loss: 232.39164250691732\n",
      "Epoch:  70 || Seq: 20 / 172 || loss: 289.8280234336853\n",
      "Epoch:  70 || Seq: 30 / 172 || loss: 279.1930469792822\n",
      "Epoch:  70 || Seq: 40 / 172 || loss: 327.17833187946906\n",
      "Epoch:  70 || Seq: 50 / 172 || loss: 582.7837270100912\n",
      "Epoch:  70 || Seq: 60 / 172 || loss: 19.794479457904924\n",
      "Epoch:  70 || Seq: 70 / 172 || loss: 160.10982844704077\n",
      "Epoch:  70 || Seq: 80 / 172 || loss: 206.52682961357965\n",
      "Epoch:  70 || Seq: 90 / 172 || loss: 57.0018477861704\n",
      "Epoch:  70 || Seq: 100 / 172 || loss: 239.6601155757904\n",
      "Epoch:  70 || Seq: 110 / 172 || loss: 9.294542652698091\n",
      "Epoch:  70 || Seq: 120 / 172 || loss: 45.53933608531952\n",
      "Epoch:  70 || Seq: 130 / 172 || loss: 37.17658295350916\n",
      "Epoch:  70 || Seq: 140 / 172 || loss: 39.213936266444975\n",
      "Epoch:  70 || Seq: 150 / 172 || loss: 398.3146139383316\n",
      "Epoch:  70 || Seq: 160 / 172 || loss: 6.031166433915496\n",
      "Epoch:  70 || Seq: 170 / 172 || loss: 225.81378820695375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  70 || Loss:  160.64871377286406\n",
      "Epoch:  71 || Seq: 0 / 172 || loss: 230.2784910935622\n",
      "Epoch:  71 || Seq: 10 / 172 || loss: 288.70890411253896\n",
      "Epoch:  71 || Seq: 20 / 172 || loss: 10.121229180691738\n",
      "Epoch:  71 || Seq: 30 / 172 || loss: 64.27909430190921\n",
      "Epoch:  71 || Seq: 40 / 172 || loss: 224.19700156317816\n",
      "Epoch:  71 || Seq: 50 / 172 || loss: 26.322694044846756\n",
      "Epoch:  71 || Seq: 60 / 172 || loss: 104.99391682334245\n",
      "Epoch:  71 || Seq: 70 / 172 || loss: 421.3192670663198\n",
      "Epoch:  71 || Seq: 80 / 172 || loss: 112.05435430086575\n",
      "Epoch:  71 || Seq: 90 / 172 || loss: 21.345636444432394\n",
      "Epoch:  71 || Seq: 100 / 172 || loss: 247.74829473776313\n",
      "Epoch:  71 || Seq: 110 / 172 || loss: 465.59149344762164\n",
      "Epoch:  71 || Seq: 120 / 172 || loss: 230.1886414885521\n",
      "Epoch:  71 || Seq: 130 / 172 || loss: 5.129070240305737\n",
      "Epoch:  71 || Seq: 140 / 172 || loss: 307.0782525981963\n",
      "Epoch:  71 || Seq: 150 / 172 || loss: 29.07293684342328\n",
      "Epoch:  71 || Seq: 160 / 172 || loss: 41.97442715508597\n",
      "Epoch:  71 || Seq: 170 / 172 || loss: 231.92577895618254\n",
      "Epoch:  71 || Loss:  169.79304270839594\n",
      "Epoch:  72 || Seq: 0 / 172 || loss: 41.75793984532356\n",
      "Epoch:  72 || Seq: 10 / 172 || loss: 92.21439214229584\n",
      "Epoch:  72 || Seq: 20 / 172 || loss: 175.87707753614947\n",
      "Epoch:  72 || Seq: 30 / 172 || loss: 107.75804976780306\n",
      "Epoch:  72 || Seq: 40 / 172 || loss: 90.0091542695698\n",
      "Epoch:  72 || Seq: 50 / 172 || loss: 277.3633282661438\n",
      "Epoch:  72 || Seq: 60 / 172 || loss: 71.18236513137818\n",
      "Epoch:  72 || Seq: 70 / 172 || loss: 207.25075645032138\n",
      "Epoch:  72 || Seq: 80 / 172 || loss: 130.1828384399414\n",
      "Epoch:  72 || Seq: 90 / 172 || loss: 182.55172817523663\n",
      "Epoch:  72 || Seq: 100 / 172 || loss: 98.66801357269287\n",
      "Epoch:  72 || Seq: 110 / 172 || loss: 13.494768836664134\n",
      "Epoch:  72 || Seq: 120 / 172 || loss: 30.618216634831494\n",
      "Epoch:  72 || Seq: 130 / 172 || loss: 259.478949873071\n",
      "Epoch:  72 || Seq: 140 / 172 || loss: 541.6052027212249\n",
      "Epoch:  72 || Seq: 150 / 172 || loss: 172.22501178099523\n",
      "Epoch:  72 || Seq: 160 / 172 || loss: 198.2903027534485\n",
      "Epoch:  72 || Seq: 170 / 172 || loss: 222.9621769464933\n",
      "Epoch:  72 || Loss:  159.97774699680147\n",
      "Epoch:  73 || Seq: 0 / 172 || loss: 232.09615972042084\n",
      "Epoch:  73 || Seq: 10 / 172 || loss: 15.327761005298942\n",
      "Epoch:  73 || Seq: 20 / 172 || loss: 286.73311816512916\n",
      "Epoch:  73 || Seq: 30 / 172 || loss: 10.065243828864325\n",
      "Epoch:  73 || Seq: 40 / 172 || loss: 4.934970743515912\n",
      "Epoch:  73 || Seq: 50 / 172 || loss: 548.5035437345505\n",
      "Epoch:  73 || Seq: 60 / 172 || loss: 60.902259826660156\n",
      "Epoch:  73 || Seq: 70 / 172 || loss: 269.58768126839084\n",
      "Epoch:  73 || Seq: 80 / 172 || loss: 309.9292228655382\n",
      "Epoch:  73 || Seq: 90 / 172 || loss: 232.62162017822266\n",
      "Epoch:  73 || Seq: 100 / 172 || loss: 68.55071929931641\n",
      "Epoch:  73 || Seq: 110 / 172 || loss: 11.835888636764139\n",
      "Epoch:  73 || Seq: 120 / 172 || loss: 99.55384922027588\n",
      "Epoch:  73 || Seq: 130 / 172 || loss: 242.76087608436742\n",
      "Epoch:  73 || Seq: 140 / 172 || loss: 351.81180136998495\n",
      "Epoch:  73 || Seq: 150 / 172 || loss: 74.38445288634726\n",
      "Epoch:  73 || Seq: 160 / 172 || loss: 257.29514533595034\n",
      "Epoch:  73 || Seq: 170 / 172 || loss: 2.6777532855980097\n",
      "Epoch:  73 || Loss:  162.51814716547568\n",
      "Epoch:  74 || Seq: 0 / 172 || loss: 41.89229701624976\n",
      "Epoch:  74 || Seq: 10 / 172 || loss: 12.966420226128927\n",
      "Epoch:  74 || Seq: 20 / 172 || loss: 401.42399336198963\n",
      "Epoch:  74 || Seq: 30 / 172 || loss: 116.90064203219178\n",
      "Epoch:  74 || Seq: 40 / 172 || loss: 18.91169796898388\n",
      "Epoch:  74 || Seq: 50 / 172 || loss: 296.35527629852294\n",
      "Epoch:  74 || Seq: 60 / 172 || loss: 98.87415802107554\n",
      "Epoch:  74 || Seq: 70 / 172 || loss: 353.9746224146623\n",
      "Epoch:  74 || Seq: 80 / 172 || loss: 95.75876426696777\n",
      "Epoch:  74 || Seq: 90 / 172 || loss: 115.10868986915139\n",
      "Epoch:  74 || Seq: 100 / 172 || loss: 5.666061085305716\n",
      "Epoch:  74 || Seq: 110 / 172 || loss: 22.24489422204594\n",
      "Epoch:  74 || Seq: 120 / 172 || loss: 23.2987320626562\n",
      "Epoch:  74 || Seq: 130 / 172 || loss: 54.18053992589315\n",
      "Epoch:  74 || Seq: 140 / 172 || loss: 44.028921127319336\n",
      "Epoch:  74 || Seq: 150 / 172 || loss: 35.17174291610718\n",
      "Epoch:  74 || Seq: 160 / 172 || loss: 548.3160302088811\n",
      "Epoch:  74 || Seq: 170 / 172 || loss: 147.8924139291048\n",
      "Epoch:  74 || Loss:  162.52957926939868\n",
      "Epoch:  75 || Seq: 0 / 172 || loss: 321.03563117980957\n",
      "Epoch:  75 || Seq: 10 / 172 || loss: 338.55609252929685\n",
      "Epoch:  75 || Seq: 20 / 172 || loss: 11.31085584769469\n",
      "Epoch:  75 || Seq: 30 / 172 || loss: 456.3289681971073\n",
      "Epoch:  75 || Seq: 40 / 172 || loss: 288.14373727112365\n",
      "Epoch:  75 || Seq: 50 / 172 || loss: 152.4959652709961\n",
      "Epoch:  75 || Seq: 60 / 172 || loss: 97.33283680876096\n",
      "Epoch:  75 || Seq: 70 / 172 || loss: 131.0415203696803\n",
      "Epoch:  75 || Seq: 80 / 172 || loss: 114.00407118101914\n",
      "Epoch:  75 || Seq: 90 / 172 || loss: 167.15249942175367\n",
      "Epoch:  75 || Seq: 100 / 172 || loss: 243.51637055657127\n",
      "Epoch:  75 || Seq: 110 / 172 || loss: 76.83429474277156\n",
      "Epoch:  75 || Seq: 120 / 172 || loss: 164.99443937230993\n",
      "Epoch:  75 || Seq: 130 / 172 || loss: 88.71556197591126\n",
      "Epoch:  75 || Seq: 140 / 172 || loss: 184.95411171023127\n",
      "Epoch:  75 || Seq: 150 / 172 || loss: 68.39715597364638\n",
      "Epoch:  75 || Seq: 160 / 172 || loss: 200.70926971662612\n",
      "Epoch:  75 || Seq: 170 / 172 || loss: 175.79237842559814\n",
      "Epoch:  75 || Loss:  164.1520327303538\n",
      "Epoch:  76 || Seq: 0 / 172 || loss: 97.99291079353542\n",
      "Epoch:  76 || Seq: 10 / 172 || loss: 493.7763784790039\n",
      "Epoch:  76 || Seq: 20 / 172 || loss: 6.136629556908327\n",
      "Epoch:  76 || Seq: 30 / 172 || loss: 5.519620618649891\n",
      "Epoch:  76 || Seq: 40 / 172 || loss: 78.88747929899316\n",
      "Epoch:  76 || Seq: 50 / 172 || loss: 368.0719814300537\n",
      "Epoch:  76 || Seq: 60 / 172 || loss: 146.52478750546774\n",
      "Epoch:  76 || Seq: 70 / 172 || loss: 4.336338145392282\n",
      "Epoch:  76 || Seq: 80 / 172 || loss: 371.2391184846992\n",
      "Epoch:  76 || Seq: 90 / 172 || loss: 187.39616119861603\n",
      "Epoch:  76 || Seq: 100 / 172 || loss: 352.32749465629456\n",
      "Epoch:  76 || Seq: 110 / 172 || loss: 11.233608555669585\n",
      "Epoch:  76 || Seq: 120 / 172 || loss: 144.7176047960917\n",
      "Epoch:  76 || Seq: 130 / 172 || loss: 19.050929443256276\n",
      "Epoch:  76 || Seq: 140 / 172 || loss: 42.15639237314463\n",
      "Epoch:  76 || Seq: 150 / 172 || loss: 79.20175751699851\n",
      "Epoch:  76 || Seq: 160 / 172 || loss: 204.85648402045754\n",
      "Epoch:  76 || Seq: 170 / 172 || loss: 350.48787060905903\n",
      "Epoch:  76 || Loss:  162.92673850708488\n",
      "Epoch:  77 || Seq: 0 / 172 || loss: 79.52856121063232\n",
      "Epoch:  77 || Seq: 10 / 172 || loss: 167.51789537270864\n",
      "Epoch:  77 || Seq: 20 / 172 || loss: 213.4743197304862\n",
      "Epoch:  77 || Seq: 30 / 172 || loss: 46.16525899796259\n",
      "Epoch:  77 || Seq: 40 / 172 || loss: 399.52895520925523\n",
      "Epoch:  77 || Seq: 50 / 172 || loss: 228.6431059902534\n",
      "Epoch:  77 || Seq: 60 / 172 || loss: 3.2339281754361258\n",
      "Epoch:  77 || Seq: 70 / 172 || loss: 296.48926172583646\n",
      "Epoch:  77 || Seq: 80 / 172 || loss: 79.98018916447957\n",
      "Epoch:  77 || Seq: 90 / 172 || loss: 35.19074699282646\n",
      "Epoch:  77 || Seq: 100 / 172 || loss: 153.59460906548932\n",
      "Epoch:  77 || Seq: 110 / 172 || loss: 4.349649119643598\n",
      "Epoch:  77 || Seq: 120 / 172 || loss: 200.05328750610352\n",
      "Epoch:  77 || Seq: 130 / 172 || loss: 248.0231413793564\n",
      "Epoch:  77 || Seq: 140 / 172 || loss: 25.111815096189577\n",
      "Epoch:  77 || Seq: 150 / 172 || loss: 74.91467593266414\n",
      "Epoch:  77 || Seq: 160 / 172 || loss: 197.6754231651624\n",
      "Epoch:  77 || Seq: 170 / 172 || loss: 16.129678414691064\n",
      "Epoch:  77 || Loss:  169.06420719526866\n",
      "Epoch:  78 || Seq: 0 / 172 || loss: 8.34347930986517\n",
      "Epoch:  78 || Seq: 10 / 172 || loss: 71.23488375719856\n",
      "Epoch:  78 || Seq: 20 / 172 || loss: 172.57556264540727\n",
      "Epoch:  78 || Seq: 30 / 172 || loss: 222.43435866038004\n",
      "Epoch:  78 || Seq: 40 / 172 || loss: 120.17201935617547\n",
      "Epoch:  78 || Seq: 50 / 172 || loss: 8.096779494424878\n",
      "Epoch:  78 || Seq: 60 / 172 || loss: 27.756940195647378\n",
      "Epoch:  78 || Seq: 70 / 172 || loss: 146.839793548584\n",
      "Epoch:  78 || Seq: 80 / 172 || loss: 54.07457632767527\n",
      "Epoch:  78 || Seq: 90 / 172 || loss: 64.93321629791032\n",
      "Epoch:  78 || Seq: 100 / 172 || loss: 360.60560486866876\n",
      "Epoch:  78 || Seq: 110 / 172 || loss: 246.27890063957736\n",
      "Epoch:  78 || Seq: 120 / 172 || loss: 46.9218700726827\n",
      "Epoch:  78 || Seq: 130 / 172 || loss: 39.48373235974993\n",
      "Epoch:  78 || Seq: 140 / 172 || loss: 5.999515335282518\n",
      "Epoch:  78 || Seq: 150 / 172 || loss: 516.7074653625489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  78 || Seq: 160 / 172 || loss: 98.77408997217815\n",
      "Epoch:  78 || Seq: 170 / 172 || loss: 134.32785044776068\n",
      "Epoch:  78 || Loss:  166.57879098741446\n",
      "Epoch:  79 || Seq: 0 / 172 || loss: 417.12524636586505\n",
      "Epoch:  79 || Seq: 10 / 172 || loss: 4.467075089551509\n",
      "Epoch:  79 || Seq: 20 / 172 || loss: 4.389943677648578\n",
      "Epoch:  79 || Seq: 30 / 172 || loss: 39.27728752086037\n",
      "Epoch:  79 || Seq: 40 / 172 || loss: 215.23952831540788\n",
      "Epoch:  79 || Seq: 50 / 172 || loss: 360.19846687316897\n",
      "Epoch:  79 || Seq: 60 / 172 || loss: 2.7160025858320296\n",
      "Epoch:  79 || Seq: 70 / 172 || loss: 36.547922439575196\n",
      "Epoch:  79 || Seq: 80 / 172 || loss: 154.04050305684407\n",
      "Epoch:  79 || Seq: 90 / 172 || loss: 231.4910159111023\n",
      "Epoch:  79 || Seq: 100 / 172 || loss: 124.55508147345648\n",
      "Epoch:  79 || Seq: 110 / 172 || loss: 57.86445960544405\n",
      "Epoch:  79 || Seq: 120 / 172 || loss: 526.6529618043166\n",
      "Epoch:  79 || Seq: 130 / 172 || loss: 166.6528789643888\n",
      "Epoch:  79 || Seq: 140 / 172 || loss: 328.22979467565364\n",
      "Epoch:  79 || Seq: 150 / 172 || loss: 228.16688556256503\n",
      "Epoch:  79 || Seq: 160 / 172 || loss: 114.202575138637\n",
      "Epoch:  79 || Seq: 170 / 172 || loss: 15.891332760453224\n",
      "Epoch:  79 || Loss:  174.3549645321884\n",
      "Epoch:  80 || Seq: 0 / 172 || loss: 138.34939633882962\n",
      "Epoch:  80 || Seq: 10 / 172 || loss: 92.70820943932785\n",
      "Epoch:  80 || Seq: 20 / 172 || loss: 358.2699921347878\n",
      "Epoch:  80 || Seq: 30 / 172 || loss: 364.6513960220312\n",
      "Epoch:  80 || Seq: 40 / 172 || loss: 204.28355516039807\n",
      "Epoch:  80 || Seq: 50 / 172 || loss: 194.43180709818134\n",
      "Epoch:  80 || Seq: 60 / 172 || loss: 2.5500821824767628\n",
      "Epoch:  80 || Seq: 70 / 172 || loss: 13.257688689652992\n",
      "Epoch:  80 || Seq: 80 / 172 || loss: 188.35816368689905\n",
      "Epoch:  80 || Seq: 90 / 172 || loss: 231.02956464886665\n",
      "Epoch:  80 || Seq: 100 / 172 || loss: 206.54657006689482\n",
      "Epoch:  80 || Seq: 110 / 172 || loss: 289.6658748149872\n",
      "Epoch:  80 || Seq: 120 / 172 || loss: 78.63950850255787\n",
      "Epoch:  80 || Seq: 130 / 172 || loss: 7.998423121869564\n",
      "Epoch:  80 || Seq: 140 / 172 || loss: 45.80766894354632\n",
      "Epoch:  80 || Seq: 150 / 172 || loss: 183.86043285501415\n",
      "Epoch:  80 || Seq: 160 / 172 || loss: 272.48669669741673\n",
      "Epoch:  80 || Seq: 170 / 172 || loss: 267.0302625957288\n",
      "Epoch:  80 || Loss:  162.47128319173453\n",
      "Epoch:  81 || Seq: 0 / 172 || loss: 252.35829264322916\n",
      "Epoch:  81 || Seq: 10 / 172 || loss: 85.71220016479492\n",
      "Epoch:  81 || Seq: 20 / 172 || loss: 97.6807704925537\n",
      "Epoch:  81 || Seq: 30 / 172 || loss: 31.054325267076493\n",
      "Epoch:  81 || Seq: 40 / 172 || loss: 539.6915768623352\n",
      "Epoch:  81 || Seq: 50 / 172 || loss: 41.75372916219696\n",
      "Epoch:  81 || Seq: 60 / 172 || loss: 154.39987337102696\n",
      "Epoch:  81 || Seq: 70 / 172 || loss: 225.45634491741657\n",
      "Epoch:  81 || Seq: 80 / 172 || loss: 31.93174131293046\n",
      "Epoch:  81 || Seq: 90 / 172 || loss: 308.67327521064067\n",
      "Epoch:  81 || Seq: 100 / 172 || loss: 8.990950721502305\n",
      "Epoch:  81 || Seq: 110 / 172 || loss: 64.62742467561075\n",
      "Epoch:  81 || Seq: 120 / 172 || loss: 89.28414953890301\n",
      "Epoch:  81 || Seq: 130 / 172 || loss: 234.0195849282401\n",
      "Epoch:  81 || Seq: 140 / 172 || loss: 351.63157653808594\n",
      "Epoch:  81 || Seq: 150 / 172 || loss: 175.0425033569336\n",
      "Epoch:  81 || Seq: 160 / 172 || loss: 234.32932944919753\n",
      "Epoch:  81 || Seq: 170 / 172 || loss: 20.764274678520255\n",
      "Epoch:  81 || Loss:  163.250659958078\n",
      "Epoch:  82 || Seq: 0 / 172 || loss: 75.24017258118029\n",
      "Epoch:  82 || Seq: 10 / 172 || loss: 164.21845307716956\n",
      "Epoch:  82 || Seq: 20 / 172 || loss: 33.9939830039954\n",
      "Epoch:  82 || Seq: 30 / 172 || loss: 331.84341249465945\n",
      "Epoch:  82 || Seq: 40 / 172 || loss: 92.9262816387674\n",
      "Epoch:  82 || Seq: 50 / 172 || loss: 200.10598204533258\n",
      "Epoch:  82 || Seq: 60 / 172 || loss: 54.193423430124916\n",
      "Epoch:  82 || Seq: 70 / 172 || loss: 186.42908298268037\n",
      "Epoch:  82 || Seq: 80 / 172 || loss: 25.263902944154463\n",
      "Epoch:  82 || Seq: 90 / 172 || loss: 121.46365358531474\n",
      "Epoch:  82 || Seq: 100 / 172 || loss: 106.71807384490967\n",
      "Epoch:  82 || Seq: 110 / 172 || loss: 38.15357381105423\n",
      "Epoch:  82 || Seq: 120 / 172 || loss: 27.77113859470074\n",
      "Epoch:  82 || Seq: 130 / 172 || loss: 211.163407831481\n",
      "Epoch:  82 || Seq: 140 / 172 || loss: 195.64611686609294\n",
      "Epoch:  82 || Seq: 150 / 172 || loss: 181.5220300427505\n",
      "Epoch:  82 || Seq: 160 / 172 || loss: 135.94171131981744\n",
      "Epoch:  82 || Seq: 170 / 172 || loss: 56.521640717983246\n",
      "Epoch:  82 || Loss:  173.10890236494353\n",
      "Epoch:  83 || Seq: 0 / 172 || loss: 57.98295458583605\n",
      "Epoch:  83 || Seq: 10 / 172 || loss: 86.58520792346252\n",
      "Epoch:  83 || Seq: 20 / 172 || loss: 97.81081866555743\n",
      "Epoch:  83 || Seq: 30 / 172 || loss: 81.10255394279957\n",
      "Epoch:  83 || Seq: 40 / 172 || loss: 294.0763496052135\n",
      "Epoch:  83 || Seq: 50 / 172 || loss: 139.49997018490518\n",
      "Epoch:  83 || Seq: 60 / 172 || loss: 137.69012922989694\n",
      "Epoch:  83 || Seq: 70 / 172 || loss: 330.6079518000285\n",
      "Epoch:  83 || Seq: 80 / 172 || loss: 166.4658771861683\n",
      "Epoch:  83 || Seq: 90 / 172 || loss: 430.6438796303489\n",
      "Epoch:  83 || Seq: 100 / 172 || loss: 403.6290913422902\n",
      "Epoch:  83 || Seq: 110 / 172 || loss: 587.8421516418457\n",
      "Epoch:  83 || Seq: 120 / 172 || loss: 266.1159220377604\n",
      "Epoch:  83 || Seq: 130 / 172 || loss: 221.53830100787803\n",
      "Epoch:  83 || Seq: 140 / 172 || loss: 85.55045620884214\n",
      "Epoch:  83 || Seq: 150 / 172 || loss: 294.8694077670308\n",
      "Epoch:  83 || Seq: 160 / 172 || loss: 500.1199469566345\n",
      "Epoch:  83 || Seq: 170 / 172 || loss: 5.847067111247295\n",
      "Epoch:  83 || Loss:  163.47666210102997\n",
      "Epoch:  84 || Seq: 0 / 172 || loss: 106.9421491951778\n",
      "Epoch:  84 || Seq: 10 / 172 || loss: 163.3528832057491\n",
      "Epoch:  84 || Seq: 20 / 172 || loss: 457.58903930664064\n",
      "Epoch:  84 || Seq: 30 / 172 || loss: 272.54021512269975\n",
      "Epoch:  84 || Seq: 40 / 172 || loss: 40.682798306147255\n",
      "Epoch:  84 || Seq: 50 / 172 || loss: 2.223377434329854\n",
      "Epoch:  84 || Seq: 60 / 172 || loss: 515.3052874113384\n",
      "Epoch:  84 || Seq: 70 / 172 || loss: 194.31032677700645\n",
      "Epoch:  84 || Seq: 80 / 172 || loss: 11.487886181325718\n",
      "Epoch:  84 || Seq: 90 / 172 || loss: 228.62160494110802\n",
      "Epoch:  84 || Seq: 100 / 172 || loss: 153.93224449157714\n",
      "Epoch:  84 || Seq: 110 / 172 || loss: 322.70512135823566\n",
      "Epoch:  84 || Seq: 120 / 172 || loss: 308.39032084147135\n",
      "Epoch:  84 || Seq: 130 / 172 || loss: 454.53316165522364\n",
      "Epoch:  84 || Seq: 140 / 172 || loss: 74.28547214076389\n",
      "Epoch:  84 || Seq: 150 / 172 || loss: 60.21181268692017\n",
      "Epoch:  84 || Seq: 160 / 172 || loss: 33.598644068198546\n",
      "Epoch:  84 || Seq: 170 / 172 || loss: 194.65485205820627\n",
      "Epoch:  84 || Loss:  167.635240632196\n",
      "Epoch:  85 || Seq: 0 / 172 || loss: 19.31755242939107\n",
      "Epoch:  85 || Seq: 10 / 172 || loss: 501.60988632837933\n",
      "Epoch:  85 || Seq: 20 / 172 || loss: 9.231004858016968\n",
      "Epoch:  85 || Seq: 30 / 172 || loss: 209.0233146762848\n",
      "Epoch:  85 || Seq: 40 / 172 || loss: 23.781033822972525\n",
      "Epoch:  85 || Seq: 50 / 172 || loss: 42.96414358475629\n",
      "Epoch:  85 || Seq: 60 / 172 || loss: 350.36390241391956\n",
      "Epoch:  85 || Seq: 70 / 172 || loss: 126.06500887870789\n",
      "Epoch:  85 || Seq: 80 / 172 || loss: 159.2040005095151\n",
      "Epoch:  85 || Seq: 90 / 172 || loss: 98.3934728779963\n",
      "Epoch:  85 || Seq: 100 / 172 || loss: 230.28200965457492\n",
      "Epoch:  85 || Seq: 110 / 172 || loss: 109.28993558883667\n",
      "Epoch:  85 || Seq: 120 / 172 || loss: 415.151639189039\n",
      "Epoch:  85 || Seq: 130 / 172 || loss: 398.21690154506973\n",
      "Epoch:  85 || Seq: 140 / 172 || loss: 267.1023389642889\n",
      "Epoch:  85 || Seq: 150 / 172 || loss: 134.68099145328299\n",
      "Epoch:  85 || Seq: 160 / 172 || loss: 110.23684935502546\n",
      "Epoch:  85 || Seq: 170 / 172 || loss: 101.68524997884577\n",
      "Epoch:  85 || Loss:  161.8149389763694\n",
      "Epoch:  86 || Seq: 0 / 172 || loss: 67.70446738256858\n",
      "Epoch:  86 || Seq: 10 / 172 || loss: 333.5372694969177\n",
      "Epoch:  86 || Seq: 20 / 172 || loss: 301.7344402275048\n",
      "Epoch:  86 || Seq: 30 / 172 || loss: 293.57893500986853\n",
      "Epoch:  86 || Seq: 40 / 172 || loss: 276.1529465251499\n",
      "Epoch:  86 || Seq: 50 / 172 || loss: 189.88052065037192\n",
      "Epoch:  86 || Seq: 60 / 172 || loss: 37.1522921456231\n",
      "Epoch:  86 || Seq: 70 / 172 || loss: 192.60757554334498\n",
      "Epoch:  86 || Seq: 80 / 172 || loss: 72.81001823753692\n",
      "Epoch:  86 || Seq: 90 / 172 || loss: 33.482484221458435\n",
      "Epoch:  86 || Seq: 100 / 172 || loss: 10.27899047408415\n",
      "Epoch:  86 || Seq: 110 / 172 || loss: 70.50248101234436\n",
      "Epoch:  86 || Seq: 120 / 172 || loss: 13.422774586205682\n",
      "Epoch:  86 || Seq: 130 / 172 || loss: 71.01918325938007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  86 || Seq: 140 / 172 || loss: 7.5702730750336364\n",
      "Epoch:  86 || Seq: 150 / 172 || loss: 7.025331166055468\n",
      "Epoch:  86 || Seq: 160 / 172 || loss: 4.019620626601146\n",
      "Epoch:  86 || Seq: 170 / 172 || loss: 27.60078066127265\n",
      "Epoch:  86 || Loss:  166.23763754064595\n",
      "Epoch:  87 || Seq: 0 / 172 || loss: 10.456558273898231\n",
      "Epoch:  87 || Seq: 10 / 172 || loss: 304.3446604988792\n",
      "Epoch:  87 || Seq: 20 / 172 || loss: 213.26556248750006\n",
      "Epoch:  87 || Seq: 30 / 172 || loss: 28.335974758321587\n",
      "Epoch:  87 || Seq: 40 / 172 || loss: 33.200747393369674\n",
      "Epoch:  87 || Seq: 50 / 172 || loss: 244.44718732833863\n",
      "Epoch:  87 || Seq: 60 / 172 || loss: 114.02666588833458\n",
      "Epoch:  87 || Seq: 70 / 172 || loss: 182.4199146270752\n",
      "Epoch:  87 || Seq: 80 / 172 || loss: 26.671608570608356\n",
      "Epoch:  87 || Seq: 90 / 172 || loss: 287.64181236674386\n",
      "Epoch:  87 || Seq: 100 / 172 || loss: 315.3272476196289\n",
      "Epoch:  87 || Seq: 110 / 172 || loss: 13.52462543311872\n",
      "Epoch:  87 || Seq: 120 / 172 || loss: 157.05904049343533\n",
      "Epoch:  87 || Seq: 130 / 172 || loss: 354.95546762824057\n",
      "Epoch:  87 || Seq: 140 / 172 || loss: 28.41961677869161\n",
      "Epoch:  87 || Seq: 150 / 172 || loss: 2.8163482019832977\n",
      "Epoch:  87 || Seq: 160 / 172 || loss: 44.221429189046226\n",
      "Epoch:  87 || Seq: 170 / 172 || loss: 258.9079942142262\n",
      "Epoch:  87 || Loss:  164.72072674451087\n",
      "Epoch:  88 || Seq: 0 / 172 || loss: 305.686996480693\n",
      "Epoch:  88 || Seq: 10 / 172 || loss: 154.014253616333\n",
      "Epoch:  88 || Seq: 20 / 172 || loss: 116.94706183534278\n",
      "Epoch:  88 || Seq: 30 / 172 || loss: 416.82388830184937\n",
      "Epoch:  88 || Seq: 40 / 172 || loss: 50.968097403845086\n",
      "Epoch:  88 || Seq: 50 / 172 || loss: 2.870304356728281\n",
      "Epoch:  88 || Seq: 60 / 172 || loss: 28.59231427533982\n",
      "Epoch:  88 || Seq: 70 / 172 || loss: 383.84435209135216\n",
      "Epoch:  88 || Seq: 80 / 172 || loss: 190.72739055421619\n",
      "Epoch:  88 || Seq: 90 / 172 || loss: 75.46648389101028\n",
      "Epoch:  88 || Seq: 100 / 172 || loss: 14.184211550843125\n",
      "Epoch:  88 || Seq: 110 / 172 || loss: 249.77216608881952\n",
      "Epoch:  88 || Seq: 120 / 172 || loss: 183.87920100458206\n",
      "Epoch:  88 || Seq: 130 / 172 || loss: 191.17350635528564\n",
      "Epoch:  88 || Seq: 140 / 172 || loss: 418.8488142830985\n",
      "Epoch:  88 || Seq: 150 / 172 || loss: 234.61434400294507\n",
      "Epoch:  88 || Seq: 160 / 172 || loss: 5.157618874808152\n",
      "Epoch:  88 || Seq: 170 / 172 || loss: 99.23071007501511\n",
      "Epoch:  88 || Loss:  165.17505239476105\n",
      "Epoch:  89 || Seq: 0 / 172 || loss: 75.33428738514583\n",
      "Epoch:  89 || Seq: 10 / 172 || loss: 313.75690722465515\n",
      "Epoch:  89 || Seq: 20 / 172 || loss: 119.00314155377839\n",
      "Epoch:  89 || Seq: 30 / 172 || loss: 122.97527697980404\n",
      "Epoch:  89 || Seq: 40 / 172 || loss: 89.44865850607555\n",
      "Epoch:  89 || Seq: 50 / 172 || loss: 10.466946414456917\n",
      "Epoch:  89 || Seq: 60 / 172 || loss: 51.98676369740413\n",
      "Epoch:  89 || Seq: 70 / 172 || loss: 212.7380293722336\n",
      "Epoch:  89 || Seq: 80 / 172 || loss: 159.34728710468\n",
      "Epoch:  89 || Seq: 90 / 172 || loss: 188.65849278767902\n",
      "Epoch:  89 || Seq: 100 / 172 || loss: 176.32882526942663\n",
      "Epoch:  89 || Seq: 110 / 172 || loss: 127.01827029524178\n",
      "Epoch:  89 || Seq: 120 / 172 || loss: 179.89528473900583\n",
      "Epoch:  89 || Seq: 130 / 172 || loss: 230.8281452494363\n",
      "Epoch:  89 || Seq: 140 / 172 || loss: 216.18173032648423\n",
      "Epoch:  89 || Seq: 150 / 172 || loss: 6.5666636562673375\n",
      "Epoch:  89 || Seq: 160 / 172 || loss: 257.92649484559894\n",
      "Epoch:  89 || Seq: 170 / 172 || loss: 26.20448783442781\n",
      "Epoch:  89 || Loss:  163.28845483241517\n",
      "Change learning rate to:  1.0000000000000002e-06\n",
      "Epoch:  90 || Seq: 0 / 172 || loss: 527.1204483929803\n",
      "Epoch:  90 || Seq: 10 / 172 || loss: 382.6983616987864\n",
      "Epoch:  90 || Seq: 20 / 172 || loss: 88.22081857919693\n",
      "Epoch:  90 || Seq: 30 / 172 || loss: 127.49105895193\n",
      "Epoch:  90 || Seq: 40 / 172 || loss: 302.66879490443637\n",
      "Epoch:  90 || Seq: 50 / 172 || loss: 18.976925386623904\n",
      "Epoch:  90 || Seq: 60 / 172 || loss: 71.39115900993347\n",
      "Epoch:  90 || Seq: 70 / 172 || loss: 362.8128652954102\n",
      "Epoch:  90 || Seq: 80 / 172 || loss: 157.06475830078125\n",
      "Epoch:  90 || Seq: 90 / 172 || loss: 197.9409345503558\n",
      "Epoch:  90 || Seq: 100 / 172 || loss: 77.10606469048395\n",
      "Epoch:  90 || Seq: 110 / 172 || loss: 535.0342672596807\n",
      "Epoch:  90 || Seq: 120 / 172 || loss: 201.89682859532974\n",
      "Epoch:  90 || Seq: 130 / 172 || loss: 83.71014822097052\n",
      "Epoch:  90 || Seq: 140 / 172 || loss: 149.7402480063694\n",
      "Epoch:  90 || Seq: 150 / 172 || loss: 50.85959108670553\n",
      "Epoch:  90 || Seq: 160 / 172 || loss: 184.67774613698325\n",
      "Epoch:  90 || Seq: 170 / 172 || loss: 60.904190798401835\n",
      "Epoch:  90 || Loss:  174.36346430158773\n",
      "Epoch:  91 || Seq: 0 / 172 || loss: 38.86948910881491\n",
      "Epoch:  91 || Seq: 10 / 172 || loss: 12.012738047943762\n",
      "Epoch:  91 || Seq: 20 / 172 || loss: 463.5248458862305\n",
      "Epoch:  91 || Seq: 30 / 172 || loss: 309.4900399017334\n",
      "Epoch:  91 || Seq: 40 / 172 || loss: 461.5002926985423\n",
      "Epoch:  91 || Seq: 50 / 172 || loss: 168.0257239393566\n",
      "Epoch:  91 || Seq: 60 / 172 || loss: 233.83412777052985\n",
      "Epoch:  91 || Seq: 70 / 172 || loss: 13.595517622797113\n",
      "Epoch:  91 || Seq: 80 / 172 || loss: 25.423744718233745\n",
      "Epoch:  91 || Seq: 90 / 172 || loss: 126.94396140358664\n",
      "Epoch:  91 || Seq: 100 / 172 || loss: 199.84854443868002\n",
      "Epoch:  91 || Seq: 110 / 172 || loss: 236.80344572521392\n",
      "Epoch:  91 || Seq: 120 / 172 || loss: 385.8615462779999\n",
      "Epoch:  91 || Seq: 130 / 172 || loss: 261.31812003806783\n",
      "Epoch:  91 || Seq: 140 / 172 || loss: 10.448916751318253\n",
      "Epoch:  91 || Seq: 150 / 172 || loss: 52.02677717059851\n",
      "Epoch:  91 || Seq: 160 / 172 || loss: 175.03266247858605\n",
      "Epoch:  91 || Seq: 170 / 172 || loss: 107.29844038646955\n",
      "Epoch:  91 || Loss:  177.66367260271426\n",
      "Epoch:  92 || Seq: 0 / 172 || loss: 15.4766295470763\n",
      "Epoch:  92 || Seq: 10 / 172 || loss: 168.42481790270125\n",
      "Epoch:  92 || Seq: 20 / 172 || loss: 416.4924675822258\n",
      "Epoch:  92 || Seq: 30 / 172 || loss: 206.12488889694214\n",
      "Epoch:  92 || Seq: 40 / 172 || loss: 147.1471081901999\n",
      "Epoch:  92 || Seq: 50 / 172 || loss: 282.0719820658366\n",
      "Epoch:  92 || Seq: 60 / 172 || loss: 11.35194312834314\n",
      "Epoch:  92 || Seq: 70 / 172 || loss: 342.7146472930908\n",
      "Epoch:  92 || Seq: 80 / 172 || loss: 287.71937924079026\n",
      "Epoch:  92 || Seq: 90 / 172 || loss: 334.9898633956909\n",
      "Epoch:  92 || Seq: 100 / 172 || loss: 48.44795328729293\n",
      "Epoch:  92 || Seq: 110 / 172 || loss: 111.30834152584984\n",
      "Epoch:  92 || Seq: 120 / 172 || loss: 489.09816096379205\n",
      "Epoch:  92 || Seq: 130 / 172 || loss: 42.970853360493976\n",
      "Epoch:  92 || Seq: 140 / 172 || loss: 290.32894643147785\n",
      "Epoch:  92 || Seq: 150 / 172 || loss: 101.39836645126343\n",
      "Epoch:  92 || Seq: 160 / 172 || loss: 159.78079874375288\n",
      "Epoch:  92 || Seq: 170 / 172 || loss: 403.8315936817842\n",
      "Epoch:  92 || Loss:  176.20680149987066\n",
      "Epoch:  93 || Seq: 0 / 172 || loss: 170.3357071876526\n",
      "Epoch:  93 || Seq: 10 / 172 || loss: 355.313680489858\n",
      "Epoch:  93 || Seq: 20 / 172 || loss: 17.66570511152968\n",
      "Epoch:  93 || Seq: 30 / 172 || loss: 375.3891210953395\n",
      "Epoch:  93 || Seq: 40 / 172 || loss: 123.15154245495796\n",
      "Epoch:  93 || Seq: 50 / 172 || loss: 446.3879699707031\n",
      "Epoch:  93 || Seq: 60 / 172 || loss: 54.900835149428424\n",
      "Epoch:  93 || Seq: 70 / 172 || loss: 28.812177499135334\n",
      "Epoch:  93 || Seq: 80 / 172 || loss: 15.035930752754211\n",
      "Epoch:  93 || Seq: 90 / 172 || loss: 158.3182223002116\n",
      "Epoch:  93 || Seq: 100 / 172 || loss: 78.98191476939246\n",
      "Epoch:  93 || Seq: 110 / 172 || loss: 277.7279140299017\n",
      "Epoch:  93 || Seq: 120 / 172 || loss: 319.62509928795043\n",
      "Epoch:  93 || Seq: 130 / 172 || loss: 477.26623557595644\n",
      "Epoch:  93 || Seq: 140 / 172 || loss: 70.32397615909576\n",
      "Epoch:  93 || Seq: 150 / 172 || loss: 18.937141934409738\n",
      "Epoch:  93 || Seq: 160 / 172 || loss: 87.80637233674526\n",
      "Epoch:  93 || Seq: 170 / 172 || loss: 13.428815193474293\n",
      "Epoch:  93 || Loss:  174.66469340795712\n",
      "Epoch:  94 || Seq: 0 / 172 || loss: 7.405356272270805\n",
      "Epoch:  94 || Seq: 10 / 172 || loss: 69.82472771135244\n",
      "Epoch:  94 || Seq: 20 / 172 || loss: 41.51541434725126\n",
      "Epoch:  94 || Seq: 30 / 172 || loss: 229.43762878576914\n",
      "Epoch:  94 || Seq: 40 / 172 || loss: 408.29487624535193\n",
      "Epoch:  94 || Seq: 50 / 172 || loss: 157.36476741518294\n",
      "Epoch:  94 || Seq: 60 / 172 || loss: 20.95117026567459\n",
      "Epoch:  94 || Seq: 70 / 172 || loss: 86.2155478908902\n",
      "Epoch:  94 || Seq: 80 / 172 || loss: 215.63930002848306\n",
      "Epoch:  94 || Seq: 90 / 172 || loss: 533.5612188720703\n",
      "Epoch:  94 || Seq: 100 / 172 || loss: 88.056638269048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  94 || Seq: 110 / 172 || loss: 456.90161000127375\n",
      "Epoch:  94 || Seq: 120 / 172 || loss: 4.439180931929609\n",
      "Epoch:  94 || Seq: 130 / 172 || loss: 29.831284475326537\n",
      "Epoch:  94 || Seq: 140 / 172 || loss: 80.6090369841829\n",
      "Epoch:  94 || Seq: 150 / 172 || loss: 136.80516289020406\n",
      "Epoch:  94 || Seq: 160 / 172 || loss: 140.289914057805\n",
      "Epoch:  94 || Seq: 170 / 172 || loss: 176.00495697021483\n",
      "Epoch:  94 || Loss:  173.69745702134486\n",
      "Epoch:  95 || Seq: 0 / 172 || loss: 11.762319995297325\n",
      "Epoch:  95 || Seq: 10 / 172 || loss: 65.93945388793945\n",
      "Epoch:  95 || Seq: 20 / 172 || loss: 20.33961331901642\n",
      "Epoch:  95 || Seq: 30 / 172 || loss: 189.98181332678212\n",
      "Epoch:  95 || Seq: 40 / 172 || loss: 128.22316257770245\n",
      "Epoch:  95 || Seq: 50 / 172 || loss: 11.83963104195538\n",
      "Epoch:  95 || Seq: 60 / 172 || loss: 19.58342283964157\n",
      "Epoch:  95 || Seq: 70 / 172 || loss: 423.8925226384943\n",
      "Epoch:  95 || Seq: 80 / 172 || loss: 205.102196188534\n",
      "Epoch:  95 || Seq: 90 / 172 || loss: 236.61783091227213\n",
      "Epoch:  95 || Seq: 100 / 172 || loss: 92.88471234998396\n",
      "Epoch:  95 || Seq: 110 / 172 || loss: 33.51718415532793\n",
      "Epoch:  95 || Seq: 120 / 172 || loss: 1.144428642014232\n",
      "Epoch:  95 || Seq: 130 / 172 || loss: 13.11227608891204\n",
      "Epoch:  95 || Seq: 140 / 172 || loss: 35.36743708660728\n",
      "Epoch:  95 || Seq: 150 / 172 || loss: 41.72605193654696\n",
      "Epoch:  95 || Seq: 160 / 172 || loss: 331.45272805955676\n",
      "Epoch:  95 || Seq: 170 / 172 || loss: 291.1475284767151\n",
      "Epoch:  95 || Loss:  177.70355483546768\n",
      "Epoch:  96 || Seq: 0 / 172 || loss: 309.18723279779607\n",
      "Epoch:  96 || Seq: 10 / 172 || loss: 17.866513808568318\n",
      "Epoch:  96 || Seq: 20 / 172 || loss: 54.58452754928952\n",
      "Epoch:  96 || Seq: 30 / 172 || loss: 28.076986965380218\n",
      "Epoch:  96 || Seq: 40 / 172 || loss: 146.69612991809845\n",
      "Epoch:  96 || Seq: 50 / 172 || loss: 17.918237254023552\n",
      "Epoch:  96 || Seq: 60 / 172 || loss: 326.42613983154297\n",
      "Epoch:  96 || Seq: 70 / 172 || loss: 2.3445545209660414\n",
      "Epoch:  96 || Seq: 80 / 172 || loss: 4.2309939712285995\n",
      "Epoch:  96 || Seq: 90 / 172 || loss: 222.34307088349996\n",
      "Epoch:  96 || Seq: 100 / 172 || loss: 103.27824359893799\n",
      "Epoch:  96 || Seq: 110 / 172 || loss: 364.199399693807\n",
      "Epoch:  96 || Seq: 120 / 172 || loss: 5.034243966036369\n",
      "Epoch:  96 || Seq: 130 / 172 || loss: 145.3447907447815\n",
      "Epoch:  96 || Seq: 140 / 172 || loss: 98.46154358807732\n",
      "Epoch:  96 || Seq: 150 / 172 || loss: 338.1168491529382\n",
      "Epoch:  96 || Seq: 160 / 172 || loss: 359.04246418292706\n",
      "Epoch:  96 || Seq: 170 / 172 || loss: 329.2448706097073\n",
      "Epoch:  96 || Loss:  172.31320428493046\n",
      "Epoch:  97 || Seq: 0 / 172 || loss: 68.15846779767205\n",
      "Epoch:  97 || Seq: 10 / 172 || loss: 97.30118426154641\n",
      "Epoch:  97 || Seq: 20 / 172 || loss: 308.3666600314054\n",
      "Epoch:  97 || Seq: 30 / 172 || loss: 189.563206354777\n",
      "Epoch:  97 || Seq: 40 / 172 || loss: 22.624000534415245\n",
      "Epoch:  97 || Seq: 50 / 172 || loss: 20.011342900139944\n",
      "Epoch:  97 || Seq: 60 / 172 || loss: 7.478150468425486\n",
      "Epoch:  97 || Seq: 70 / 172 || loss: 132.8969660864936\n",
      "Epoch:  97 || Seq: 80 / 172 || loss: 110.93307355963267\n",
      "Epoch:  97 || Seq: 90 / 172 || loss: 21.514913823311264\n",
      "Epoch:  97 || Seq: 100 / 172 || loss: 5.462034315362366\n",
      "Epoch:  97 || Seq: 110 / 172 || loss: 170.20339590708414\n",
      "Epoch:  97 || Seq: 120 / 172 || loss: 238.03681421279907\n",
      "Epoch:  97 || Seq: 130 / 172 || loss: 231.89951957355845\n",
      "Epoch:  97 || Seq: 140 / 172 || loss: 148.03081663723648\n",
      "Epoch:  97 || Seq: 150 / 172 || loss: 127.38844631910324\n",
      "Epoch:  97 || Seq: 160 / 172 || loss: 170.36795616149902\n",
      "Epoch:  97 || Seq: 170 / 172 || loss: 72.5070595741272\n",
      "Epoch:  97 || Loss:  172.15093541062902\n",
      "Epoch:  98 || Seq: 0 / 172 || loss: 206.5478966361598\n",
      "Epoch:  98 || Seq: 10 / 172 || loss: 160.69381870209742\n",
      "Epoch:  98 || Seq: 20 / 172 || loss: 293.1936192512512\n",
      "Epoch:  98 || Seq: 30 / 172 || loss: 49.923792911900414\n",
      "Epoch:  98 || Seq: 40 / 172 || loss: 221.69934418622185\n",
      "Epoch:  98 || Seq: 50 / 172 || loss: 263.2399034500122\n",
      "Epoch:  98 || Seq: 60 / 172 || loss: 92.6840574645996\n",
      "Epoch:  98 || Seq: 70 / 172 || loss: 384.51581325531004\n",
      "Epoch:  98 || Seq: 80 / 172 || loss: 249.69957652458777\n",
      "Epoch:  98 || Seq: 90 / 172 || loss: 167.72435131669044\n",
      "Epoch:  98 || Seq: 100 / 172 || loss: 353.50108746119906\n",
      "Epoch:  98 || Seq: 110 / 172 || loss: 175.73554154372576\n",
      "Epoch:  98 || Seq: 120 / 172 || loss: 8.977906685609083\n",
      "Epoch:  98 || Seq: 130 / 172 || loss: 52.950578313941755\n",
      "Epoch:  98 || Seq: 140 / 172 || loss: 212.6176488598188\n",
      "Epoch:  98 || Seq: 150 / 172 || loss: 200.33237134493314\n",
      "Epoch:  98 || Seq: 160 / 172 || loss: 204.3560922924353\n",
      "Epoch:  98 || Seq: 170 / 172 || loss: 101.91685161590576\n",
      "Epoch:  98 || Loss:  178.90255939300425\n",
      "Epoch:  99 || Seq: 0 / 172 || loss: 0.4036525403156274\n",
      "Epoch:  99 || Seq: 10 / 172 || loss: 5.8389568462684425\n",
      "Epoch:  99 || Seq: 20 / 172 || loss: 78.13917847683555\n",
      "Epoch:  99 || Seq: 30 / 172 || loss: 185.01779606088155\n",
      "Epoch:  99 || Seq: 40 / 172 || loss: 1.9930840416604445\n",
      "Epoch:  99 || Seq: 50 / 172 || loss: 75.36832488328218\n",
      "Epoch:  99 || Seq: 60 / 172 || loss: 419.79039755620454\n",
      "Epoch:  99 || Seq: 70 / 172 || loss: 451.8787648677826\n",
      "Epoch:  99 || Seq: 80 / 172 || loss: 244.38678608769956\n",
      "Epoch:  99 || Seq: 90 / 172 || loss: 309.67778835296633\n",
      "Epoch:  99 || Seq: 100 / 172 || loss: 258.56269031100805\n",
      "Epoch:  99 || Seq: 110 / 172 || loss: 275.4558159157082\n",
      "Epoch:  99 || Seq: 120 / 172 || loss: 244.6688711643219\n",
      "Epoch:  99 || Seq: 130 / 172 || loss: 67.56665870121547\n",
      "Epoch:  99 || Seq: 140 / 172 || loss: 131.77715833563553\n",
      "Epoch:  99 || Seq: 150 / 172 || loss: 222.26023671121308\n",
      "Epoch:  99 || Seq: 160 / 172 || loss: 225.18391563778832\n",
      "Epoch:  99 || Seq: 170 / 172 || loss: 232.7461168501112\n",
      "Epoch:  99 || Loss:  176.12213881230295\n",
      "Epoch:  100 || Seq: 0 / 172 || loss: 252.67018011341924\n",
      "Epoch:  100 || Seq: 10 / 172 || loss: 3.352652626879075\n",
      "Epoch:  100 || Seq: 20 / 172 || loss: 98.42083530840667\n",
      "Epoch:  100 || Seq: 30 / 172 || loss: 244.27596274289218\n",
      "Epoch:  100 || Seq: 40 / 172 || loss: 440.92958215566784\n",
      "Epoch:  100 || Seq: 50 / 172 || loss: 66.43626869292487\n",
      "Epoch:  100 || Seq: 60 / 172 || loss: 154.00597361514443\n",
      "Epoch:  100 || Seq: 70 / 172 || loss: 431.59541858673094\n",
      "Epoch:  100 || Seq: 80 / 172 || loss: 9.67230939435271\n",
      "Epoch:  100 || Seq: 90 / 172 || loss: 123.29930586141089\n",
      "Epoch:  100 || Seq: 100 / 172 || loss: 12.6427424964495\n",
      "Epoch:  100 || Seq: 110 / 172 || loss: 114.93262708820232\n",
      "Epoch:  100 || Seq: 120 / 172 || loss: 40.00293529033661\n",
      "Epoch:  100 || Seq: 130 / 172 || loss: 9.744461154937744\n",
      "Epoch:  100 || Seq: 140 / 172 || loss: 24.231581301302523\n",
      "Epoch:  100 || Seq: 150 / 172 || loss: 115.03677733048148\n",
      "Epoch:  100 || Seq: 160 / 172 || loss: 14.788160681724548\n",
      "Epoch:  100 || Seq: 170 / 172 || loss: 411.6488974786574\n",
      "Epoch:  100 || Loss:  168.855111410547\n",
      "Epoch:  101 || Seq: 0 / 172 || loss: 5.0217401997039195\n",
      "Epoch:  101 || Seq: 10 / 172 || loss: 86.19799201190472\n",
      "Epoch:  101 || Seq: 20 / 172 || loss: 161.20845718383788\n",
      "Epoch:  101 || Seq: 30 / 172 || loss: 135.5403357744217\n",
      "Epoch:  101 || Seq: 40 / 172 || loss: 170.75513280762567\n",
      "Epoch:  101 || Seq: 50 / 172 || loss: 261.41675906711157\n",
      "Epoch:  101 || Seq: 60 / 172 || loss: 3.3064095904232493\n",
      "Epoch:  101 || Seq: 70 / 172 || loss: 25.22466007868449\n",
      "Epoch:  101 || Seq: 80 / 172 || loss: 295.0255065917969\n",
      "Epoch:  101 || Seq: 90 / 172 || loss: 261.83145663612765\n",
      "Epoch:  101 || Seq: 100 / 172 || loss: 246.40637839467902\n",
      "Epoch:  101 || Seq: 110 / 172 || loss: 332.8150974273682\n",
      "Epoch:  101 || Seq: 120 / 172 || loss: 172.69709990705763\n",
      "Epoch:  101 || Seq: 130 / 172 || loss: 66.3327287197113\n",
      "Epoch:  101 || Seq: 140 / 172 || loss: 526.4809278405231\n",
      "Epoch:  101 || Seq: 150 / 172 || loss: 210.7341379324595\n",
      "Epoch:  101 || Seq: 160 / 172 || loss: 181.42876955668132\n",
      "Epoch:  101 || Seq: 170 / 172 || loss: 306.836653175354\n",
      "Epoch:  101 || Loss:  172.96424486933654\n",
      "Epoch:  102 || Seq: 0 / 172 || loss: 213.37364117304483\n",
      "Epoch:  102 || Seq: 10 / 172 || loss: 372.1159127069556\n",
      "Epoch:  102 || Seq: 20 / 172 || loss: 281.0394693112373\n",
      "Epoch:  102 || Seq: 30 / 172 || loss: 442.2723193670574\n",
      "Epoch:  102 || Seq: 40 / 172 || loss: 359.2856869514172\n",
      "Epoch:  102 || Seq: 50 / 172 || loss: 225.91427902553392\n",
      "Epoch:  102 || Seq: 60 / 172 || loss: 259.99528009125163\n",
      "Epoch:  102 || Seq: 70 / 172 || loss: 262.8175340228611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  102 || Seq: 80 / 172 || loss: 6.86854808529218\n",
      "Epoch:  102 || Seq: 90 / 172 || loss: 95.60970133001155\n",
      "Epoch:  102 || Seq: 100 / 172 || loss: 137.8910645521604\n",
      "Epoch:  102 || Seq: 110 / 172 || loss: 241.60350279374555\n",
      "Epoch:  102 || Seq: 120 / 172 || loss: 232.34581184387207\n",
      "Epoch:  102 || Seq: 130 / 172 || loss: 290.589111328125\n",
      "Epoch:  102 || Seq: 140 / 172 || loss: 9.54002491065434\n",
      "Epoch:  102 || Seq: 150 / 172 || loss: 67.3296365737915\n",
      "Epoch:  102 || Seq: 160 / 172 || loss: 241.5235117594401\n",
      "Epoch:  102 || Seq: 170 / 172 || loss: 91.31642296200707\n",
      "Epoch:  102 || Loss:  173.8913451413672\n",
      "Epoch:  103 || Seq: 0 / 172 || loss: 80.10104699134827\n",
      "Epoch:  103 || Seq: 10 / 172 || loss: 372.07442306518556\n",
      "Epoch:  103 || Seq: 20 / 172 || loss: 87.43646319707234\n",
      "Epoch:  103 || Seq: 30 / 172 || loss: 38.295786004317435\n",
      "Epoch:  103 || Seq: 40 / 172 || loss: 216.14017044870477\n",
      "Epoch:  103 || Seq: 50 / 172 || loss: 273.04298109757275\n",
      "Epoch:  103 || Seq: 60 / 172 || loss: 173.26894614810035\n",
      "Epoch:  103 || Seq: 70 / 172 || loss: 5.830459882815679\n",
      "Epoch:  103 || Seq: 80 / 172 || loss: 332.3672735040838\n",
      "Epoch:  103 || Seq: 90 / 172 || loss: 232.77018394470215\n",
      "Epoch:  103 || Seq: 100 / 172 || loss: 14.166136691346765\n",
      "Epoch:  103 || Seq: 110 / 172 || loss: 22.06867603155283\n",
      "Epoch:  103 || Seq: 120 / 172 || loss: 489.28948003595525\n",
      "Epoch:  103 || Seq: 130 / 172 || loss: 24.2464960542404\n",
      "Epoch:  103 || Seq: 140 / 172 || loss: 95.18478770005076\n",
      "Epoch:  103 || Seq: 150 / 172 || loss: 214.75991295123922\n",
      "Epoch:  103 || Seq: 160 / 172 || loss: 378.2991349029541\n",
      "Epoch:  103 || Seq: 170 / 172 || loss: 132.32339965105058\n",
      "Epoch:  103 || Loss:  173.32455165258403\n",
      "Epoch:  104 || Seq: 0 / 172 || loss: 115.66472159491644\n",
      "Epoch:  104 || Seq: 10 / 172 || loss: 190.57840879900115\n",
      "Epoch:  104 || Seq: 20 / 172 || loss: 250.59409101804098\n",
      "Epoch:  104 || Seq: 30 / 172 || loss: 27.905049346742175\n",
      "Epoch:  104 || Seq: 40 / 172 || loss: 394.4250699189993\n",
      "Epoch:  104 || Seq: 50 / 172 || loss: 424.7626399596532\n",
      "Epoch:  104 || Seq: 60 / 172 || loss: 239.0317211151123\n",
      "Epoch:  104 || Seq: 70 / 172 || loss: 74.26839315248071\n",
      "Epoch:  104 || Seq: 80 / 172 || loss: 178.59640620304987\n",
      "Epoch:  104 || Seq: 90 / 172 || loss: 32.197264264027275\n",
      "Epoch:  104 || Seq: 100 / 172 || loss: 448.4507189800865\n",
      "Epoch:  104 || Seq: 110 / 172 || loss: 165.38907885551453\n",
      "Epoch:  104 || Seq: 120 / 172 || loss: 13.131650863134343\n",
      "Epoch:  104 || Seq: 130 / 172 || loss: 227.2814937773205\n",
      "Epoch:  104 || Seq: 140 / 172 || loss: 304.74636639719427\n",
      "Epoch:  104 || Seq: 150 / 172 || loss: 297.2847842261905\n",
      "Epoch:  104 || Seq: 160 / 172 || loss: 391.13921880722046\n",
      "Epoch:  104 || Seq: 170 / 172 || loss: 197.27021966212325\n",
      "Epoch:  104 || Loss:  172.02622330785226\n",
      "Epoch:  105 || Seq: 0 / 172 || loss: 226.89837229251862\n",
      "Epoch:  105 || Seq: 10 / 172 || loss: 309.4200819206238\n",
      "Epoch:  105 || Seq: 20 / 172 || loss: 454.6684014456613\n",
      "Epoch:  105 || Seq: 30 / 172 || loss: 33.43868052332025\n",
      "Epoch:  105 || Seq: 40 / 172 || loss: 266.8060158177426\n",
      "Epoch:  105 || Seq: 50 / 172 || loss: 86.69725281851632\n",
      "Epoch:  105 || Seq: 60 / 172 || loss: 133.83963262347075\n",
      "Epoch:  105 || Seq: 70 / 172 || loss: 345.7559617115901\n",
      "Epoch:  105 || Seq: 80 / 172 || loss: 87.70911985635757\n",
      "Epoch:  105 || Seq: 90 / 172 || loss: 386.2728151321411\n",
      "Epoch:  105 || Seq: 100 / 172 || loss: 25.267384860826574\n",
      "Epoch:  105 || Seq: 110 / 172 || loss: 263.0198735131158\n",
      "Epoch:  105 || Seq: 120 / 172 || loss: 289.98306341171264\n",
      "Epoch:  105 || Seq: 130 / 172 || loss: 204.71823916715735\n",
      "Epoch:  105 || Seq: 140 / 172 || loss: 291.93855165180406\n",
      "Epoch:  105 || Seq: 150 / 172 || loss: 337.11714511447485\n",
      "Epoch:  105 || Seq: 160 / 172 || loss: 42.11333870887756\n",
      "Epoch:  105 || Seq: 170 / 172 || loss: 63.70239202181498\n",
      "Epoch:  105 || Loss:  172.57549110467224\n",
      "Epoch:  106 || Seq: 0 / 172 || loss: 214.28083610534668\n",
      "Epoch:  106 || Seq: 10 / 172 || loss: 15.632617654604719\n",
      "Epoch:  106 || Seq: 20 / 172 || loss: 234.5385880103478\n",
      "Epoch:  106 || Seq: 30 / 172 || loss: 175.24925686064222\n",
      "Epoch:  106 || Seq: 40 / 172 || loss: 12.928239464759827\n",
      "Epoch:  106 || Seq: 50 / 172 || loss: 447.9751064076143\n",
      "Epoch:  106 || Seq: 60 / 172 || loss: 199.8271541595459\n",
      "Epoch:  106 || Seq: 70 / 172 || loss: 9.57536521460861\n",
      "Epoch:  106 || Seq: 80 / 172 || loss: 350.5285266821201\n",
      "Epoch:  106 || Seq: 90 / 172 || loss: 125.31242386500041\n",
      "Epoch:  106 || Seq: 100 / 172 || loss: 231.85032211650503\n",
      "Epoch:  106 || Seq: 110 / 172 || loss: 170.65646573475428\n",
      "Epoch:  106 || Seq: 120 / 172 || loss: 275.6027241547902\n",
      "Epoch:  106 || Seq: 130 / 172 || loss: 344.9663311640422\n",
      "Epoch:  106 || Seq: 140 / 172 || loss: 440.2149875634595\n",
      "Epoch:  106 || Seq: 150 / 172 || loss: 150.81794834136963\n",
      "Epoch:  106 || Seq: 160 / 172 || loss: 117.90883895874023\n",
      "Epoch:  106 || Seq: 170 / 172 || loss: 462.17452875773114\n",
      "Epoch:  106 || Loss:  171.9909086353387\n",
      "Epoch:  107 || Seq: 0 / 172 || loss: 35.701626777648926\n",
      "Epoch:  107 || Seq: 10 / 172 || loss: 265.25454076131183\n",
      "Epoch:  107 || Seq: 20 / 172 || loss: 434.6490864753723\n",
      "Epoch:  107 || Seq: 30 / 172 || loss: 15.204259713490805\n",
      "Epoch:  107 || Seq: 40 / 172 || loss: 122.01428207984337\n",
      "Epoch:  107 || Seq: 50 / 172 || loss: 385.1925352536715\n",
      "Epoch:  107 || Seq: 60 / 172 || loss: 179.29967750202525\n",
      "Epoch:  107 || Seq: 70 / 172 || loss: 362.86163245307074\n",
      "Epoch:  107 || Seq: 80 / 172 || loss: 1.3554102932546466\n",
      "Epoch:  107 || Seq: 90 / 172 || loss: 368.2739052405724\n",
      "Epoch:  107 || Seq: 100 / 172 || loss: 257.4697346687317\n",
      "Epoch:  107 || Seq: 110 / 172 || loss: 382.2447706858317\n",
      "Epoch:  107 || Seq: 120 / 172 || loss: 8.41510293985668\n",
      "Epoch:  107 || Seq: 130 / 172 || loss: 19.268141098320484\n",
      "Epoch:  107 || Seq: 140 / 172 || loss: 127.630929366402\n",
      "Epoch:  107 || Seq: 150 / 172 || loss: 46.79410972854211\n",
      "Epoch:  107 || Seq: 160 / 172 || loss: 209.03197193145752\n",
      "Epoch:  107 || Seq: 170 / 172 || loss: 14.02699635530773\n",
      "Epoch:  107 || Loss:  177.3165744965043\n",
      "Epoch:  108 || Seq: 0 / 172 || loss: 54.57658891677856\n",
      "Epoch:  108 || Seq: 10 / 172 || loss: 75.91818237304688\n",
      "Epoch:  108 || Seq: 20 / 172 || loss: 71.8011767578125\n",
      "Epoch:  108 || Seq: 30 / 172 || loss: 221.63425954182944\n",
      "Epoch:  108 || Seq: 40 / 172 || loss: 56.944992916117755\n",
      "Epoch:  108 || Seq: 50 / 172 || loss: 33.94803800806403\n",
      "Epoch:  108 || Seq: 60 / 172 || loss: 151.7934421609949\n",
      "Epoch:  108 || Seq: 70 / 172 || loss: 164.19984844326973\n",
      "Epoch:  108 || Seq: 80 / 172 || loss: 39.62624450824556\n",
      "Epoch:  108 || Seq: 90 / 172 || loss: 5.385745345018222\n",
      "Epoch:  108 || Seq: 100 / 172 || loss: 275.12225768566134\n",
      "Epoch:  108 || Seq: 110 / 172 || loss: 304.8232153029669\n",
      "Epoch:  108 || Seq: 120 / 172 || loss: 135.54938868818613\n",
      "Epoch:  108 || Seq: 130 / 172 || loss: 146.44170790452225\n",
      "Epoch:  108 || Seq: 140 / 172 || loss: 9.313873402415574\n",
      "Epoch:  108 || Seq: 150 / 172 || loss: 290.24549023310345\n",
      "Epoch:  108 || Seq: 160 / 172 || loss: 192.876490873449\n",
      "Epoch:  108 || Seq: 170 / 172 || loss: 255.8871794613925\n",
      "Epoch:  108 || Loss:  166.41202284250474\n",
      "Epoch:  109 || Seq: 0 / 172 || loss: 243.3256607055664\n",
      "Epoch:  109 || Seq: 10 / 172 || loss: 199.7546871358698\n",
      "Epoch:  109 || Seq: 20 / 172 || loss: 258.2088466557589\n",
      "Epoch:  109 || Seq: 30 / 172 || loss: 27.766551243631465\n",
      "Epoch:  109 || Seq: 40 / 172 || loss: 201.58804416656494\n",
      "Epoch:  109 || Seq: 50 / 172 || loss: 203.96192766272503\n",
      "Epoch:  109 || Seq: 60 / 172 || loss: 30.157848685004172\n",
      "Epoch:  109 || Seq: 70 / 172 || loss: 295.4493582438339\n",
      "Epoch:  109 || Seq: 80 / 172 || loss: 389.9197689667344\n",
      "Epoch:  109 || Seq: 90 / 172 || loss: 6.4071569873006355\n",
      "Epoch:  109 || Seq: 100 / 172 || loss: 314.6073980023784\n",
      "Epoch:  109 || Seq: 110 / 172 || loss: 219.83969679332915\n",
      "Epoch:  109 || Seq: 120 / 172 || loss: 19.903031170368195\n",
      "Epoch:  109 || Seq: 130 / 172 || loss: 34.8149419148763\n",
      "Epoch:  109 || Seq: 140 / 172 || loss: 279.471577493768\n",
      "Epoch:  109 || Seq: 150 / 172 || loss: 339.22932192858525\n",
      "Epoch:  109 || Seq: 160 / 172 || loss: 206.0519066398794\n",
      "Epoch:  109 || Seq: 170 / 172 || loss: 533.7227429199219\n",
      "Epoch:  109 || Loss:  168.8173124658983\n",
      "Epoch:  110 || Seq: 0 / 172 || loss: 10.35663314642046\n",
      "Epoch:  110 || Seq: 10 / 172 || loss: 57.94569609804373\n",
      "Epoch:  110 || Seq: 20 / 172 || loss: 328.71159133911135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  110 || Seq: 30 / 172 || loss: 8.146857768297195\n",
      "Epoch:  110 || Seq: 40 / 172 || loss: 279.8096141199912\n",
      "Epoch:  110 || Seq: 50 / 172 || loss: 115.94602421909804\n",
      "Epoch:  110 || Seq: 60 / 172 || loss: 206.27910410563152\n",
      "Epoch:  110 || Seq: 70 / 172 || loss: 45.81143355369568\n",
      "Epoch:  110 || Seq: 80 / 172 || loss: 50.18365586371649\n",
      "Epoch:  110 || Seq: 90 / 172 || loss: 3.7767375239024035\n",
      "Epoch:  110 || Seq: 100 / 172 || loss: 75.07371783256531\n",
      "Epoch:  110 || Seq: 110 / 172 || loss: 47.401743221282956\n",
      "Epoch:  110 || Seq: 120 / 172 || loss: 86.88957009996686\n",
      "Epoch:  110 || Seq: 130 / 172 || loss: 211.65007963634673\n",
      "Epoch:  110 || Seq: 140 / 172 || loss: 154.4742317199707\n",
      "Epoch:  110 || Seq: 150 / 172 || loss: 21.12568686803182\n",
      "Epoch:  110 || Seq: 160 / 172 || loss: 254.1183586854201\n",
      "Epoch:  110 || Seq: 170 / 172 || loss: 98.496033008282\n",
      "Epoch:  110 || Loss:  167.61334827741686\n",
      "Epoch:  111 || Seq: 0 / 172 || loss: 256.45363896687826\n",
      "Epoch:  111 || Seq: 10 / 172 || loss: 403.56699162263135\n",
      "Epoch:  111 || Seq: 20 / 172 || loss: 32.64565668106079\n",
      "Epoch:  111 || Seq: 30 / 172 || loss: 292.98973019917804\n",
      "Epoch:  111 || Seq: 40 / 172 || loss: 175.5830036799113\n",
      "Epoch:  111 || Seq: 50 / 172 || loss: 146.51411933898925\n",
      "Epoch:  111 || Seq: 60 / 172 || loss: 305.36815053766423\n",
      "Epoch:  111 || Seq: 70 / 172 || loss: 34.97321558992068\n",
      "Epoch:  111 || Seq: 80 / 172 || loss: 160.4324256181717\n",
      "Epoch:  111 || Seq: 90 / 172 || loss: 157.59977083206178\n",
      "Epoch:  111 || Seq: 100 / 172 || loss: 4.354365556784298\n",
      "Epoch:  111 || Seq: 110 / 172 || loss: 23.086732841673353\n",
      "Epoch:  111 || Seq: 120 / 172 || loss: 432.6614646911621\n",
      "Epoch:  111 || Seq: 130 / 172 || loss: 41.6906526312232\n",
      "Epoch:  111 || Seq: 140 / 172 || loss: 193.92870819568634\n",
      "Epoch:  111 || Seq: 150 / 172 || loss: 227.2761597999412\n",
      "Epoch:  111 || Seq: 160 / 172 || loss: 208.50169758001962\n",
      "Epoch:  111 || Seq: 170 / 172 || loss: 8.185855687991715\n",
      "Epoch:  111 || Loss:  165.9738864372456\n",
      "Epoch:  112 || Seq: 0 / 172 || loss: 169.19609985500574\n",
      "Epoch:  112 || Seq: 10 / 172 || loss: 406.7681184387207\n",
      "Epoch:  112 || Seq: 20 / 172 || loss: 77.95450159758329\n",
      "Epoch:  112 || Seq: 30 / 172 || loss: 7.700436623472917\n",
      "Epoch:  112 || Seq: 40 / 172 || loss: 263.57687412608755\n",
      "Epoch:  112 || Seq: 50 / 172 || loss: 137.9821094390063\n",
      "Epoch:  112 || Seq: 60 / 172 || loss: 199.69604201938796\n",
      "Epoch:  112 || Seq: 70 / 172 || loss: 247.08213766689957\n",
      "Epoch:  112 || Seq: 80 / 172 || loss: 164.4786603053411\n",
      "Epoch:  112 || Seq: 90 / 172 || loss: 87.20701747105039\n",
      "Epoch:  112 || Seq: 100 / 172 || loss: 123.4867670395795\n",
      "Epoch:  112 || Seq: 110 / 172 || loss: 7.640731147357395\n",
      "Epoch:  112 || Seq: 120 / 172 || loss: 286.6040086746216\n",
      "Epoch:  112 || Seq: 130 / 172 || loss: 134.2816305894118\n",
      "Epoch:  112 || Seq: 140 / 172 || loss: 41.653993252664804\n",
      "Epoch:  112 || Seq: 150 / 172 || loss: 57.38371040788479\n",
      "Epoch:  112 || Seq: 160 / 172 || loss: 39.869136929512024\n",
      "Epoch:  112 || Seq: 170 / 172 || loss: 200.0840151468913\n",
      "Epoch:  112 || Loss:  174.9380510991827\n",
      "Epoch:  113 || Seq: 0 / 172 || loss: 12.462873254747441\n",
      "Epoch:  113 || Seq: 10 / 172 || loss: 452.2736665010452\n",
      "Epoch:  113 || Seq: 20 / 172 || loss: 35.486172480508685\n",
      "Epoch:  113 || Seq: 30 / 172 || loss: 87.95728475397283\n",
      "Epoch:  113 || Seq: 40 / 172 || loss: 259.0198437690735\n",
      "Epoch:  113 || Seq: 50 / 172 || loss: 169.64298980466782\n",
      "Epoch:  113 || Seq: 60 / 172 || loss: 83.15123398900032\n",
      "Epoch:  113 || Seq: 70 / 172 || loss: 94.1314069155989\n",
      "Epoch:  113 || Seq: 80 / 172 || loss: 323.42351862589516\n",
      "Epoch:  113 || Seq: 90 / 172 || loss: 7.483916949480772\n",
      "Epoch:  113 || Seq: 100 / 172 || loss: 25.04947555065155\n",
      "Epoch:  113 || Seq: 110 / 172 || loss: 39.24507631195916\n",
      "Epoch:  113 || Seq: 120 / 172 || loss: 177.86968503892422\n",
      "Epoch:  113 || Seq: 130 / 172 || loss: 6.220622190657784\n",
      "Epoch:  113 || Seq: 140 / 172 || loss: 148.98144577940306\n",
      "Epoch:  113 || Seq: 150 / 172 || loss: 10.294242272774378\n",
      "Epoch:  113 || Seq: 160 / 172 || loss: 11.074396327137947\n",
      "Epoch:  113 || Seq: 170 / 172 || loss: 417.7470194498698\n",
      "Epoch:  113 || Loss:  170.23805246075727\n",
      "Epoch:  114 || Seq: 0 / 172 || loss: 9.262906513765062\n",
      "Epoch:  114 || Seq: 10 / 172 || loss: 173.2938003540039\n",
      "Epoch:  114 || Seq: 20 / 172 || loss: 192.6477957367897\n",
      "Epoch:  114 || Seq: 30 / 172 || loss: 205.61376870473225\n",
      "Epoch:  114 || Seq: 40 / 172 || loss: 8.399436380714178\n",
      "Epoch:  114 || Seq: 50 / 172 || loss: 291.8908493644313\n",
      "Epoch:  114 || Seq: 60 / 172 || loss: 225.8240958113562\n",
      "Epoch:  114 || Seq: 70 / 172 || loss: 5.481021676580177\n",
      "Epoch:  114 || Seq: 80 / 172 || loss: 206.15534901618958\n",
      "Epoch:  114 || Seq: 90 / 172 || loss: 14.360850535541921\n",
      "Epoch:  114 || Seq: 100 / 172 || loss: 467.78343017284686\n",
      "Epoch:  114 || Seq: 110 / 172 || loss: 165.9500334866737\n",
      "Epoch:  114 || Seq: 120 / 172 || loss: 38.937311232089996\n",
      "Epoch:  114 || Seq: 130 / 172 || loss: 263.5787568933824\n",
      "Epoch:  114 || Seq: 140 / 172 || loss: 125.60690422058106\n",
      "Epoch:  114 || Seq: 150 / 172 || loss: 10.420381056791586\n",
      "Epoch:  114 || Seq: 160 / 172 || loss: 299.40143273093486\n",
      "Epoch:  114 || Seq: 170 / 172 || loss: 21.84868114301935\n",
      "Epoch:  114 || Loss:  175.46414748783715\n",
      "Epoch:  115 || Seq: 0 / 172 || loss: 265.64064795882615\n",
      "Epoch:  115 || Seq: 10 / 172 || loss: 238.1785373687744\n",
      "Epoch:  115 || Seq: 20 / 172 || loss: 9.712284771991628\n",
      "Epoch:  115 || Seq: 30 / 172 || loss: 20.258501689043204\n",
      "Epoch:  115 || Seq: 40 / 172 || loss: 178.89292321886336\n",
      "Epoch:  115 || Seq: 50 / 172 || loss: 481.0265255978233\n",
      "Epoch:  115 || Seq: 60 / 172 || loss: 244.98785624137292\n",
      "Epoch:  115 || Seq: 70 / 172 || loss: 84.83150364802434\n",
      "Epoch:  115 || Seq: 80 / 172 || loss: 184.0894859880209\n",
      "Epoch:  115 || Seq: 90 / 172 || loss: 140.6086920186093\n",
      "Epoch:  115 || Seq: 100 / 172 || loss: 119.44210925855134\n",
      "Epoch:  115 || Seq: 110 / 172 || loss: 9.022036903670855\n",
      "Epoch:  115 || Seq: 120 / 172 || loss: 201.82055681687305\n",
      "Epoch:  115 || Seq: 130 / 172 || loss: 23.83185817629454\n",
      "Epoch:  115 || Seq: 140 / 172 || loss: 188.26943262418112\n",
      "Epoch:  115 || Seq: 150 / 172 || loss: 319.33635945753616\n",
      "Epoch:  115 || Seq: 160 / 172 || loss: 19.696813106536865\n",
      "Epoch:  115 || Seq: 170 / 172 || loss: 427.22899571587055\n",
      "Epoch:  115 || Loss:  177.34558784984782\n",
      "Epoch:  116 || Seq: 0 / 172 || loss: 400.4159847606312\n",
      "Epoch:  116 || Seq: 10 / 172 || loss: 376.4013723645891\n",
      "Epoch:  116 || Seq: 20 / 172 || loss: 44.7310171941894\n",
      "Epoch:  116 || Seq: 30 / 172 || loss: 38.108209316547104\n",
      "Epoch:  116 || Seq: 40 / 172 || loss: 7.476724003490649\n",
      "Epoch:  116 || Seq: 50 / 172 || loss: 72.90990161895752\n",
      "Epoch:  116 || Seq: 60 / 172 || loss: 59.64391460418701\n",
      "Epoch:  116 || Seq: 70 / 172 || loss: 318.6979144536532\n",
      "Epoch:  116 || Seq: 80 / 172 || loss: 394.11005306243896\n",
      "Epoch:  116 || Seq: 90 / 172 || loss: 364.06828144073484\n",
      "Epoch:  116 || Seq: 100 / 172 || loss: 213.31083295219824\n",
      "Epoch:  116 || Seq: 110 / 172 || loss: 116.68153507709503\n",
      "Epoch:  116 || Seq: 120 / 172 || loss: 93.59236916251804\n",
      "Epoch:  116 || Seq: 130 / 172 || loss: 3.883713862962193\n",
      "Epoch:  116 || Seq: 140 / 172 || loss: 20.302456919963543\n",
      "Epoch:  116 || Seq: 150 / 172 || loss: 50.232316398620604\n",
      "Epoch:  116 || Seq: 160 / 172 || loss: 369.0568333731757\n",
      "Epoch:  116 || Seq: 170 / 172 || loss: 249.3567873084027\n",
      "Epoch:  116 || Loss:  169.9873276455562\n",
      "Epoch:  117 || Seq: 0 / 172 || loss: 93.68204206228256\n",
      "Epoch:  117 || Seq: 10 / 172 || loss: 93.04129242897034\n",
      "Epoch:  117 || Seq: 20 / 172 || loss: 141.69921894073485\n",
      "Epoch:  117 || Seq: 30 / 172 || loss: 54.855325031280515\n",
      "Epoch:  117 || Seq: 40 / 172 || loss: 1.3480332667365604\n",
      "Epoch:  117 || Seq: 50 / 172 || loss: 17.426758321126304\n",
      "Epoch:  117 || Seq: 60 / 172 || loss: 50.39547602335612\n",
      "Epoch:  117 || Seq: 70 / 172 || loss: 27.500625049366672\n",
      "Epoch:  117 || Seq: 80 / 172 || loss: 121.30444732666015\n",
      "Epoch:  117 || Seq: 90 / 172 || loss: 73.23325087213888\n",
      "Epoch:  117 || Seq: 100 / 172 || loss: 35.816630372875615\n",
      "Epoch:  117 || Seq: 110 / 172 || loss: 81.48216099574648\n",
      "Epoch:  117 || Seq: 120 / 172 || loss: 100.789855333631\n",
      "Epoch:  117 || Seq: 130 / 172 || loss: 223.59480945880597\n",
      "Epoch:  117 || Seq: 140 / 172 || loss: 46.43703105714586\n",
      "Epoch:  117 || Seq: 150 / 172 || loss: 176.66204250560088\n",
      "Epoch:  117 || Seq: 160 / 172 || loss: 233.90287467766552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  117 || Seq: 170 / 172 || loss: 71.85996748850896\n",
      "Epoch:  117 || Loss:  170.55130921935609\n",
      "Epoch:  118 || Seq: 0 / 172 || loss: 57.59023582935333\n",
      "Epoch:  118 || Seq: 10 / 172 || loss: 117.78181985708383\n",
      "Epoch:  118 || Seq: 20 / 172 || loss: 101.89977149168651\n",
      "Epoch:  118 || Seq: 30 / 172 || loss: 401.79774375583816\n",
      "Epoch:  118 || Seq: 40 / 172 || loss: 416.0406232561384\n",
      "Epoch:  118 || Seq: 50 / 172 || loss: 269.3784860309802\n",
      "Epoch:  118 || Seq: 60 / 172 || loss: 7.394460752484433\n",
      "Epoch:  118 || Seq: 70 / 172 || loss: 242.72089297771453\n",
      "Epoch:  118 || Seq: 80 / 172 || loss: 143.51511588463416\n",
      "Epoch:  118 || Seq: 90 / 172 || loss: 42.471140480041505\n",
      "Epoch:  118 || Seq: 100 / 172 || loss: 27.549252250364848\n",
      "Epoch:  118 || Seq: 110 / 172 || loss: 386.0720491409302\n",
      "Epoch:  118 || Seq: 120 / 172 || loss: 150.39960405561658\n",
      "Epoch:  118 || Seq: 130 / 172 || loss: 10.854068623412223\n",
      "Epoch:  118 || Seq: 140 / 172 || loss: 154.8272507455614\n",
      "Epoch:  118 || Seq: 150 / 172 || loss: 28.39245393541124\n",
      "Epoch:  118 || Seq: 160 / 172 || loss: 192.94577859334956\n",
      "Epoch:  118 || Seq: 170 / 172 || loss: 251.00892886748682\n",
      "Epoch:  118 || Loss:  170.08759232112536\n",
      "Epoch:  119 || Seq: 0 / 172 || loss: 255.14388005873735\n",
      "Epoch:  119 || Seq: 10 / 172 || loss: 367.52162742614746\n",
      "Epoch:  119 || Seq: 20 / 172 || loss: 158.91577844775242\n",
      "Epoch:  119 || Seq: 30 / 172 || loss: 174.77617559649727\n",
      "Epoch:  119 || Seq: 40 / 172 || loss: 108.08460070536687\n",
      "Epoch:  119 || Seq: 50 / 172 || loss: 134.37118480080053\n",
      "Epoch:  119 || Seq: 60 / 172 || loss: 6.411396003904796\n",
      "Epoch:  119 || Seq: 70 / 172 || loss: 130.574952545166\n",
      "Epoch:  119 || Seq: 80 / 172 || loss: 251.4853761846369\n",
      "Epoch:  119 || Seq: 90 / 172 || loss: 248.36378421783448\n",
      "Epoch:  119 || Seq: 100 / 172 || loss: 8.098277161664823\n",
      "Epoch:  119 || Seq: 110 / 172 || loss: 105.12762931415013\n",
      "Epoch:  119 || Seq: 120 / 172 || loss: 284.04599975794554\n",
      "Epoch:  119 || Seq: 130 / 172 || loss: 162.5062255859375\n",
      "Epoch:  119 || Seq: 140 / 172 || loss: 396.7828660583496\n",
      "Epoch:  119 || Seq: 150 / 172 || loss: 18.07698906213045\n",
      "Epoch:  119 || Seq: 160 / 172 || loss: 121.95862214116809\n",
      "Epoch:  119 || Seq: 170 / 172 || loss: 25.703213024139405\n",
      "Epoch:  119 || Loss:  170.88028712045494\n",
      "Change learning rate to:  1.0000000000000002e-07\n",
      "Epoch:  120 || Seq: 0 / 172 || loss: 71.40418463945389\n",
      "Epoch:  120 || Seq: 10 / 172 || loss: 111.99641275405884\n",
      "Epoch:  120 || Seq: 20 / 172 || loss: 32.85592982036511\n",
      "Epoch:  120 || Seq: 30 / 172 || loss: 224.75044087802664\n",
      "Epoch:  120 || Seq: 40 / 172 || loss: 182.98271687825522\n",
      "Epoch:  120 || Seq: 50 / 172 || loss: 139.18314788818358\n",
      "Epoch:  120 || Seq: 60 / 172 || loss: 189.2223866780599\n",
      "Epoch:  120 || Seq: 70 / 172 || loss: 379.568873332097\n",
      "Epoch:  120 || Seq: 80 / 172 || loss: 81.19780239305999\n",
      "Epoch:  120 || Seq: 90 / 172 || loss: 20.085699160893757\n",
      "Epoch:  120 || Seq: 100 / 172 || loss: 35.33387631177902\n",
      "Epoch:  120 || Seq: 110 / 172 || loss: 443.1184105873108\n",
      "Epoch:  120 || Seq: 120 / 172 || loss: 554.1098664601644\n",
      "Epoch:  120 || Seq: 130 / 172 || loss: 203.03104405932956\n",
      "Epoch:  120 || Seq: 140 / 172 || loss: 168.8839549014443\n",
      "Epoch:  120 || Seq: 150 / 172 || loss: 30.078300521487282\n",
      "Epoch:  120 || Seq: 160 / 172 || loss: 380.5857396443685\n",
      "Epoch:  120 || Seq: 170 / 172 || loss: 43.15898479769627\n",
      "Epoch:  120 || Loss:  170.52602280610066\n",
      "Epoch:  121 || Seq: 0 / 172 || loss: 326.5569118923611\n",
      "Epoch:  121 || Seq: 10 / 172 || loss: 10.785525566790826\n",
      "Epoch:  121 || Seq: 20 / 172 || loss: 331.58852923357927\n",
      "Epoch:  121 || Seq: 30 / 172 || loss: 59.7299643301151\n",
      "Epoch:  121 || Seq: 40 / 172 || loss: 361.51164363861085\n",
      "Epoch:  121 || Seq: 50 / 172 || loss: 44.38490765889485\n",
      "Epoch:  121 || Seq: 60 / 172 || loss: 4.736463392419474\n",
      "Epoch:  121 || Seq: 70 / 172 || loss: 99.53520349355844\n",
      "Epoch:  121 || Seq: 80 / 172 || loss: 73.84209460505973\n",
      "Epoch:  121 || Seq: 90 / 172 || loss: 150.97448560926648\n",
      "Epoch:  121 || Seq: 100 / 172 || loss: 274.5591474533081\n",
      "Epoch:  121 || Seq: 110 / 172 || loss: 220.0766246102073\n",
      "Epoch:  121 || Seq: 120 / 172 || loss: 93.02487950575978\n",
      "Epoch:  121 || Seq: 130 / 172 || loss: 19.79788563126012\n",
      "Epoch:  121 || Seq: 140 / 172 || loss: 100.25482334810145\n",
      "Epoch:  121 || Seq: 150 / 172 || loss: 133.21708749469957\n",
      "Epoch:  121 || Seq: 160 / 172 || loss: 120.20667930271314\n",
      "Epoch:  121 || Seq: 170 / 172 || loss: 167.43455198022627\n",
      "Epoch:  121 || Loss:  168.32824459875917\n",
      "Epoch:  122 || Seq: 0 / 172 || loss: 138.66396236419678\n",
      "Epoch:  122 || Seq: 10 / 172 || loss: 201.96376355489096\n",
      "Epoch:  122 || Seq: 20 / 172 || loss: 229.71234448750815\n",
      "Epoch:  122 || Seq: 30 / 172 || loss: 335.171179991502\n",
      "Epoch:  122 || Seq: 40 / 172 || loss: 131.46896845134705\n",
      "Epoch:  122 || Seq: 50 / 172 || loss: 243.79733324050903\n",
      "Epoch:  122 || Seq: 60 / 172 || loss: 172.61401557922363\n",
      "Epoch:  122 || Seq: 70 / 172 || loss: 211.78124078114828\n",
      "Epoch:  122 || Seq: 80 / 172 || loss: 273.8291513486342\n",
      "Epoch:  122 || Seq: 90 / 172 || loss: 350.56414662874664\n",
      "Epoch:  122 || Seq: 100 / 172 || loss: 18.39930964178509\n",
      "Epoch:  122 || Seq: 110 / 172 || loss: 343.51483690171017\n",
      "Epoch:  122 || Seq: 120 / 172 || loss: 85.22643032073975\n",
      "Epoch:  122 || Seq: 130 / 172 || loss: 242.49287816097862\n",
      "Epoch:  122 || Seq: 140 / 172 || loss: 274.6132120203089\n",
      "Epoch:  122 || Seq: 150 / 172 || loss: 11.580186426639557\n",
      "Epoch:  122 || Seq: 160 / 172 || loss: 220.24086487293243\n",
      "Epoch:  122 || Seq: 170 / 172 || loss: 104.17789491017659\n",
      "Epoch:  122 || Loss:  171.10771263959276\n",
      "Epoch:  123 || Seq: 0 / 172 || loss: 1.9111729285546712\n",
      "Epoch:  123 || Seq: 10 / 172 || loss: 93.84873607706116\n",
      "Epoch:  123 || Seq: 20 / 172 || loss: 302.5145586187189\n",
      "Epoch:  123 || Seq: 30 / 172 || loss: 301.5070918233771\n",
      "Epoch:  123 || Seq: 40 / 172 || loss: 13.955694675445557\n",
      "Epoch:  123 || Seq: 50 / 172 || loss: 35.575936046118535\n",
      "Epoch:  123 || Seq: 60 / 172 || loss: 103.67751167297364\n",
      "Epoch:  123 || Seq: 70 / 172 || loss: 182.65880049044085\n",
      "Epoch:  123 || Seq: 80 / 172 || loss: 492.3780330657959\n",
      "Epoch:  123 || Seq: 90 / 172 || loss: 9.86545551344394\n",
      "Epoch:  123 || Seq: 100 / 172 || loss: 221.46870485941568\n",
      "Epoch:  123 || Seq: 110 / 172 || loss: 11.13210782637963\n",
      "Epoch:  123 || Seq: 120 / 172 || loss: 417.3631033216204\n",
      "Epoch:  123 || Seq: 130 / 172 || loss: 368.726442082723\n",
      "Epoch:  123 || Seq: 140 / 172 || loss: 380.08352453568403\n",
      "Epoch:  123 || Seq: 150 / 172 || loss: 363.2007401784261\n",
      "Epoch:  123 || Seq: 160 / 172 || loss: 221.65871231644243\n",
      "Epoch:  123 || Seq: 170 / 172 || loss: 152.57021822248186\n",
      "Epoch:  123 || Loss:  174.09154080174665\n",
      "Epoch:  124 || Seq: 0 / 172 || loss: 166.31194626657586\n",
      "Epoch:  124 || Seq: 10 / 172 || loss: 351.94550756974655\n",
      "Epoch:  124 || Seq: 20 / 172 || loss: 285.24746656417847\n",
      "Epoch:  124 || Seq: 30 / 172 || loss: 8.567999243736267\n",
      "Epoch:  124 || Seq: 40 / 172 || loss: 16.892167267046478\n",
      "Epoch:  124 || Seq: 50 / 172 || loss: 174.60293197631836\n",
      "Epoch:  124 || Seq: 60 / 172 || loss: 27.951042109727858\n",
      "Epoch:  124 || Seq: 70 / 172 || loss: 482.59395950317383\n",
      "Epoch:  124 || Seq: 80 / 172 || loss: 242.2365499643179\n",
      "Epoch:  124 || Seq: 90 / 172 || loss: 378.38950969696043\n",
      "Epoch:  124 || Seq: 100 / 172 || loss: 232.24164286526766\n",
      "Epoch:  124 || Seq: 110 / 172 || loss: 116.99588473637898\n",
      "Epoch:  124 || Seq: 120 / 172 || loss: 191.45069993336995\n",
      "Epoch:  124 || Seq: 130 / 172 || loss: 281.67298503355545\n",
      "Epoch:  124 || Seq: 140 / 172 || loss: 7.942243590950966\n",
      "Epoch:  124 || Seq: 150 / 172 || loss: 267.23627062197085\n",
      "Epoch:  124 || Seq: 160 / 172 || loss: 131.68287808512864\n",
      "Epoch:  124 || Seq: 170 / 172 || loss: 9.846233037596738\n",
      "Epoch:  124 || Loss:  172.6164122932569\n",
      "Epoch:  125 || Seq: 0 / 172 || loss: 343.1668114228682\n",
      "Epoch:  125 || Seq: 10 / 172 || loss: 128.21951217651366\n",
      "Epoch:  125 || Seq: 20 / 172 || loss: 512.796160252889\n",
      "Epoch:  125 || Seq: 30 / 172 || loss: 169.3809222711504\n",
      "Epoch:  125 || Seq: 40 / 172 || loss: 247.61850776672364\n",
      "Epoch:  125 || Seq: 50 / 172 || loss: 148.6287728881836\n",
      "Epoch:  125 || Seq: 60 / 172 || loss: 209.36127553473818\n",
      "Epoch:  125 || Seq: 70 / 172 || loss: 97.23173772692681\n",
      "Epoch:  125 || Seq: 80 / 172 || loss: 161.99866406122842\n",
      "Epoch:  125 || Seq: 90 / 172 || loss: 48.46789493758697\n",
      "Epoch:  125 || Seq: 100 / 172 || loss: 44.12816108190096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125 || Seq: 110 / 172 || loss: 8.921585619449615\n",
      "Epoch:  125 || Seq: 120 / 172 || loss: 139.84544471002394\n",
      "Epoch:  125 || Seq: 130 / 172 || loss: 26.359266155958174\n",
      "Epoch:  125 || Seq: 140 / 172 || loss: 111.60177128890464\n",
      "Epoch:  125 || Seq: 150 / 172 || loss: 242.33567449781629\n",
      "Epoch:  125 || Seq: 160 / 172 || loss: 346.82707336970736\n",
      "Epoch:  125 || Seq: 170 / 172 || loss: 28.94562043641743\n",
      "Epoch:  125 || Loss:  168.4312982619538\n",
      "Epoch:  126 || Seq: 0 / 172 || loss: 453.6405053138733\n",
      "Epoch:  126 || Seq: 10 / 172 || loss: 295.54597695668537\n",
      "Epoch:  126 || Seq: 20 / 172 || loss: 12.165681856019157\n",
      "Epoch:  126 || Seq: 30 / 172 || loss: 8.553824070375413\n",
      "Epoch:  126 || Seq: 40 / 172 || loss: 3.6794644698108496\n",
      "Epoch:  126 || Seq: 50 / 172 || loss: 327.0911602973938\n",
      "Epoch:  126 || Seq: 60 / 172 || loss: 246.32068753242493\n",
      "Epoch:  126 || Seq: 70 / 172 || loss: 295.5489911528734\n",
      "Epoch:  126 || Seq: 80 / 172 || loss: 6.6758823694013385\n",
      "Epoch:  126 || Seq: 90 / 172 || loss: 107.16916988741967\n",
      "Epoch:  126 || Seq: 100 / 172 || loss: 69.30702791715923\n",
      "Epoch:  126 || Seq: 110 / 172 || loss: 10.332688084003166\n",
      "Epoch:  126 || Seq: 120 / 172 || loss: 2.8363033395881456\n",
      "Epoch:  126 || Seq: 130 / 172 || loss: 570.6729347229004\n",
      "Epoch:  126 || Seq: 140 / 172 || loss: 26.40298279788759\n",
      "Epoch:  126 || Seq: 150 / 172 || loss: 35.28262419700623\n",
      "Epoch:  126 || Seq: 160 / 172 || loss: 85.82776389886503\n",
      "Epoch:  126 || Seq: 170 / 172 || loss: 262.4315992656507\n",
      "Epoch:  126 || Loss:  172.617614495455\n",
      "Epoch:  127 || Seq: 0 / 172 || loss: 313.2799401601156\n",
      "Epoch:  127 || Seq: 10 / 172 || loss: 190.44019048354204\n",
      "Epoch:  127 || Seq: 20 / 172 || loss: 311.94469651579857\n",
      "Epoch:  127 || Seq: 30 / 172 || loss: 375.18817234039307\n",
      "Epoch:  127 || Seq: 40 / 172 || loss: 34.86953042103694\n",
      "Epoch:  127 || Seq: 50 / 172 || loss: 107.03736144617984\n",
      "Epoch:  127 || Seq: 60 / 172 || loss: 22.10994029045105\n",
      "Epoch:  127 || Seq: 70 / 172 || loss: 129.47648965611177\n",
      "Epoch:  127 || Seq: 80 / 172 || loss: 350.4108909260143\n",
      "Epoch:  127 || Seq: 90 / 172 || loss: 203.48229954792902\n",
      "Epoch:  127 || Seq: 100 / 172 || loss: 336.9344648881392\n",
      "Epoch:  127 || Seq: 110 / 172 || loss: 46.91341018676758\n",
      "Epoch:  127 || Seq: 120 / 172 || loss: 118.281498336792\n",
      "Epoch:  127 || Seq: 130 / 172 || loss: 121.66958281786546\n",
      "Epoch:  127 || Seq: 140 / 172 || loss: 396.09815657138824\n",
      "Epoch:  127 || Seq: 150 / 172 || loss: 403.2893560028076\n",
      "Epoch:  127 || Seq: 160 / 172 || loss: 219.7545623779297\n",
      "Epoch:  127 || Seq: 170 / 172 || loss: 160.9416987049964\n",
      "Epoch:  127 || Loss:  168.27579208434062\n",
      "Epoch:  128 || Seq: 0 / 172 || loss: 27.195131142934162\n",
      "Epoch:  128 || Seq: 10 / 172 || loss: 138.57155713168058\n",
      "Epoch:  128 || Seq: 20 / 172 || loss: 32.38695350266062\n",
      "Epoch:  128 || Seq: 30 / 172 || loss: 229.57220001220702\n",
      "Epoch:  128 || Seq: 40 / 172 || loss: 85.19462233004363\n",
      "Epoch:  128 || Seq: 50 / 172 || loss: 179.10314759341153\n",
      "Epoch:  128 || Seq: 60 / 172 || loss: 406.16003624598187\n",
      "Epoch:  128 || Seq: 70 / 172 || loss: 18.75224814414978\n",
      "Epoch:  128 || Seq: 80 / 172 || loss: 235.20269563463\n",
      "Epoch:  128 || Seq: 90 / 172 || loss: 23.318630175996358\n",
      "Epoch:  128 || Seq: 100 / 172 || loss: 109.80078698970654\n",
      "Epoch:  128 || Seq: 110 / 172 || loss: 44.06067469716072\n",
      "Epoch:  128 || Seq: 120 / 172 || loss: 48.48868790268898\n",
      "Epoch:  128 || Seq: 130 / 172 || loss: 244.0744477185336\n",
      "Epoch:  128 || Seq: 140 / 172 || loss: 530.5685882568359\n",
      "Epoch:  128 || Seq: 150 / 172 || loss: 381.21141242980957\n",
      "Epoch:  128 || Seq: 160 / 172 || loss: 155.02941710608346\n",
      "Epoch:  128 || Seq: 170 / 172 || loss: 192.20977380275727\n",
      "Epoch:  128 || Loss:  165.8284407430773\n",
      "Epoch:  129 || Seq: 0 / 172 || loss: 3.8967329123755916\n",
      "Epoch:  129 || Seq: 10 / 172 || loss: 286.4626427526059\n",
      "Epoch:  129 || Seq: 20 / 172 || loss: 157.75647972689734\n",
      "Epoch:  129 || Seq: 30 / 172 || loss: 9.082811411747054\n",
      "Epoch:  129 || Seq: 40 / 172 || loss: 43.77817530381052\n",
      "Epoch:  129 || Seq: 50 / 172 || loss: 184.0609370470047\n",
      "Epoch:  129 || Seq: 60 / 172 || loss: 215.61009407043457\n",
      "Epoch:  129 || Seq: 70 / 172 || loss: 196.50789953624047\n",
      "Epoch:  129 || Seq: 80 / 172 || loss: 46.653371810913086\n",
      "Epoch:  129 || Seq: 90 / 172 || loss: 144.20960587721603\n",
      "Epoch:  129 || Seq: 100 / 172 || loss: 2.8838013397673\n",
      "Epoch:  129 || Seq: 110 / 172 || loss: 15.090313822031021\n",
      "Epoch:  129 || Seq: 120 / 172 || loss: 590.0933987544133\n",
      "Epoch:  129 || Seq: 130 / 172 || loss: 104.70887117385864\n",
      "Epoch:  129 || Seq: 140 / 172 || loss: 22.357194304466248\n",
      "Epoch:  129 || Seq: 150 / 172 || loss: 378.50361899768603\n",
      "Epoch:  129 || Seq: 160 / 172 || loss: 407.65553967158\n",
      "Epoch:  129 || Seq: 170 / 172 || loss: 24.570402450859547\n",
      "Epoch:  129 || Loss:  171.22060850758714\n",
      "Epoch:  130 || Seq: 0 / 172 || loss: 15.289408197277226\n",
      "Epoch:  130 || Seq: 10 / 172 || loss: 225.1729317540708\n",
      "Epoch:  130 || Seq: 20 / 172 || loss: 192.21045128504434\n",
      "Epoch:  130 || Seq: 30 / 172 || loss: 7.552028543070743\n",
      "Epoch:  130 || Seq: 40 / 172 || loss: 221.94255907248174\n",
      "Epoch:  130 || Seq: 50 / 172 || loss: 38.764446115493776\n",
      "Epoch:  130 || Seq: 60 / 172 || loss: 21.292140874544902\n",
      "Epoch:  130 || Seq: 70 / 172 || loss: 176.03936862945557\n",
      "Epoch:  130 || Seq: 80 / 172 || loss: 187.68783801368303\n",
      "Epoch:  130 || Seq: 90 / 172 || loss: 321.09349212646487\n",
      "Epoch:  130 || Seq: 100 / 172 || loss: 8.719049419843941\n",
      "Epoch:  130 || Seq: 110 / 172 || loss: 191.68219284784226\n",
      "Epoch:  130 || Seq: 120 / 172 || loss: 4.823588482325985\n",
      "Epoch:  130 || Seq: 130 / 172 || loss: 86.16043747273775\n",
      "Epoch:  130 || Seq: 140 / 172 || loss: 316.59570911952426\n",
      "Epoch:  130 || Seq: 150 / 172 || loss: 149.96473022460938\n",
      "Epoch:  130 || Seq: 160 / 172 || loss: 100.70935808306398\n",
      "Epoch:  130 || Seq: 170 / 172 || loss: 324.3646199968126\n",
      "Epoch:  130 || Loss:  177.51860412941866\n",
      "Epoch:  131 || Seq: 0 / 172 || loss: 249.19929725245424\n",
      "Epoch:  131 || Seq: 10 / 172 || loss: 121.66773674704812\n",
      "Epoch:  131 || Seq: 20 / 172 || loss: 239.88315667046442\n",
      "Epoch:  131 || Seq: 30 / 172 || loss: 83.14271587795682\n",
      "Epoch:  131 || Seq: 40 / 172 || loss: 228.14500130547418\n",
      "Epoch:  131 || Seq: 50 / 172 || loss: 441.02652579087476\n",
      "Epoch:  131 || Seq: 60 / 172 || loss: 198.68735872144285\n",
      "Epoch:  131 || Seq: 70 / 172 || loss: 467.7141500854492\n",
      "Epoch:  131 || Seq: 80 / 172 || loss: 177.63274657726288\n",
      "Epoch:  131 || Seq: 90 / 172 || loss: 306.4262203640408\n",
      "Epoch:  131 || Seq: 100 / 172 || loss: 390.1727712014142\n",
      "Epoch:  131 || Seq: 110 / 172 || loss: 100.44569091796875\n",
      "Epoch:  131 || Seq: 120 / 172 || loss: 431.1605559639309\n",
      "Epoch:  131 || Seq: 130 / 172 || loss: 236.92627093371223\n",
      "Epoch:  131 || Seq: 140 / 172 || loss: 115.95850958647551\n",
      "Epoch:  131 || Seq: 150 / 172 || loss: 22.54172679356166\n",
      "Epoch:  131 || Seq: 160 / 172 || loss: 7.391943048863184\n",
      "Epoch:  131 || Seq: 170 / 172 || loss: 44.2028352022171\n",
      "Epoch:  131 || Loss:  174.49256672823907\n",
      "Epoch:  132 || Seq: 0 / 172 || loss: 89.74536029497783\n",
      "Epoch:  132 || Seq: 10 / 172 || loss: 24.074466281467014\n",
      "Epoch:  132 || Seq: 20 / 172 || loss: 278.02931318283083\n",
      "Epoch:  132 || Seq: 30 / 172 || loss: 101.80770898113649\n",
      "Epoch:  132 || Seq: 40 / 172 || loss: 116.46943622369032\n",
      "Epoch:  132 || Seq: 50 / 172 || loss: 254.3100449855511\n",
      "Epoch:  132 || Seq: 60 / 172 || loss: 13.46038452145719\n",
      "Epoch:  132 || Seq: 70 / 172 || loss: 11.893749260902405\n",
      "Epoch:  132 || Seq: 80 / 172 || loss: 111.92446427596242\n",
      "Epoch:  132 || Seq: 90 / 172 || loss: 159.1103285275973\n",
      "Epoch:  132 || Seq: 100 / 172 || loss: 159.74258058547974\n",
      "Epoch:  132 || Seq: 110 / 172 || loss: 209.25453686714172\n",
      "Epoch:  132 || Seq: 120 / 172 || loss: 354.3386608660221\n",
      "Epoch:  132 || Seq: 130 / 172 || loss: 242.42498195872588\n",
      "Epoch:  132 || Seq: 140 / 172 || loss: 371.9394662276558\n",
      "Epoch:  132 || Seq: 150 / 172 || loss: 262.97344991895886\n",
      "Epoch:  132 || Seq: 160 / 172 || loss: 27.910193297654455\n",
      "Epoch:  132 || Seq: 170 / 172 || loss: 385.79581233433316\n",
      "Epoch:  132 || Loss:  169.28409758794965\n",
      "Epoch:  133 || Seq: 0 / 172 || loss: 51.302040312025284\n",
      "Epoch:  133 || Seq: 10 / 172 || loss: 175.3906307220459\n",
      "Epoch:  133 || Seq: 20 / 172 || loss: 227.8202760219574\n",
      "Epoch:  133 || Seq: 30 / 172 || loss: 312.94149729410805\n",
      "Epoch:  133 || Seq: 40 / 172 || loss: 20.286316990852356\n",
      "Epoch:  133 || Seq: 50 / 172 || loss: 300.3467965807234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  133 || Seq: 60 / 172 || loss: 144.81119713416467\n",
      "Epoch:  133 || Seq: 70 / 172 || loss: 297.2934920403265\n",
      "Epoch:  133 || Seq: 80 / 172 || loss: 189.1466638124906\n",
      "Epoch:  133 || Seq: 90 / 172 || loss: 40.96249241204489\n",
      "Epoch:  133 || Seq: 100 / 172 || loss: 276.46886751386853\n",
      "Epoch:  133 || Seq: 110 / 172 || loss: 475.11264979839325\n",
      "Epoch:  133 || Seq: 120 / 172 || loss: 239.30413754781088\n",
      "Epoch:  133 || Seq: 130 / 172 || loss: 63.2588465841193\n",
      "Epoch:  133 || Seq: 140 / 172 || loss: 14.542454541170013\n",
      "Epoch:  133 || Seq: 150 / 172 || loss: 112.13514796158542\n",
      "Epoch:  133 || Seq: 160 / 172 || loss: 238.66267502307892\n",
      "Epoch:  133 || Seq: 170 / 172 || loss: 6.309310382062739\n",
      "Epoch:  133 || Loss:  177.87316671967938\n",
      "Epoch:  134 || Seq: 0 / 172 || loss: 101.41891489530865\n",
      "Epoch:  134 || Seq: 10 / 172 || loss: 312.7892436981201\n",
      "Epoch:  134 || Seq: 20 / 172 || loss: 208.06153263648352\n",
      "Epoch:  134 || Seq: 30 / 172 || loss: 194.27597671084933\n",
      "Epoch:  134 || Seq: 40 / 172 || loss: 3.39612170311323\n",
      "Epoch:  134 || Seq: 50 / 172 || loss: 4.995216371724382\n",
      "Epoch:  134 || Seq: 60 / 172 || loss: 484.7064767984244\n",
      "Epoch:  134 || Seq: 70 / 172 || loss: 267.44735501029277\n",
      "Epoch:  134 || Seq: 80 / 172 || loss: 33.20730093268988\n",
      "Epoch:  134 || Seq: 90 / 172 || loss: 30.447457695007323\n",
      "Epoch:  134 || Seq: 100 / 172 || loss: 62.376577275139944\n",
      "Epoch:  134 || Seq: 110 / 172 || loss: 69.94472408294678\n",
      "Epoch:  134 || Seq: 120 / 172 || loss: 263.7965335845947\n",
      "Epoch:  134 || Seq: 130 / 172 || loss: 32.81597984404791\n",
      "Epoch:  134 || Seq: 140 / 172 || loss: 251.3341911315918\n",
      "Epoch:  134 || Seq: 150 / 172 || loss: 123.17077174939607\n",
      "Epoch:  134 || Seq: 160 / 172 || loss: 74.53434560515664\n",
      "Epoch:  134 || Seq: 170 / 172 || loss: 12.52041801561912\n",
      "Epoch:  134 || Loss:  172.03111448853033\n",
      "Epoch:  135 || Seq: 0 / 172 || loss: 88.9893664309853\n",
      "Epoch:  135 || Seq: 10 / 172 || loss: 11.075470410860502\n",
      "Epoch:  135 || Seq: 20 / 172 || loss: 224.08059292747862\n",
      "Epoch:  135 || Seq: 30 / 172 || loss: 23.240669387791836\n",
      "Epoch:  135 || Seq: 40 / 172 || loss: 55.608912876674104\n",
      "Epoch:  135 || Seq: 50 / 172 || loss: 330.5508234024048\n",
      "Epoch:  135 || Seq: 60 / 172 || loss: 158.97389086087546\n",
      "Epoch:  135 || Seq: 70 / 172 || loss: 58.13894611597061\n",
      "Epoch:  135 || Seq: 80 / 172 || loss: 370.78884415004563\n",
      "Epoch:  135 || Seq: 90 / 172 || loss: 234.37739896774292\n",
      "Epoch:  135 || Seq: 100 / 172 || loss: 25.44122247134938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0b44c46815bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0b44c46815bb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(optimizer, criterion, net, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mframe_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdata_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mframe_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-22d0125edfe3>\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-22d0125edfe3>\u001b[0m in \u001b[0;36mget_hidden\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m#return (Variable(torch.rand(1, num)).cuda(), Variable(torch.rand(1, num)).cuda())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m#return torch.zeros(1, num).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "img_transform = transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                    ])\n",
    "def train_model(optimizer, criterion, net, num_epochs):\n",
    "    x = None\n",
    "    y = None\n",
    "    img = None\n",
    "    epoch_loss = 0.\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        dataset_size = len(data_list)\n",
    "        sample_list = random.sample(range(0, dataset_size), dataset_size)\n",
    "        if epoch != 0 and epoch % 30 == 0:\n",
    "            adjust_learning_rate(optimizer)\n",
    "        if epoch != 0 and epoch % 5 == 0:\n",
    "            torch.save(net.state_dict(), model_path + 'predict_' + str(epoch) + '_' + str(epoch_loss/dataset_size) + '.pth')\n",
    "        epoch_loss = 0.\n",
    "        for i, idx in enumerate(sample_list):\n",
    "            frame_name = data_list[idx][0]\n",
    "            data_len = data_list[idx][1]\n",
    "            net.init_hidden(net.LSTM_SIZE)\n",
    "            total_loss = 0.\n",
    "            for frame_num in range(data_len-1):\n",
    "                file = open(ann_path + frame_name + '.txt','r')\n",
    "                file_split = file.read().splitlines()\n",
    "                next_frame = file_split[1]\n",
    "                img = Image.open(img_path + next_frame + '.jpg')\n",
    "                img = img.convert('RGB')\n",
    "                img = img_transform(img)\n",
    "                x = torch.tensor([(float(file_split[4]) - float(file_split[2]))*10])\n",
    "                y = torch.tensor([(float(file_split[5]) - float(file_split[3]))*10])\n",
    "                if USE_GPU:\n",
    "                    img = img.cuda()\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                output = net(img, x, y)\n",
    "                frame_name = next_frame\n",
    "                file = open(ann_path + frame_name + '.txt','r')\n",
    "                file_split = file.read().splitlines()\n",
    "                target_x = torch.tensor([(float(file_split[4]) - float(file_split[2]))*10])\n",
    "                target_y = torch.tensor([(float(file_split[4]) - float(file_split[2]))*10])\n",
    "                if USE_GPU:\n",
    "                    target_x = target_x.cuda()\n",
    "                    target_y = target_y.cuda()\n",
    "                target = torch.cat((target_x, target_y))\n",
    "                loss = criterion(output, target)\n",
    "                total_loss = total_loss + loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "            epoch_loss = epoch_loss + total_loss/(data_len-1)\n",
    "            if i % 10 == 0:\n",
    "                print('Epoch: ', epoch, '|| Seq: ' + str(i) + ' / '+ str(dataset_size), '|| loss:', total_loss/(data_len-1))\n",
    "        print('Epoch: ', epoch, '|| Loss: ', (epoch_loss/dataset_size))\n",
    "    return net\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "net = PredictNet()\n",
    "#net.load_state_dict(torch.load(\"/home/arg_ws3/argbot/catkin_ws/src/network/model/predict_5_0.0.pth\"))\n",
    "#net.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001)\n",
    "if USE_GPU:\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "net = train_model(optimizer, criterion, net, 201)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
