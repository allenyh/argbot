{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCH = 50\n",
    "GAMMA = 0.9\n",
    "STEP_SIZE = 200\n",
    "LR = 0.001\n",
    "USE_GPU = True\n",
    "decoder = ['cliff', 'end', 'four', 'straight']\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.Resize(227),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img_path = '/media/arg_ws3/5E703E3A703E18EB/data/subt_laser'\n",
    "model_path = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alexnet = torchvision.models.alexnet(pretrained = True)\n",
    "alexnet.classifier[6] = nn.Linear(4096, 4)\n",
    "if USE_GPU:\n",
    "    alexnet = alexnet.cuda()\n",
    "#optimizer = torch.optim.Adam(alexnet.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr = LR, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "if USE_GPU:\n",
    "    loss_func = loss_func.cuda()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (base_features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (base_classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "  )\n",
      "  (new_classifier): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # Everything except the last linear layer\n",
    "        self.base_features = base_model.features\n",
    "        self.base_classifier = nn.Sequential(*base_model.classifier[:-1])\n",
    "        self.new_classifier = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        self.modelName = 'MultiViewNet'\n",
    "        \n",
    "        '''\n",
    "        # Freeze those weights\n",
    "        for p in self.features.parameters():\n",
    "            p.requires_grad = False\n",
    "        '''\n",
    "\n",
    "    def forward(self, img):\n",
    "        f = self.base_features(img)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        fc2 = self.base_classifier(f)\n",
    "        y = self.new_classifier(fc2)\n",
    "        return y\n",
    "\n",
    "base_model = torchvision.models.alexnet(pretrained = True)\n",
    "alexnet = AlexNet(base_model, 4)\n",
    "if USE_GPU:\n",
    "    alexnet = alexnet.cuda()\n",
    "#for p in multiviewnet.base_features[0].parameters():\n",
    "#    print(p.name, p.data)\n",
    "#optimizer = torch.optim.Adam(alexnet.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr = LR, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "if USE_GPU:\n",
    "    loss_func = loss_func.cuda()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class laserDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = '/media/arg_ws3/5E703E3A703E18EB/data/subt_laser', transform = None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgData = datasets.ImageFolder(root = self.root, \\\n",
    "                                transform = self.transform)\n",
    "        self.classes = self.imgData.classes\n",
    "        self.class_to_idx = self.imgData.class_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgData.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgData.__getitem__(idx)[0]\n",
    "        label = self.imgData.__getitem__(idx)[1]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiViewScaleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = '../images', transform = None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgData = datasets.ImageFolder(root = self.root, \\\n",
    "                                transform = self.transform)\n",
    "        self.classes = self.imgData.classes\n",
    "        self.class_to_idx = self.imgData.class_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgData.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgData.__getitem__(idx)[0]\n",
    "        label = self.imgData.__getitem__(idx)[1]\n",
    "        img_name = self.imgData.samples[idx][0]\n",
    "        s = img_name.split('.jpg')\n",
    "        s = s[0].split('/')\n",
    "        scalar_nums = s[-1].split('_')[-3:]\n",
    "        scalar_nums = [torch.FloatTensor([math.log(float(i))*2]) for i in scalar_nums]\n",
    "        if len(scalar_nums)!=3:\n",
    "            return img, label, ['0', '0', '0']\n",
    "        return img, label, scalar_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_loader = torch.utils.data.DataLoader(dataset = train_data,                                 batch_size = BATCH_SIZE,                                 shuffle = True)'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_data = datasets.ImageFolder(root = img_path, \\\n",
    "                                transform = data_transform)'''\n",
    "\n",
    "'''train_loader = torch.utils.data.DataLoader(dataset = train_data, \\\n",
    "                                batch_size = BATCH_SIZE, \\\n",
    "                                shuffle = True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cliff', 'end', 'four', 'straight']\n"
     ]
    }
   ],
   "source": [
    "laserDst = laserDataset(root = img_path,\\\n",
    "                            transform = data_transform)\n",
    "decoder = laserDst.classes\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset to training & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1451\n",
      "train size: 1306\n",
      "validation size: 145\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(laserDst) #3080 #2080\n",
    "indices = list(range(dataset_size))\n",
    "validation_split = .1\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = laserDst, \\\n",
    "                                batch_size = BATCH_SIZE, \\\n",
    "                                sampler = train_sampler)\n",
    "train_size = len(train_indices)\n",
    "validation_size = len(val_indices)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = laserDst, \\\n",
    "                                batch_size = validation_size, \\\n",
    "                                sampler = valid_sampler)\n",
    "valid_iter = iter(validation_loader)\n",
    "valid_data = next(valid_iter)\n",
    "valid_y = valid_data[1].cuda()\n",
    "print('Dataset size:', dataset_size)\n",
    "print('train size:', train_size)\n",
    "print('validation size:', validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_scale(model, validset, label):\n",
    "    if USE_GPU:\n",
    "        validset[0] = validset[0].cuda()\n",
    "        scale = [x.cuda() for x in validset[2]]\n",
    "    output = model(validset[0], scale)\n",
    "    pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "    accuracy = (pred_y == label).sum().item() / float(label.size(0))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, validset, label):\n",
    "    if USE_GPU:\n",
    "        validset[0] = validset[0].cuda()\n",
    "    output = model(validset[0])\n",
    "    pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "    accuracy = (pred_y == label).sum().item() / float(label.size(0))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 1.365610 | accuracy: 0.4552 | learning rate: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-214ae1a4ec5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#for step, (b_x, b_y, b_scale) in enumerate(train_loader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# gives batch data, normalize x when iterate train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mb_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-54e3e9ce8f85>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \"\"\"\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "count = 0\n",
    "loss_values = []\n",
    "lr = LR\n",
    "for epoch in range(EPOCH):\n",
    "    #for step, (b_x, b_y, b_scale) in enumerate(train_loader):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        if USE_GPU:\n",
    "            b_x = b_x.cuda()\n",
    "            b_y = b_y.cuda()\n",
    "            \n",
    "        '''if USE_GPU:\n",
    "            b_x = b_x.cuda()\n",
    "            b_y = b_y.cuda()\n",
    "            b_scale = [x.cuda() for x in b_scale]'''\n",
    "        \n",
    "        #output = multiviewnet(b_x, b_scale)  # cnn output\n",
    "        output = alexnet(b_x)  # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        #scheduler.step()                # dynamic learning rate\n",
    "        \n",
    "        if count % STEP_SIZE == 0 and count != 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= GAMMA\n",
    "                lr = param_group['lr']\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            #accuracy = compute_accuracy_scale(multiviewnet, valid_data, valid_y)\n",
    "            accuracy = compute_accuracy(alexnet, valid_data, valid_y)\n",
    "            print('Epoch: ', epoch, '| train loss: %.6f' % loss.cpu().data.numpy(), \\\n",
    "                  '| accuracy: %.4f' % accuracy, \\\n",
    "                  '| learning rate: %.6f' % lr)\n",
    "        if count % 10 == 0 and count != 0:\n",
    "            loss_values.append(loss.item())\n",
    "        if count % 200 == 0 and count != 0:\n",
    "            plt.plot(loss_values, '-b', label='loss')\n",
    "            plt.show()\n",
    "        count = count + 1\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        PATH = model_path + '/epoch' + str(epoch) + '.pth'\n",
    "        #torch.save(multiviewnet.state_dict(), PATH)\n",
    "        torch.save(alexnet.state_dict(), PATH)\n",
    "        print(\"Save net: \", PATH)\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (base_features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (base_classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "  )\n",
       "  (new_classifier): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.load_state_dict(torch.load('./model/epoch45.pth'))\n",
    "alexnet.eval()\n",
    "#alexnet = torch.load('./robotx_ch3.pth')\n",
    "#alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loader = torch.utils.data.DataLoader(dataset = laserDst, \\\n",
    "                                batch_size = 1, \\\n",
    "                                sampler = valid_sampler)\n",
    "show_iter = iter(show_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  torch.Size([1, 3, 227, 227])\n",
      "One channel:  torch.Size([1, 1, 227, 227])\n",
      "Prediction:  straight\n",
      "Label:  straight\n",
      "Image shape:  (227, 227, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5bf6b5f6a0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = next(show_iter)\n",
    "img = data[0][0].numpy()\n",
    "torch_img = np.expand_dims(img, axis=0)\n",
    "x = torch.tensor(torch_img).type('torch.FloatTensor').cuda()\n",
    "print('Original: ', x.shape)\n",
    "one_channel = x[:,0,:,:]\n",
    "one_channel = one_channel.unsqueeze(0)\n",
    "print('One channel: ', one_channel.shape)\n",
    "output = alexnet(x)\n",
    "pred_y = torch.max(output, 1)[1].cpu().data.numpy()\n",
    "print('Prediction: ', decoder[int(pred_y)])\n",
    "\n",
    "print('Label: ', decoder[data[1][0].numpy()])\n",
    "#img = img[:][:]\n",
    "#img = np.squeeze(img)\n",
    "cv_img = np.transpose(img, (1, 2, 0))\n",
    "print('Image shape: ', cv_img.shape)\n",
    "plt.imshow(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0008870a7cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "data = next(show_iter)\n",
    "img = data[0][0].numpy()\n",
    "print(img.shape)\n",
    "torch_img = np.expand_dims(img, axis=0)\n",
    "x = torch.tensor(torch_img).type('torch.FloatTensor').cuda()\n",
    "print(x.shape)\n",
    "scalar_data = [i.cuda() for i in data[2]]\n",
    "output = multiviewnet(x, scalar_data)\n",
    "print(scalar_data)\n",
    "pred_y = torch.max(output, 1)[1].cpu().data.numpy()\n",
    "print('Prediction: ', decoder[int(pred_y)])\n",
    "\n",
    "print('Label: ', decoder[data[1][0].numpy()])\n",
    "#img = img[:][:]\n",
    "#img = np.squeeze(img)\n",
    "cv_img = np.transpose(img, (1, 2, 0))\n",
    "print('Image shape: ', cv_img.shape)\n",
    "plt.imshow(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type MultiViewNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(multiviewnet, '../model_scale/robotx_final.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
